{"meta":{"version":1,"warehouse":"4.0.0"},"models":{"Asset":[{"_id":"source/keybase.txt","path":"keybase.txt","modified":0,"renderable":0},{"_id":"source/about/index/ruiramos.pdf","path":"about/index/ruiramos.pdf","modified":0,"renderable":0},{"_id":"themes/hexo-theme-light/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/js/gallery.js","path":"js/gallery.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/js/jquery-3.4.1.min.js","path":"js/jquery-3.4.1.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/js/jquery.imagesloaded.min.js","path":"js/jquery.imagesloaded.min.js","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/css/font/fontawesome-webfont.eot","path":"css/font/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/css/font/fontawesome-webfont.svg","path":"css/font/fontawesome-webfont.svg","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/css/font/fontawesome-webfont.ttf","path":"css/font/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/hexo-theme-light/source/css/font/fontawesome-webfont.woff","path":"css/font/fontawesome-webfont.woff","modified":0,"renderable":1}],"Cache":[{"_id":"source/keybase.txt","hash":"03b7f6d8ff3f16b3d599e2d15c2cf2ca73bc046e","modified":1599403117863},{"_id":"source/_drafts/AirflowPoC.md","hash":"ca567fea6f57dd44a6e906e48a20f217ee2c4975","modified":1599403117851},{"_id":"source/_drafts/alluxio.md","hash":"ff5e8d05729ca4f13e5f67bf91c21bfc4cbfcfed","modified":1599403117851},{"_id":"source/_drafts/quicksparkdocker.md","hash":"f929c0fe5f463d949fb5bc14d9a6cad5adc2af7c","modified":1599403117851},{"_id":"source/about/index.md","hash":"bc5e26b6f3c381d598388fa2f706b4373d571c03","modified":1599403117863},{"_id":"source/_posts/ChromeProxy.md","hash":"c29429e7c6cf36d6e19702431a39b28992e2add6","modified":1599403117851},{"_id":"source/_posts/CreateAvroTablesForBigQuery.md","hash":"750be879be9e1e59adbb9753c5470199213a0050","modified":1599403117851},{"_id":"source/_posts/Created-rramos-github-io.md","hash":"84bb8b53100e7fcfb48475de6695e618b8778130","modified":1599403117851},{"_id":"source/_posts/GA-CustomDimension.md","hash":"9fc526359a405611d288a6d298da13800cb7637b","modified":1599403117851},{"_id":"source/_posts/PolybaseCDH.md","hash":"f5ea0ac8ec7cb7cdeef53da8a14ec7be3288b34b","modified":1599403117851},{"_id":"source/_posts/SparkSummit2016.md","hash":"6253e90fc1529ea21656be307d6e280250dc7ed4","modified":1599403117851},{"_id":"source/_posts/ansible-cmdb.md","hash":"3a1d26f20ff60f7ea7e1c7d83aa1d4d9689ac9c5","modified":1599403117851},{"_id":"source/_posts/dockerclean.md","hash":"ce06d03db184a779a495106ef8e771b744f2669b","modified":1599403117851},{"_id":"source/_posts/dockers.md","hash":"263f07179bed6130f8f312b781d18a5477bd8819","modified":1599403117855},{"_id":"source/_posts/drelephat.md","hash":"95eac6f93e0cbd1a8d7af7354e49cedcb08c339e","modified":1599403117855},{"_id":"source/_posts/git-cheat-sheet.md","hash":"3e3202cbd242b1cf0e67c2d47cb94d8feca8b45a","modified":1599403117855},{"_id":"source/_posts/hive-udfs.md","hash":"53a54508bcfc91708f7eaa1872b4285bc3b72424","modified":1599403117855},{"_id":"source/_posts/hiveGC-md.md","hash":"8ad9515534dfe9f9a9bc7e11195b990fae36216d","modified":1599403117855},{"_id":"source/_posts/keybase-io.md","hash":"36a6908532589c990543a7f432a0cd8d7336e02b","modified":1599403117855},{"_id":"source/_posts/ksql.md","hash":"1ef40f9cedca1519419915fb7e04c655510f4e0b","modified":1599403117855},{"_id":"source/_posts/mkv.md","hash":"2b860ce58fc227e9fd0b5ae6a40d454280822fb2","modified":1599403117855},{"_id":"source/_posts/nifiparcel.md","hash":"b9e99dd278b47dc4b6af59f9ef03bfb5109b30e5","modified":1599403117855},{"_id":"source/_posts/portainer.md","hash":"0e8e242bd0e89f67b385cf9eb77448b77c3b59d6","modified":1599403117859},{"_id":"source/_posts/portotechhub-com.md","hash":"fc2ae16cddd192772aaa31807f65e179d5a28894","modified":1599403117859},{"_id":"source/_posts/powershellonlinux.md","hash":"8fb2b8f2ab0cd7f8eb9885fefcb67f561f1384f3","modified":1599403117859},{"_id":"source/_posts/quickzeppelin.md","hash":"640713464d6c358b606ea9b4b586f82d2ac58b38","modified":1599403117859},{"_id":"source/_posts/rddstransformations.md","hash":"312cb9ce1f8af51870c811adc74091c3cc7e181b","modified":1599403117859},{"_id":"source/_posts/ssh-config.md","hash":"00380fbc84583bf1d7175a5df4a1b9ef1e69cd27","modified":1599403117859},{"_id":"source/_posts/sublimeinstall.md","hash":"04024fc7e1f89ff7680d591689e2343ac1098c69","modified":1599403117859},{"_id":"source/_posts/unpivot.md","hash":"f1f4d1409441126331ac33bf786c57dd2cb099cd","modified":1599403117859},{"_id":"source/_posts/vagrant-qemu.md","hash":"af62f895e7b02619218139d8e5b2b64ab006cc9d","modified":1599403117859},{"_id":"source/_posts/vdi2qemu.md","hash":"53058ad49f107ec06c50f4e81f2a53d7bf020cb1","modified":1599403117859},{"_id":"source/_posts/GA-CustomDimension/Custom_Dimension.png","hash":"a7723352b4f35225a4dafefb03d8f4f0766e5e73","modified":1599403117851},{"_id":"source/about/index/ruiramos.pdf","hash":"ba5821347a2896484e67e064e7a73b835ce27fd3","modified":1599403117863},{"_id":"source/_posts/GA-CustomDimension/GA_CustomDimension_js.png","hash":"a63cf90c29215002c0cb1f3c02ac76acc7f1c0ea","modified":1599403117851},{"_id":"source/_posts/nifiparcel/parcel01.png","hash":"54fd2ac30115921f041477a4dd1810b42c53eed9","modified":1599403117855},{"_id":"source/_posts/nifiparcel/parcel02.png","hash":"705593597fe6b7d67f31b56c003d1ee5cb7edc18","modified":1599403117855},{"_id":"source/_posts/quickzeppelin/radiant_win.png","hash":"f5004a14984cc86d88f6ba9a342893af02241139","modified":1599403117859},{"_id":"source/_posts/vagrant-qemu/virt-manager.png","hash":"f40609ac621553b2fa30352e490aa2965afe8ed4","modified":1599403117859},{"_id":"source/_posts/ansible-cmdb/screenshot-overview.png","hash":"30fb11235b83ce75d5088ac1ace8201932baf304","modified":1599403117851},{"_id":"source/_posts/portainer/portainer.io.png","hash":"de8c189f6a7f62ba0db1cb735c086ff9aefd5e72","modified":1599403117859},{"_id":"source/_posts/vdi2qemu/virt-manager.png","hash":"85a5691d17b5dc3498e4712ed33b50f8cf0984d4","modified":1599403117863},{"_id":"source/_posts/nifiparcel/parcel03.png","hash":"ae828f913ead1e383183317d61b939a8a933be4d","modified":1599403117859},{"_id":"source/_posts/drelephat/DrElephant-2-dashboard.jpg","hash":"677510544f5d9cbc0b123f3fe5d4629efc418b5d","modified":1599403117855},{"_id":"public/atom.xml","hash":"95b9be4ad0647a089e5860d06d5cde31f4791089","modified":1613857956441},{"_id":"public/sitemap.xml","hash":"eb94014e57dbbc3df60c88d51933ad6968ea6d59","modified":1613857956441},{"_id":"public/about/index.html","hash":"6a507a60618ee659246c80a266c90508b4954fba","modified":1613860004810},{"_id":"public/2017/10/18/dockers/index.html","hash":"99ba09a4a22ef8fac17ee012694d25a60e23f5c7","modified":1613860004810},{"_id":"public/2017/10/13/drelephat/index.html","hash":"7f2bbe1522c264eff86f8f33c8379a5e4d326bed","modified":1613860004810},{"_id":"public/2017/10/09/ansible-cmdb/index.html","hash":"8a747b90d72cfdaffe907f6757ae68326f96eaa5","modified":1613860004810},{"_id":"public/2017/10/09/rddstransformations/index.html","hash":"c5b13d79bc860633b467e943082625f1c738c83b","modified":1613860004810},{"_id":"public/2017/10/06/powershellonlinux/index.html","hash":"0abe8a834e60068e1561f62354d206dbf971f324","modified":1613860004810},{"_id":"public/2017/10/02/vdi2qemu/index.html","hash":"5c5e9eed57390bdc8a7269c000d33f161ab32727","modified":1613860004810},{"_id":"public/2017/09/29/nifiparcel/index.html","hash":"21f7906020131efb0f108a738f37d9762870e2e0","modified":1613860004810},{"_id":"public/2017/09/27/hive-udfs/index.html","hash":"57ef571df5e4caa1c9167e8c006d8f922e7343ee","modified":1613860004810},{"_id":"public/2017/09/26/vagrant-qemu/index.html","hash":"c5353e9572fbb7a9188574e3b1bcfd982515981e","modified":1613860004810},{"_id":"public/2017/09/24/ksql/index.html","hash":"c41266f577a7883ee740d4255c1586c96e16399a","modified":1613860004810},{"_id":"public/2017/09/23/portainer/index.html","hash":"c5817b4134e6c0ab21ae148de5ce17ab7c1dde2b","modified":1613860004810},{"_id":"public/2017/09/23/sublimeinstall/index.html","hash":"0bf942d7d0fc55cdcc1cf242c514a351be81a7e2","modified":1613860004810},{"_id":"public/2017/09/19/CreateAvroTablesForBigQuery/index.html","hash":"c5989e588b6e75b38ccd9b35cc8dc2fb46e31b0e","modified":1613860004810},{"_id":"public/2017/09/19/quickzeppelin/index.html","hash":"6e80211a62d5bfdb1c2f56d97e5ba440e6594d3a","modified":1613860004810},{"_id":"public/2017/09/19/dockerclean/index.html","hash":"b837abbf782c140aaa90df488e7b9a1544b82820","modified":1613860004810},{"_id":"public/2017/09/13/ssh-config/index.html","hash":"587ab90b344a9bdac6e682e3cee35b24626c7b1f","modified":1613860004810},{"_id":"public/2017/09/13/ChromeProxy/index.html","hash":"6d5893018fe1e56c29f20f1060526baafac83a55","modified":1613860004810},{"_id":"public/2017/09/09/PolybaseCDH/index.html","hash":"7fe0d58f818c5738fb99fe8f757dec13ba6e7865","modified":1613860004810},{"_id":"public/2017/09/07/GA-CustomDimension/index.html","hash":"75bd3623cdd26edb1bd50b335cc1e33c5b959273","modified":1613860004810},{"_id":"public/2017/09/07/unpivot/index.html","hash":"0bafde763afef7b2f468444ff9fc43afc23e5cd5","modified":1613860004810},{"_id":"public/2017/09/07/mkv/index.html","hash":"15e410106ea7e363f33a6322483a3a6aa295e7c0","modified":1613860004810},{"_id":"public/2016/11/03/SparkSummit2016/index.html","hash":"d1bae22cc398a7648ec9239f046197565547c602","modified":1613860004810},{"_id":"public/2016/10/19/hiveGC-md/index.html","hash":"66b996138b58326182960e26894c832baf4e6e70","modified":1613860004810},{"_id":"public/2016/09/23/portotechhub-com/index.html","hash":"69c60da0a4b476365559afe2621b5e608b458e47","modified":1613860004810},{"_id":"public/2016/09/18/git-cheat-sheet/index.html","hash":"acb43fe4e189796587a88f727c7b9379ea467e80","modified":1613860004810},{"_id":"public/2016/09/18/keybase-io/index.html","hash":"813dd60ee97f630e765b8140cb0da7e4aceba380","modified":1613860004810},{"_id":"public/2016/09/17/Created-rramos-github-io/index.html","hash":"858f6f328752712c625dbf66c657b80cec62becf","modified":1613860004810},{"_id":"public/archives/index.html","hash":"8f37458e1d655968f48eb7e063ca6d6bb406b632","modified":1613860004810},{"_id":"public/archives/2016/index.html","hash":"6ea5e85a28d746b8e891cbaa48577e2efa9c52ca","modified":1613860004810},{"_id":"public/archives/2016/09/index.html","hash":"d603c628d88a93233a4cd5f12f634344265bd3d5","modified":1613860004810},{"_id":"public/archives/2016/10/index.html","hash":"8144377c876b24297c7e93d2f297f7cdde59e29d","modified":1613860004810},{"_id":"public/archives/2016/11/index.html","hash":"39b9b17cd597b24ba57cd7169f2805623610310e","modified":1613860004810},{"_id":"public/archives/2017/index.html","hash":"9fa57a3f4fc4e6d18da2c9711f5ac1da55080947","modified":1613860004810},{"_id":"public/archives/2017/09/index.html","hash":"475ba6679a6a2715af87fa27e78311c63ebd88f2","modified":1613860004810},{"_id":"public/archives/2017/10/index.html","hash":"643862953d992a52c95a91ecfa64dc6eaabf5184","modified":1613860004810},{"_id":"public/index.html","hash":"f456b31b9edcccbae127b8ab18ad5ee56c2403c8","modified":1613860004810},{"_id":"public/page/2/index.html","hash":"b83bc7e0df8cae9d101660d0106526a64a800df8","modified":1613860004810},{"_id":"public/page/3/index.html","hash":"fea21befeff00d028ea61b945524c212203c8dc9","modified":1613860004810},{"_id":"public/tags/HDFS/index.html","hash":"04415837f87cf3dc17ee6f06cd2a9e9863a38096","modified":1613860004810},{"_id":"public/tags/BigData/index.html","hash":"6ff7967082abbb526953c30088c23659dc21bca2","modified":1613860004810},{"_id":"public/tags/Hive/index.html","hash":"5bd6776e35b0d949e6acf2169defc7990d52ba38","modified":1613860004810},{"_id":"public/tags/Sqoop/index.html","hash":"e6f5e4ecc4a0b1046937aae083f8ca35e0a85390","modified":1613860004810},{"_id":"public/tags/Docker/index.html","hash":"2b4526ea0c9c6f55fcb12f42e1cd9a6771d04ca1","modified":1613860004810},{"_id":"public/tags/Spark/index.html","hash":"8ea032920a1fb7156cf6489fb26a945e26776f68","modified":1613860004810},{"_id":"public/tags/Metrics/index.html","hash":"43d1b72be1a3a3a7be359e70dd2a207b26050f9e","modified":1613860004810},{"_id":"public/tags/GoogleChrome/index.html","hash":"1585d4878887e72bd16ad65f7c91a0e0bb537904","modified":1613860004810},{"_id":"public/tags/Customization/index.html","hash":"06ec413f4bc3bcfead3cb468598c52703944ab63","modified":1613860004810},{"_id":"public/tags/Internet/index.html","hash":"a0bc1d77a9ff8766885536248ce1a9f0e9c921c9","modified":1613860004810},{"_id":"public/tags/Proxy/index.html","hash":"852ccd2d5cb5f5b73285f7108ea7b913218c146d","modified":1613860004810},{"_id":"public/tags/Linux/index.html","hash":"50b5b91eae281aad4ad42fd3404e5e2e2394583e","modified":1613860004810},{"_id":"public/tags/BigQuery/index.html","hash":"1d6b79d3412ff3231d73ef757cc7b68b2342fc92","modified":1613860004810},{"_id":"public/tags/Avro/index.html","hash":"a9eab1e81ac9b131da027177462cfb0a06a89336","modified":1613860004810},{"_id":"public/tags/Parquet/index.html","hash":"cc5625d1b99c13835dc4058620bc221163e3fbcc","modified":1613860004810},{"_id":"public/tags/GCS/index.html","hash":"b9905d870f9838c0826c4c681cd2ec502f5255ab","modified":1613860004810},{"_id":"public/tags/github/index.html","hash":"5638e76d4d7f1e54bb9be86882ad4ffd87607020","modified":1613860004810},{"_id":"public/tags/hexo/index.html","hash":"4ab0c6a8edd42653e0c60a04162c4ab7c03f30cc","modified":1613860004810},{"_id":"public/tags/blog/index.html","hash":"47091b76163da4eda818364880af48d733600a08","modified":1613860004810},{"_id":"public/tags/git/index.html","hash":"c2cef8bdfaf6eddb46e679c415e5721523df5117","modified":1613860004810},{"_id":"public/tags/markdown/index.html","hash":"8dd787506139b070af897fcd6174bcbd73b26664","modified":1613860004810},{"_id":"public/tags/Google-Analytics/index.html","hash":"133a4957ccdb3db89c9a747ce1ac79724d4feb85","modified":1613860004810},{"_id":"public/tags/Analytics/index.html","hash":"66eb9f83d27a086c3242f347c2623bb1043b4a17","modified":1613860004810},{"_id":"public/tags/Dimensions/index.html","hash":"5f8e9359d627849676be0959d265b1338eb20de2","modified":1613860004810},{"_id":"public/tags/Cloudera/index.html","hash":"44c071f41487010a22f41c871030b689212989e8","modified":1613860004810},{"_id":"public/tags/Polybase/index.html","hash":"856e648246f497fbcde8aa8bf3d5f4485bb44ba2","modified":1613860004810},{"_id":"public/tags/SQLServer/index.html","hash":"bc755575320562ce0d5cbe77ec2effb4452761d9","modified":1613860004810},{"_id":"public/tags/Data-Ingestion/index.html","hash":"1dc6dcf202b47ecaf74be0fc87d53e2fcf6c9471","modified":1613860004810},{"_id":"public/tags/Conferences/index.html","hash":"69bbb88a1f39bea56468da20370ce9a38391bc25","modified":1613860004810},{"_id":"public/tags/SparkSummit/index.html","hash":"a1bbdc2fca57880f0d9bea0bf3549d5599f0de70","modified":1613860004810},{"_id":"public/tags/Hadoop/index.html","hash":"b5cca00a7f5bcccb6ae3902cdaa4bbaebb36b6cc","modified":1613860004810},{"_id":"public/tags/Streamming/index.html","hash":"f66536fd83894b068d7e2eb8b2404d57aef6db1a","modified":1613860004810},{"_id":"public/tags/Kafka/index.html","hash":"06b49d5101a55d29f98d3c7d0b3c8bc1d4fe6b68","modified":1613860004810},{"_id":"public/tags/Ansible/index.html","hash":"6ef788d41e12aafffecf335330b01cabecef5c4c","modified":1613860004810},{"_id":"public/tags/btrfs/index.html","hash":"3f7a1d24da7481813e47ef822d2d7b8401dd3972","modified":1613860004810},{"_id":"public/tags/docker-compose/index.html","hash":"7c5ab827cbe2d9aac7677c5bb069e973f4a195e6","modified":1613860004810},{"_id":"public/tags/utils/index.html","hash":"67d47ee9a0d94b681c2b35e262f020c11adac45f","modified":1613860004810},{"_id":"public/tags/dr-elephant/index.html","hash":"62aa0549d3bf5ac8dad6019b2b9fcb3ce85eaa79","modified":1613860004810},{"_id":"public/tags/Performance/index.html","hash":"7de5ea39629d65857b059ff4661499caaf34642e","modified":1613860004810},{"_id":"public/tags/Monitoring/index.html","hash":"db7e207ee58ac012aaa7f33c01b18ba4b868264e","modified":1613860004810},{"_id":"public/tags/Tunning/index.html","hash":"b94956e7848fef56b7da5db3504fcad452f0c4ff","modified":1613860004810},{"_id":"public/tags/cheat/index.html","hash":"65f5478826055406e2c43c5044bebd254ede62d1","modified":1613860004810},{"_id":"public/tags/sheet/index.html","hash":"72ff01e68759d2bf2bc0b1b994bb8f88cbe05cef","modified":1613860004810},{"_id":"public/tags/udfs/index.html","hash":"2bbedb93fd73321624899dcadb46101eaf671fbb","modified":1613860004810},{"_id":"public/tags/Java/index.html","hash":"9ea5675c2ed4c712401fd83b61bee8caae0ba6f9","modified":1613860004810},{"_id":"public/tags/hadoop/index.html","hash":"53b639ac1c706bbca14f79e51365b9b1b19acb92","modified":1613860004810},{"_id":"public/tags/hive/index.html","hash":"cb46b413a70e29de2d18ca206c471bda4b86258f","modified":1613860004810},{"_id":"public/tags/GC/index.html","hash":"82bb87ed0f2d99eb587279877ffa67a00840d7f9","modified":1613860004810},{"_id":"public/tags/exception/index.html","hash":"7fc62e038ebeebd4dafe727732acbeba886ef059","modified":1613860004810},{"_id":"public/tags/beeline/index.html","hash":"2435c25de9865f5da664d73cd787a70abb427ec3","modified":1613860004810},{"_id":"public/tags/keybase/index.html","hash":"da7a6808603d4b411d5928793cc92a20ee756510","modified":1613860004810},{"_id":"public/tags/pgp/index.html","hash":"2a30477c6d47d545199db64354bf93aa5fe7f27f","modified":1613860004810},{"_id":"public/tags/encryption/index.html","hash":"369d634f3338dd7113a9d5cf30f3a03cd3a0f4c6","modified":1613860004810},{"_id":"public/tags/pki/index.html","hash":"60ce05483d95e0b82c086a87d4d1ee85d58de1df","modified":1613860004810},{"_id":"public/tags/keys/index.html","hash":"a913654e9ac75325a983d1ff35019b4e7068b91d","modified":1613860004810},{"_id":"public/tags/kbfs/index.html","hash":"94aa65295baf32c875459e07ad68dda70600d7dc","modified":1613860004810},{"_id":"public/tags/KSQL/index.html","hash":"f2ac276400fe36131cfd8313016a0b1686b9a4d7","modified":1613860004810},{"_id":"public/tags/SQL/index.html","hash":"bbad3451dfffaee873426239fbc72c522c0f407c","modified":1613860004810},{"_id":"public/tags/Streaming/index.html","hash":"70b769d508f675624cae47c4782b73a1429b8e04","modified":1613860004810},{"_id":"public/tags/Markdown/index.html","hash":"d8342ceecca5b4b11abcb26c0baa9b3037ec5138","modified":1613860004810},{"_id":"public/tags/Tools/index.html","hash":"4e1b5c430da61b773630938b5a7d7ab40eeb535c","modified":1613860004810},{"_id":"public/tags/Utils/index.html","hash":"44f6dcd66a3e80a740ed3c800d40bf635fea5c98","modified":1613860004810},{"_id":"public/tags/Nifi/index.html","hash":"a55f6a5a167cfa4c7637089cee160d06b2fb033a","modified":1613860004810},{"_id":"public/tags/CDH-Parcel/index.html","hash":"cd867c192ef15903802f7af46c1e8a7876d9da6c","modified":1613860004810},{"_id":"public/tags/Portainer/index.html","hash":"516ca25b6abfcff714a46f8cdda4cfd9352fa3cc","modified":1613860004810},{"_id":"public/tags/PortoTechHub/index.html","hash":"2aa3b46a5cd3d75a613f1629283c984170b19ad8","modified":1613860004810},{"_id":"public/tags/Porto/index.html","hash":"1651ea7c55266af690b474ee2760e401b253942d","modified":1613860004810},{"_id":"public/tags/Tech/index.html","hash":"ad7db7dd3e6e794c370f19454a289b7552b993bc","modified":1613860004810},{"_id":"public/tags/PowerShell/index.html","hash":"5621761bfd560e3b841096ea27347161ebd02df6","modified":1613860004810},{"_id":"public/tags/Windows/index.html","hash":"d246d0fe613b1bfb8a534232e6f56050dc49f476","modified":1613860004810},{"_id":"public/tags/Zeepelin/index.html","hash":"d3a6bcf0e507f6b6ad3b51ce6bc32158c3bfea9b","modified":1613860004810},{"_id":"public/tags/Scala/index.html","hash":"b19952d4ca37580fdfd6f45ef5a58a4f891d3728","modified":1613860004810},{"_id":"public/tags/RDD/index.html","hash":"f1f24279dd6993ed700eaa929bebcf22da644c30","modified":1613860004810},{"_id":"public/tags/CheatSheet/index.html","hash":"f6536f78f030d91532734b3f18d06ae19905f0d7","modified":1613860004810},{"_id":"public/tags/ssh/index.html","hash":"5e54fbb2c0ad1ac56e087b2db5983d7bb406f4d8","modified":1613860004810},{"_id":"public/tags/IDE/index.html","hash":"28d88328c617eb5a9050a147792d015743cb45fc","modified":1613860004810},{"_id":"public/tags/Install/index.html","hash":"0ed3a55e4185cd504c2c7a984a9c7351b9299369","modified":1613860004810},{"_id":"public/tags/Optimization/index.html","hash":"c6e3cac8d96af994cafbd654f3d2a0af1128b115","modified":1613860004810},{"_id":"public/tags/Vagrant/index.html","hash":"c8724da095227d650068549e4f8d35a5ff199a2a","modified":1613860004810},{"_id":"public/tags/Qemu/index.html","hash":"1223a1ff1163073625b5f085aed5b50060841c62","modified":1613860004810},{"_id":"public/tags/KVM/index.html","hash":"8b729bd351d7ccfc7e99c7e976d485110713b0a0","modified":1613860004810},{"_id":"public/tags/libvirt/index.html","hash":"72e4933c0700d765834bfb05f00ab4978a9a130f","modified":1613860004810},{"_id":"public/tags/Mesos/index.html","hash":"a5989975617c0cad0e032e32b21dfd5519bb959c","modified":1613860004810},{"_id":"public/tags/VirtualBox/index.html","hash":"2a7eb81af1ca52c4d7ef0b509561dc9b527199e5","modified":1613860004810},{"_id":"public/tags/QEMU/index.html","hash":"a7ed216e5cff1fdc755603e9bcb1fe7a7f5d2af5","modified":1613860004810},{"_id":"public/tags/vdi/index.html","hash":"9c7b1de9b04d2f784aaa6e8052fde8cd92027ae3","modified":1613860004810},{"_id":"public/tags/qcow2/index.html","hash":"9c185e683bf907b8a71fc16ae7767bd5d414747b","modified":1613860004810},{"_id":"public/keybase.txt","hash":"03b7f6d8ff3f16b3d599e2d15c2cf2ca73bc046e","modified":1613857956441},{"_id":"public/about/index/ruiramos.pdf","hash":"ba5821347a2896484e67e064e7a73b835ce27fd3","modified":1613857956441},{"_id":"public/2017/09/07/GA-CustomDimension/GA_CustomDimension_js.png","hash":"a63cf90c29215002c0cb1f3c02ac76acc7f1c0ea","modified":1613857956441},{"_id":"public/2017/09/07/GA-CustomDimension/Custom_Dimension.png","hash":"a7723352b4f35225a4dafefb03d8f4f0766e5e73","modified":1613857956441},{"_id":"public/2017/09/29/nifiparcel/parcel01.png","hash":"54fd2ac30115921f041477a4dd1810b42c53eed9","modified":1613857956441},{"_id":"public/2017/09/29/nifiparcel/parcel02.png","hash":"705593597fe6b7d67f31b56c003d1ee5cb7edc18","modified":1613857956441},{"_id":"public/2017/09/19/quickzeppelin/radiant_win.png","hash":"f5004a14984cc86d88f6ba9a342893af02241139","modified":1613857956441},{"_id":"public/2017/09/26/vagrant-qemu/virt-manager.png","hash":"f40609ac621553b2fa30352e490aa2965afe8ed4","modified":1613857956441},{"_id":"public/2017/09/23/portainer/portainer.io.png","hash":"de8c189f6a7f62ba0db1cb735c086ff9aefd5e72","modified":1613857956441},{"_id":"public/2017/10/09/ansible-cmdb/screenshot-overview.png","hash":"30fb11235b83ce75d5088ac1ace8201932baf304","modified":1613857956441},{"_id":"public/2017/10/02/vdi2qemu/virt-manager.png","hash":"85a5691d17b5dc3498e4712ed33b50f8cf0984d4","modified":1613857956441},{"_id":"public/2017/09/29/nifiparcel/parcel03.png","hash":"ae828f913ead1e383183317d61b939a8a933be4d","modified":1613857956441},{"_id":"public/2017/10/13/drelephat/DrElephant-2-dashboard.jpg","hash":"677510544f5d9cbc0b123f3fe5d4629efc418b5d","modified":1613857956441},{"_id":"themes/hexo-theme-light/source/css/_base/utils.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1613858137371},{"_id":"themes/hexo-theme-light/LICENSE","hash":"c6f301bc722f0af3a55267a36c1c147aeddc6e46","modified":1613858137371},{"_id":"themes/hexo-theme-light/README.md","hash":"3adc0ef8d3d021fb6b98d8426bb6dbd485375caf","modified":1613858137371},{"_id":"themes/hexo-theme-light/_config.yml","hash":"935e2e683887ab222f9479933bf31db76045cafc","modified":1613859095336},{"_id":"themes/hexo-theme-light/languages/de-DE.yml","hash":"e076c7f2eb29ebcfb04d94861bf3063c4b08078c","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/de.yml","hash":"e076c7f2eb29ebcfb04d94861bf3063c4b08078c","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/default.yml","hash":"fd7397be7789b43c1c163ab4faf106318811c2a8","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/en-GB.yml","hash":"fd7397be7789b43c1c163ab4faf106318811c2a8","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/en-US.yml","hash":"fd7397be7789b43c1c163ab4faf106318811c2a8","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/en.yml","hash":"fd7397be7789b43c1c163ab4faf106318811c2a8","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/es-ES.yml","hash":"de273af604b27812cfd4195e7b7f28ceff2734b3","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/es.yml","hash":"de273af604b27812cfd4195e7b7f28ceff2734b3","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/ja-JP.yml","hash":"1511143393fb86819a9d8685ee81c3bbf1e10b23","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/ja.yml","hash":"1511143393fb86819a9d8685ee81c3bbf1e10b23","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/lt-LT.yml","hash":"8826ef5b3911e094f8a118d8db981532d0919bb6","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/lt.yml","hash":"8826ef5b3911e094f8a118d8db981532d0919bb6","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/no.yml","hash":"bf11017d77f64fbafb9c99ac219d076b20d53afc","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/pl-PL.yml","hash":"3f36d08e84a85651bf777cec0752193057c08430","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/pl.yml","hash":"3f36d08e84a85651bf777cec0752193057c08430","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/ru-RU.yml","hash":"35aadf8fdd28aaff8a1c8f50e80201dcf8ce0604","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/ru.yml","hash":"35aadf8fdd28aaff8a1c8f50e80201dcf8ce0604","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/zh-CN.yml","hash":"ca0118e9081b54cc0fca8596660bd6acf4c0308f","modified":1613858137367},{"_id":"themes/hexo-theme-light/languages/zh-TW.yml","hash":"6141b4c7a094c74bd9df7c08908d92b561c1a0c0","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/archive.ejs","hash":"a18842e3d719fe3ca9b977a6995f8facc75c8673","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/category.ejs","hash":"9b740fc33f6f028df60f0bc4312bf3ebd03aa8ea","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/index.ejs","hash":"e569d8fe0741a24efb89e44781f9e616da17e036","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/layout.ejs","hash":"1b4ee853dcd80892ba971954641c0e283ddb2e6e","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/page.ejs","hash":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/post.ejs","hash":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/tag.ejs","hash":"45150a2365768b6b67880193c9264ad2bb4814db","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/style.styl","hash":"c03b2520e4a85b981e29516cadc0a365e6500e3d","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/fancybox/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/fancybox/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/js/gallery.js","hash":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/js/jquery.imagesloaded.min.js","hash":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/_partial/after_footer.ejs","hash":"38ffeacf3eefdbd8134fa3b3f455ff37520fd89e","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/archive.ejs","hash":"92e6fa48d15f2ccf91f589dacc898489a40974ba","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/article.ejs","hash":"115eff85927dcfee01e986571a051a43cfe0d6b6","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/comment.ejs","hash":"be7d9849855f2bb31e626db88b49ac1d87446e21","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/facebook_comment.ejs","hash":"3fdc1d0ce9177825e7417635fbc545a35d528d04","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/footer.ejs","hash":"1deac5914b2fc93b271732fd4d5cbd0a6f78875f","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/google_analytics.ejs","hash":"85785202d60795ce0844daa85ee0c661aabf429e","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/head.ejs","hash":"db2e0e61efe1d6c4bf6eb020983f1d1fdc3d521c","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/header.ejs","hash":"ba97d05f033c98aa3acea060f7e18020ccfe5b7d","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/pagination.ejs","hash":"bca8de52586b7a929abdfef92dad93d98f4ac77f","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/sidebar.ejs","hash":"caf351797a18d03d8ee945ceb9f83785c50c09f9","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_widget/category.ejs","hash":"1b4276f68e6bcab9dbe5538955445b0ad689176a","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/_widget/recent_posts.ejs","hash":"9e299ce56b221fc287828ad0c73e91ae6bba0fb7","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/_widget/search.ejs","hash":"f3600ade452f70bd49cb8cf53f3b6e59015f2ce3","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/_widget/tag.ejs","hash":"06c53c04bd4e3e5924fc50feb97b0319ddc7c834","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/_widget/tagcloud.ejs","hash":"a236c86481196ae43206e056ba78cec14f1ac014","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/_base/layout.styl","hash":"744c51d08ced7369b7fed88f7f51f62b1b6950c5","modified":1613859501550},{"_id":"themes/hexo-theme-light/source/css/_base/variable.styl","hash":"6f3ad13e49634dae8cd992bbd598f5ff0b39a816","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/_partial/archive.styl","hash":"072e9b8c5ee9acf95ac7cce9c34706d41e412229","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/_partial/article.styl","hash":"52b49b09c3a1f1879cee4a6271393e5c797b3e40","modified":1613859973460},{"_id":"themes/hexo-theme-light/source/css/_partial/comment.styl","hash":"e7f8c085bfa8c26afc4b2fbc9f2092f4f07aef34","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/_partial/footer.styl","hash":"1757872dbdbd09295a625f13e356aa798a8bb308","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/_partial/header.styl","hash":"641d6e63bc2d19af2db78f19824289d87ac6353f","modified":1613859632276},{"_id":"themes/hexo-theme-light/source/css/_partial/index.styl","hash":"7a8c0ec6ab99a9f8e00c9687aca29d31752424a2","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/_partial/sidebar.styl","hash":"a8bf5237d7d2fba66988cfb85a3ae218be8709ae","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/_partial/syntax.styl","hash":"e766ae76f993733421c69020a030eb2ae1899895","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/font/fontawesome-webfont.eot","hash":"d775f599ff3f23be082e6a9604b4898718923a37","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/font/fontawesome-webfont.woff","hash":"0612cddf2f835cceffccc88fd194f97367d0b024","modified":1613858137371},{"_id":"themes/hexo-theme-light/layout/_partial/post/category.ejs","hash":"037a6e5d51407205846c6c21bb08e5928f424ef0","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/post/gallery.ejs","hash":"fafc2501d7e65983b0f5c2b58151ca12e57c0574","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/post/share.ejs","hash":"75c5225accbead57245d36f33e29b5ff5af2deca","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/post/tag.ejs","hash":"ec06dba9abac62d6bec84baab0e185467ca0bce1","modified":1613858137367},{"_id":"themes/hexo-theme-light/layout/_partial/post/title.ejs","hash":"fcb98a84fe5beffa3162fa2e12e06dc937a789b6","modified":1613858137367},{"_id":"themes/hexo-theme-light/source/js/jquery-3.4.1.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/font/fontawesome-webfont.svg","hash":"d162419c91b8bab3a4fd327c933a0fcf3799c251","modified":1613858137371},{"_id":"themes/hexo-theme-light/source/css/font/fontawesome-webfont.ttf","hash":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c","modified":1613858137371},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1613858147541},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1613858147541},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1613858147541},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1613858147541},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1613858147541},{"_id":"public/css/font/fontawesome-webfont.eot","hash":"d775f599ff3f23be082e6a9604b4898718923a37","modified":1613858147541},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1613858147541},{"_id":"public/css/font/fontawesome-webfont.woff","hash":"0612cddf2f835cceffccc88fd194f97367d0b024","modified":1613858147541},{"_id":"public/css/font/fontawesome-webfont.ttf","hash":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c","modified":1613858147541},{"_id":"public/css/font/fontawesome-webfont.svg","hash":"d162419c91b8bab3a4fd327c933a0fcf3799c251","modified":1613858147541},{"_id":"public/css/style.css","hash":"e12b642935224fbf0254ae9270f78f2fa2d90bb5","modified":1613858147541},{"_id":"public/js/gallery.js","hash":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1613858147541},{"_id":"public/fancybox/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1613858147541},{"_id":"public/js/jquery.imagesloaded.min.js","hash":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1613858147541},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1613858147541},{"_id":"public/js/jquery-3.4.1.min.js","hash":"88523924351bac0b5d560fe0c5781e2556e7693d","modified":1613858147541}],"Category":[],"Data":[],"Page":[{"title":"About","date":"2017-09-07T00:27:03.000Z","_content":"\n\n# Rui Ramos\n\n* **Name:** Rui Ramos\n* **Role:** BigData Engineer\n* **Location:** Porto Area, Portugal\n* **Email:** rui.ms.ramos(at)gmail.com\n\n-------------------------------------------------\n\n* LinkedIn: https://www.linkedin.com/in/ruimsramos\n* [CV](https://rramos.github.io/about/index/ruiramos.pdf)\n\n","source":"about/index.md","raw":"---\ntitle: About\ndate: 2017-09-07 01:27:03\n---\n\n\n# Rui Ramos\n\n* **Name:** Rui Ramos\n* **Role:** BigData Engineer\n* **Location:** Porto Area, Portugal\n* **Email:** rui.ms.ramos(at)gmail.com\n\n-------------------------------------------------\n\n* LinkedIn: https://www.linkedin.com/in/ruimsramos\n* [CV](https://rramos.github.io/about/index/ruiramos.pdf)\n\n","updated":"2020-09-06T14:38:37.863Z","path":"about/index.html","comments":1,"layout":"page","_id":"ckle9cxj70000chpf2jynh833","content":"<h1 id=\"Rui-Ramos\"><a href=\"#Rui-Ramos\" class=\"headerlink\" title=\"Rui Ramos\"></a>Rui Ramos</h1><ul>\n<li><strong>Name:</strong> Rui Ramos</li>\n<li><strong>Role:</strong> BigData Engineer</li>\n<li><strong>Location:</strong> Porto Area, Portugal</li>\n<li><strong>Email:</strong> rui.ms.ramos(at)gmail.com</li>\n</ul>\n<hr>\n<ul>\n<li>LinkedIn: <a href=\"https://www.linkedin.com/in/ruimsramos\">https://www.linkedin.com/in/ruimsramos</a></li>\n<li><a href=\"https://rramos.github.io/about/index/ruiramos.pdf\">CV</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Rui-Ramos\"><a href=\"#Rui-Ramos\" class=\"headerlink\" title=\"Rui Ramos\"></a>Rui Ramos</h1><ul>\n<li><strong>Name:</strong> Rui Ramos</li>\n<li><strong>Role:</strong> BigData Engineer</li>\n<li><strong>Location:</strong> Porto Area, Portugal</li>\n<li><strong>Email:</strong> rui.ms.ramos(at)gmail.com</li>\n</ul>\n<hr>\n<ul>\n<li>LinkedIn: <a href=\"https://www.linkedin.com/in/ruimsramos\">https://www.linkedin.com/in/ruimsramos</a></li>\n<li><a href=\"https://rramos.github.io/about/index/ruiramos.pdf\">CV</a></li>\n</ul>\n"}],"Post":[{"title":"Airflow PoC","_content":"\n\nIn this article i'll describe the procedure to create a [Airflow](https://airflow.incubator.apache.org/) environment running in docker supporting several BigData Operators.\n\nI'll explain how it works, how to setup and try to document my personal thoughts on the software compared to other solutions.\n\n# Introduction\n\nAirflow is a python platform to create, schedule and monitor workflows.\n\n{% blockquote Offical Website Definition%}\n\nUse airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n\n{% endblockquote %}\n\n# PoC architecture\n\nFor this PoC i'm considering the following architetcure:\n\n* 1 docker : Scheduler\n* 1 docker : Worker\n* 1 docker : Flower\n* 1 docker : DB\n* 1 docker : Webserver\n* 1 docker : Metrics exporter \n\nFor a production environment, it should be taken into account a \nbetter HA-Solution.\n\n# Dockers Setup\n\nTODO\n\n# Launch the Environment\n\nTODO\n\n# Create a initial Workflow\n\nTODO\n\n# Operations\n\nTODO\n\n# Tests\n\nTODO\n\n# Conclusion\n\nTODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","source":"_drafts/AirflowPoC.md","raw":"---\ntitle: Airflow PoC\ntags:\n    - HDFS\n    - Airflow\n    - Orchestration\n    - BigData\n    - Hive\n    - Sqoop\n    - Docker\n    - Spark\n    - Metrics\n---\n\n\nIn this article i'll describe the procedure to create a [Airflow](https://airflow.incubator.apache.org/) environment running in docker supporting several BigData Operators.\n\nI'll explain how it works, how to setup and try to document my personal thoughts on the software compared to other solutions.\n\n# Introduction\n\nAirflow is a python platform to create, schedule and monitor workflows.\n\n{% blockquote Offical Website Definition%}\n\nUse airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.\n\n{% endblockquote %}\n\n# PoC architecture\n\nFor this PoC i'm considering the following architetcure:\n\n* 1 docker : Scheduler\n* 1 docker : Worker\n* 1 docker : Flower\n* 1 docker : DB\n* 1 docker : Webserver\n* 1 docker : Metrics exporter \n\nFor a production environment, it should be taken into account a \nbetter HA-Solution.\n\n# Dockers Setup\n\nTODO\n\n# Launch the Environment\n\nTODO\n\n# Create a initial Workflow\n\nTODO\n\n# Operations\n\nTODO\n\n# Tests\n\nTODO\n\n# Conclusion\n\nTODO\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n","slug":"AirflowPoC","published":0,"date":"2020-09-06T14:38:37.851Z","updated":"2020-09-06T14:38:37.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjb0001chpfb2tsdaod","content":"<p>In this article i’ll describe the procedure to create a <a href=\"https://airflow.incubator.apache.org/\">Airflow</a> environment running in docker supporting several BigData Operators.</p>\n<p>I’ll explain how it works, how to setup and try to document my personal thoughts on the software compared to other solutions.</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>Airflow is a python platform to create, schedule and monitor workflows.</p>\n<blockquote><p>Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.</p>\n<footer><strong>Offical Website Definition</strong></footer></blockquote>\n\n<h1 id=\"PoC-architecture\"><a href=\"#PoC-architecture\" class=\"headerlink\" title=\"PoC architecture\"></a>PoC architecture</h1><p>For this PoC i’m considering the following architetcure:</p>\n<ul>\n<li>1 docker : Scheduler</li>\n<li>1 docker : Worker</li>\n<li>1 docker : Flower</li>\n<li>1 docker : DB</li>\n<li>1 docker : Webserver</li>\n<li>1 docker : Metrics exporter </li>\n</ul>\n<p>For a production environment, it should be taken into account a<br>better HA-Solution.</p>\n<h1 id=\"Dockers-Setup\"><a href=\"#Dockers-Setup\" class=\"headerlink\" title=\"Dockers Setup\"></a>Dockers Setup</h1><p>TODO</p>\n<h1 id=\"Launch-the-Environment\"><a href=\"#Launch-the-Environment\" class=\"headerlink\" title=\"Launch the Environment\"></a>Launch the Environment</h1><p>TODO</p>\n<h1 id=\"Create-a-initial-Workflow\"><a href=\"#Create-a-initial-Workflow\" class=\"headerlink\" title=\"Create a initial Workflow\"></a>Create a initial Workflow</h1><p>TODO</p>\n<h1 id=\"Operations\"><a href=\"#Operations\" class=\"headerlink\" title=\"Operations\"></a>Operations</h1><p>TODO</p>\n<h1 id=\"Tests\"><a href=\"#Tests\" class=\"headerlink\" title=\"Tests\"></a>Tests</h1><p>TODO</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>TODO</p>\n","site":{"data":{}},"excerpt":"","more":"<p>In this article i’ll describe the procedure to create a <a href=\"https://airflow.incubator.apache.org/\">Airflow</a> environment running in docker supporting several BigData Operators.</p>\n<p>I’ll explain how it works, how to setup and try to document my personal thoughts on the software compared to other solutions.</p>\n<h1 id=\"Introduction\"><a href=\"#Introduction\" class=\"headerlink\" title=\"Introduction\"></a>Introduction</h1><p>Airflow is a python platform to create, schedule and monitor workflows.</p>\n<blockquote><p>Use airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed.</p>\n<footer><strong>Offical Website Definition</strong></footer></blockquote>\n\n<h1 id=\"PoC-architecture\"><a href=\"#PoC-architecture\" class=\"headerlink\" title=\"PoC architecture\"></a>PoC architecture</h1><p>For this PoC i’m considering the following architetcure:</p>\n<ul>\n<li>1 docker : Scheduler</li>\n<li>1 docker : Worker</li>\n<li>1 docker : Flower</li>\n<li>1 docker : DB</li>\n<li>1 docker : Webserver</li>\n<li>1 docker : Metrics exporter </li>\n</ul>\n<p>For a production environment, it should be taken into account a<br>better HA-Solution.</p>\n<h1 id=\"Dockers-Setup\"><a href=\"#Dockers-Setup\" class=\"headerlink\" title=\"Dockers Setup\"></a>Dockers Setup</h1><p>TODO</p>\n<h1 id=\"Launch-the-Environment\"><a href=\"#Launch-the-Environment\" class=\"headerlink\" title=\"Launch the Environment\"></a>Launch the Environment</h1><p>TODO</p>\n<h1 id=\"Create-a-initial-Workflow\"><a href=\"#Create-a-initial-Workflow\" class=\"headerlink\" title=\"Create a initial Workflow\"></a>Create a initial Workflow</h1><p>TODO</p>\n<h1 id=\"Operations\"><a href=\"#Operations\" class=\"headerlink\" title=\"Operations\"></a>Operations</h1><p>TODO</p>\n<h1 id=\"Tests\"><a href=\"#Tests\" class=\"headerlink\" title=\"Tests\"></a>Tests</h1><p>TODO</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>TODO</p>\n"},{"title":"Alluxio PoC","date":"2017-09-07T22:53:28.000Z","_content":"\n\nSomething that i have in mind for some time is to test [Alluxio](http://www.alluxio.org/) as a distributed storage layer for Spark.\n\nIn this article i'll describe my PoC on this. \n\n# Objective\n\n* Test Alluxio as a Storage layer for Spark\n* Create a docker environment that could be quick to setup\n* Document the procedure and test the solution\n\n# Infrastructure\n\nCreate several dockers to run the tests\n* 1 dockers for HDFS (TBD)\n* 3 dockers for aluxio cluster (TBD)\n\n# Pre-Setup\n\nLet's start by creating a docker with CDH which will simulate our HDFS Infrastructure.\n\nWe could use the provide docker image from Cloudera\n\n```\ndocker pull cloudera/quickstart:latest\n```\n\nLet's create the iamge with docker compose\n\nTODO: ...\n\nTo fireup a Cloudera Quickstart Container\n\n```\ndocker run --hostname=quickstart.cloudera --privileged=true -t -i cloudera/quickstart /usr/bin/docker-quickstart -d\n```\n\n\n# Setup \n\n## Setup Alluxio\n\nTODO\n\n## Setup HDFS\n\nTODO\n\n## Setup Spark\n\nTODO\n\n# Tests\n\nWIP\n\n# References\n\n* https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html\n* http://www.alluxio.org/\n* http://www.alluxio.org/docs/1.5/en/Running-Alluxio-On-Docker.html\n\n","source":"_drafts/alluxio.md","raw":"---\ntitle: Alluxio PoC\ntags:\n  - BigData\n  - Alluxio\n  - HDFS\n  - Distributed Storage\n  - PoC\ndate: 2017-09-07 23:53:28\n---\n\n\nSomething that i have in mind for some time is to test [Alluxio](http://www.alluxio.org/) as a distributed storage layer for Spark.\n\nIn this article i'll describe my PoC on this. \n\n# Objective\n\n* Test Alluxio as a Storage layer for Spark\n* Create a docker environment that could be quick to setup\n* Document the procedure and test the solution\n\n# Infrastructure\n\nCreate several dockers to run the tests\n* 1 dockers for HDFS (TBD)\n* 3 dockers for aluxio cluster (TBD)\n\n# Pre-Setup\n\nLet's start by creating a docker with CDH which will simulate our HDFS Infrastructure.\n\nWe could use the provide docker image from Cloudera\n\n```\ndocker pull cloudera/quickstart:latest\n```\n\nLet's create the iamge with docker compose\n\nTODO: ...\n\nTo fireup a Cloudera Quickstart Container\n\n```\ndocker run --hostname=quickstart.cloudera --privileged=true -t -i cloudera/quickstart /usr/bin/docker-quickstart -d\n```\n\n\n# Setup \n\n## Setup Alluxio\n\nTODO\n\n## Setup HDFS\n\nTODO\n\n## Setup Spark\n\nTODO\n\n# Tests\n\nWIP\n\n# References\n\n* https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html\n* http://www.alluxio.org/\n* http://www.alluxio.org/docs/1.5/en/Running-Alluxio-On-Docker.html\n\n","slug":"alluxio","published":0,"updated":"2020-09-06T14:38:37.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxje0002chpfdv3i3whm","content":"<p>Something that i have in mind for some time is to test <a href=\"http://www.alluxio.org/\">Alluxio</a> as a distributed storage layer for Spark.</p>\n<p>In this article i’ll describe my PoC on this. </p>\n<h1 id=\"Objective\"><a href=\"#Objective\" class=\"headerlink\" title=\"Objective\"></a>Objective</h1><ul>\n<li>Test Alluxio as a Storage layer for Spark</li>\n<li>Create a docker environment that could be quick to setup</li>\n<li>Document the procedure and test the solution</li>\n</ul>\n<h1 id=\"Infrastructure\"><a href=\"#Infrastructure\" class=\"headerlink\" title=\"Infrastructure\"></a>Infrastructure</h1><p>Create several dockers to run the tests</p>\n<ul>\n<li>1 dockers for HDFS (TBD)</li>\n<li>3 dockers for aluxio cluster (TBD)</li>\n</ul>\n<h1 id=\"Pre-Setup\"><a href=\"#Pre-Setup\" class=\"headerlink\" title=\"Pre-Setup\"></a>Pre-Setup</h1><p>Let’s start by creating a docker with CDH which will simulate our HDFS Infrastructure.</p>\n<p>We could use the provide docker image from Cloudera</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull cloudera&#x2F;quickstart:latest</span><br></pre></td></tr></table></figure>\n<p>Let’s create the iamge with docker compose</p>\n<p>TODO: …</p>\n<p>To fireup a Cloudera Quickstart Container</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --hostname&#x3D;quickstart.cloudera --privileged&#x3D;true -t -i cloudera&#x2F;quickstart &#x2F;usr&#x2F;bin&#x2F;docker-quickstart -d</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><h2 id=\"Setup-Alluxio\"><a href=\"#Setup-Alluxio\" class=\"headerlink\" title=\"Setup Alluxio\"></a>Setup Alluxio</h2><p>TODO</p>\n<h2 id=\"Setup-HDFS\"><a href=\"#Setup-HDFS\" class=\"headerlink\" title=\"Setup HDFS\"></a>Setup HDFS</h2><p>TODO</p>\n<h2 id=\"Setup-Spark\"><a href=\"#Setup-Spark\" class=\"headerlink\" title=\"Setup Spark\"></a>Setup Spark</h2><p>TODO</p>\n<h1 id=\"Tests\"><a href=\"#Tests\" class=\"headerlink\" title=\"Tests\"></a>Tests</h1><p>WIP</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html\">https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html</a></li>\n<li><a href=\"http://www.alluxio.org/\">http://www.alluxio.org/</a></li>\n<li><a href=\"http://www.alluxio.org/docs/1.5/en/Running-Alluxio-On-Docker.html\">http://www.alluxio.org/docs/1.5/en/Running-Alluxio-On-Docker.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>Something that i have in mind for some time is to test <a href=\"http://www.alluxio.org/\">Alluxio</a> as a distributed storage layer for Spark.</p>\n<p>In this article i’ll describe my PoC on this. </p>\n<h1 id=\"Objective\"><a href=\"#Objective\" class=\"headerlink\" title=\"Objective\"></a>Objective</h1><ul>\n<li>Test Alluxio as a Storage layer for Spark</li>\n<li>Create a docker environment that could be quick to setup</li>\n<li>Document the procedure and test the solution</li>\n</ul>\n<h1 id=\"Infrastructure\"><a href=\"#Infrastructure\" class=\"headerlink\" title=\"Infrastructure\"></a>Infrastructure</h1><p>Create several dockers to run the tests</p>\n<ul>\n<li>1 dockers for HDFS (TBD)</li>\n<li>3 dockers for aluxio cluster (TBD)</li>\n</ul>\n<h1 id=\"Pre-Setup\"><a href=\"#Pre-Setup\" class=\"headerlink\" title=\"Pre-Setup\"></a>Pre-Setup</h1><p>Let’s start by creating a docker with CDH which will simulate our HDFS Infrastructure.</p>\n<p>We could use the provide docker image from Cloudera</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker pull cloudera&#x2F;quickstart:latest</span><br></pre></td></tr></table></figure>\n<p>Let’s create the iamge with docker compose</p>\n<p>TODO: …</p>\n<p>To fireup a Cloudera Quickstart Container</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">docker run --hostname&#x3D;quickstart.cloudera --privileged&#x3D;true -t -i cloudera&#x2F;quickstart &#x2F;usr&#x2F;bin&#x2F;docker-quickstart -d</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><h2 id=\"Setup-Alluxio\"><a href=\"#Setup-Alluxio\" class=\"headerlink\" title=\"Setup Alluxio\"></a>Setup Alluxio</h2><p>TODO</p>\n<h2 id=\"Setup-HDFS\"><a href=\"#Setup-HDFS\" class=\"headerlink\" title=\"Setup HDFS\"></a>Setup HDFS</h2><p>TODO</p>\n<h2 id=\"Setup-Spark\"><a href=\"#Setup-Spark\" class=\"headerlink\" title=\"Setup Spark\"></a>Setup Spark</h2><p>TODO</p>\n<h1 id=\"Tests\"><a href=\"#Tests\" class=\"headerlink\" title=\"Tests\"></a>Tests</h1><p>WIP</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html\">https://www.cloudera.com/documentation/enterprise/5-6-x/topics/quickstart_docker_container.html</a></li>\n<li><a href=\"http://www.alluxio.org/\">http://www.alluxio.org/</a></li>\n<li><a href=\"http://www.alluxio.org/docs/1.5/en/Running-Alluxio-On-Docker.html\">http://www.alluxio.org/docs/1.5/en/Running-Alluxio-On-Docker.html</a></li>\n</ul>\n"},{"title":"Quick Firing a Spark Enviroment in Docker","_content":"\nIn this article i'll explain a quick way to fireup a [Spark](https://spark.apache.org/) Cluster in dockers. It could be usefull for PoCs other tests you consider.\n\nThis configuration shouldn't be used in prodiction environments.\n\n## Requirements\n\nFor this setup one should have installed:\n\n* Docker\n\n## Architecture\n\n\n\n## Setup\n\n\n## Tests\n\n\n## Integration with Zeppelin\n\n\nCheers,\nRR","source":"_drafts/quicksparkdocker.md","raw":"---\ntitle: Quick Firing a Spark Enviroment in Docker\ntags:\n    - Spark\n    - Docker\n    - PoC\n---\n\nIn this article i'll explain a quick way to fireup a [Spark](https://spark.apache.org/) Cluster in dockers. It could be usefull for PoCs other tests you consider.\n\nThis configuration shouldn't be used in prodiction environments.\n\n## Requirements\n\nFor this setup one should have installed:\n\n* Docker\n\n## Architecture\n\n\n\n## Setup\n\n\n## Tests\n\n\n## Integration with Zeppelin\n\n\nCheers,\nRR","slug":"quicksparkdocker","published":0,"date":"2020-09-06T14:38:37.851Z","updated":"2020-09-06T14:38:37.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjf0003chpf50na27lv","content":"<p>In this article i’ll explain a quick way to fireup a <a href=\"https://spark.apache.org/\">Spark</a> Cluster in dockers. It could be usefull for PoCs other tests you consider.</p>\n<p>This configuration shouldn’t be used in prodiction environments.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><p>For this setup one should have installed:</p>\n<ul>\n<li>Docker</li>\n</ul>\n<h2 id=\"Architecture\"><a href=\"#Architecture\" class=\"headerlink\" title=\"Architecture\"></a>Architecture</h2><h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><h2 id=\"Tests\"><a href=\"#Tests\" class=\"headerlink\" title=\"Tests\"></a>Tests</h2><h2 id=\"Integration-with-Zeppelin\"><a href=\"#Integration-with-Zeppelin\" class=\"headerlink\" title=\"Integration with Zeppelin\"></a>Integration with Zeppelin</h2><p>Cheers,<br>RR</p>\n","site":{"data":{}},"excerpt":"","more":"<p>In this article i’ll explain a quick way to fireup a <a href=\"https://spark.apache.org/\">Spark</a> Cluster in dockers. It could be usefull for PoCs other tests you consider.</p>\n<p>This configuration shouldn’t be used in prodiction environments.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><p>For this setup one should have installed:</p>\n<ul>\n<li>Docker</li>\n</ul>\n<h2 id=\"Architecture\"><a href=\"#Architecture\" class=\"headerlink\" title=\"Architecture\"></a>Architecture</h2><h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><h2 id=\"Tests\"><a href=\"#Tests\" class=\"headerlink\" title=\"Tests\"></a>Tests</h2><h2 id=\"Integration-with-Zeppelin\"><a href=\"#Integration-with-Zeppelin\" class=\"headerlink\" title=\"Integration with Zeppelin\"></a>Integration with Zeppelin</h2><p>Cheers,<br>RR</p>\n"},{"title":"Setting Up Proxy Environments With Chrome","date":"2017-09-13T21:18:10.000Z","_content":"\n\nSomething that keeps bugging me is the need to access different environments in the browser through proxy tunnels, due to security restrictions or lack of routing between your local network and the environment you are working. Very common on Cloud environments or if you are working has a consultant for several clients at the same time.\n\nSo this guide is intended to help on the setup of Google Chrome to use your configured tunnels.\n\n## Requirements\n\n### Configure your local tunnel\n\nYou require a tunnel configuration setup. For Linux i recommend setting this up in your `.ssh/config` file. I'll give some examples, if you have interest on expanding you `./ssh/config` check the next article ill go in depth with it. There are also other tools such as [gSTM](https://sourceforge.net/projects/gstm/) tool.\n\nSo let's begin to create our reverse tunnel\n\n```\nssh -R 19999:localhost:22 sourceuser@remoteip\n```\n\nYou could use any other available localport\n\n### Install plugin in Google Chrome\n\nGo into your google chrome settings and search for **Extensions** .\nI'm using **Proxy SwitchySharp** But you could use any other.\n\nGo to your Extension Options and let's create a profile\n\n* **Name:** mylocaldomain.dev\n* **SOCKS HOST:** localhost\n* **Port:** 19999\n* **SOCKS v5:** (enable)\n\nNow let's create a rule for our local domain in **Switch Rules**\n\nAdd the rule for our domain: \n\n* **Rule Name:** MyLocalDomain\n* **URL Pattern:** *.mylocaldomain.dev\n* **Pattern Type:** Wildcard\n* **ProxyProfile:** mylocaldomain.dev\n\n**NOTE:** Don't forget to active the option **Enable Switch Rules**\n\nAlso don't forget to activate the **Auto Switch Mode** in the extension.\n\nAnd that's it now when you access the browser to domain http://something.mylocaldomain.dev you be using your defined ssh tunnel for it.\n\n\n## Test\n\nLets do a quick test :)\n\n```\nsudo sh -c 'echo \"127.0.0.1  test.mylocaldomain.dev\"  >> /etc/hosts'\nssh -R 19999:localhost:22 localhost\npython -m SimpleHTTPServer 8000\n```\n\nNow access on the browser to http://test.mylocaldomain.dev:8000\n\nYou could now have different profiles configured and autoswitch will do the job for you of choosing the appropriate tunnel.\n\nCheers,\nRR \n\n\n\n\n\n\n","source":"_posts/ChromeProxy.md","raw":"---\ntitle: Setting up Proxy Environments with Chrome\ntags:\n  - GoogleChrome\n  - Customization\n  - Internet\n  - Proxy\n  - Linux\ndate: 2017-09-13 22:18:10\n---\n\n\nSomething that keeps bugging me is the need to access different environments in the browser through proxy tunnels, due to security restrictions or lack of routing between your local network and the environment you are working. Very common on Cloud environments or if you are working has a consultant for several clients at the same time.\n\nSo this guide is intended to help on the setup of Google Chrome to use your configured tunnels.\n\n## Requirements\n\n### Configure your local tunnel\n\nYou require a tunnel configuration setup. For Linux i recommend setting this up in your `.ssh/config` file. I'll give some examples, if you have interest on expanding you `./ssh/config` check the next article ill go in depth with it. There are also other tools such as [gSTM](https://sourceforge.net/projects/gstm/) tool.\n\nSo let's begin to create our reverse tunnel\n\n```\nssh -R 19999:localhost:22 sourceuser@remoteip\n```\n\nYou could use any other available localport\n\n### Install plugin in Google Chrome\n\nGo into your google chrome settings and search for **Extensions** .\nI'm using **Proxy SwitchySharp** But you could use any other.\n\nGo to your Extension Options and let's create a profile\n\n* **Name:** mylocaldomain.dev\n* **SOCKS HOST:** localhost\n* **Port:** 19999\n* **SOCKS v5:** (enable)\n\nNow let's create a rule for our local domain in **Switch Rules**\n\nAdd the rule for our domain: \n\n* **Rule Name:** MyLocalDomain\n* **URL Pattern:** *.mylocaldomain.dev\n* **Pattern Type:** Wildcard\n* **ProxyProfile:** mylocaldomain.dev\n\n**NOTE:** Don't forget to active the option **Enable Switch Rules**\n\nAlso don't forget to activate the **Auto Switch Mode** in the extension.\n\nAnd that's it now when you access the browser to domain http://something.mylocaldomain.dev you be using your defined ssh tunnel for it.\n\n\n## Test\n\nLets do a quick test :)\n\n```\nsudo sh -c 'echo \"127.0.0.1  test.mylocaldomain.dev\"  >> /etc/hosts'\nssh -R 19999:localhost:22 localhost\npython -m SimpleHTTPServer 8000\n```\n\nNow access on the browser to http://test.mylocaldomain.dev:8000\n\nYou could now have different profiles configured and autoswitch will do the job for you of choosing the appropriate tunnel.\n\nCheers,\nRR \n\n\n\n\n\n\n","slug":"ChromeProxy","published":1,"updated":"2020-09-06T14:38:37.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjg0004chpfhiwd26o4","content":"<p>Something that keeps bugging me is the need to access different environments in the browser through proxy tunnels, due to security restrictions or lack of routing between your local network and the environment you are working. Very common on Cloud environments or if you are working has a consultant for several clients at the same time.</p>\n<p>So this guide is intended to help on the setup of Google Chrome to use your configured tunnels.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><h3 id=\"Configure-your-local-tunnel\"><a href=\"#Configure-your-local-tunnel\" class=\"headerlink\" title=\"Configure your local tunnel\"></a>Configure your local tunnel</h3><p>You require a tunnel configuration setup. For Linux i recommend setting this up in your <code>.ssh/config</code> file. I’ll give some examples, if you have interest on expanding you <code>./ssh/config</code> check the next article ill go in depth with it. There are also other tools such as <a href=\"https://sourceforge.net/projects/gstm/\">gSTM</a> tool.</p>\n<p>So let’s begin to create our reverse tunnel</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh -R 19999:localhost:22 sourceuser@remoteip</span><br></pre></td></tr></table></figure>\n<p>You could use any other available localport</p>\n<h3 id=\"Install-plugin-in-Google-Chrome\"><a href=\"#Install-plugin-in-Google-Chrome\" class=\"headerlink\" title=\"Install plugin in Google Chrome\"></a>Install plugin in Google Chrome</h3><p>Go into your google chrome settings and search for <strong>Extensions</strong> .<br>I’m using <strong>Proxy SwitchySharp</strong> But you could use any other.</p>\n<p>Go to your Extension Options and let’s create a profile</p>\n<ul>\n<li><strong>Name:</strong> mylocaldomain.dev</li>\n<li><strong>SOCKS HOST:</strong> localhost</li>\n<li><strong>Port:</strong> 19999</li>\n<li><strong>SOCKS v5:</strong> (enable)</li>\n</ul>\n<p>Now let’s create a rule for our local domain in <strong>Switch Rules</strong></p>\n<p>Add the rule for our domain: </p>\n<ul>\n<li><strong>Rule Name:</strong> MyLocalDomain</li>\n<li><strong>URL Pattern:</strong> *.mylocaldomain.dev</li>\n<li><strong>Pattern Type:</strong> Wildcard</li>\n<li><strong>ProxyProfile:</strong> mylocaldomain.dev</li>\n</ul>\n<p><strong>NOTE:</strong> Don’t forget to active the option <strong>Enable Switch Rules</strong></p>\n<p>Also don’t forget to activate the <strong>Auto Switch Mode</strong> in the extension.</p>\n<p>And that’s it now when you access the browser to domain <a href=\"http://something.mylocaldomain.dev/\">http://something.mylocaldomain.dev</a> you be using your defined ssh tunnel for it.</p>\n<h2 id=\"Test\"><a href=\"#Test\" class=\"headerlink\" title=\"Test\"></a>Test</h2><p>Lets do a quick test :)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo sh -c &#39;echo &quot;127.0.0.1  test.mylocaldomain.dev&quot;  &gt;&gt; &#x2F;etc&#x2F;hosts&#39;</span><br><span class=\"line\">ssh -R 19999:localhost:22 localhost</span><br><span class=\"line\">python -m SimpleHTTPServer 8000</span><br></pre></td></tr></table></figure>\n<p>Now access on the browser to <a href=\"http://test.mylocaldomain.dev:8000/\">http://test.mylocaldomain.dev:8000</a></p>\n<p>You could now have different profiles configured and autoswitch will do the job for you of choosing the appropriate tunnel.</p>\n<p>Cheers,<br>RR </p>\n","site":{"data":{}},"excerpt":"","more":"<p>Something that keeps bugging me is the need to access different environments in the browser through proxy tunnels, due to security restrictions or lack of routing between your local network and the environment you are working. Very common on Cloud environments or if you are working has a consultant for several clients at the same time.</p>\n<p>So this guide is intended to help on the setup of Google Chrome to use your configured tunnels.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><h3 id=\"Configure-your-local-tunnel\"><a href=\"#Configure-your-local-tunnel\" class=\"headerlink\" title=\"Configure your local tunnel\"></a>Configure your local tunnel</h3><p>You require a tunnel configuration setup. For Linux i recommend setting this up in your <code>.ssh/config</code> file. I’ll give some examples, if you have interest on expanding you <code>./ssh/config</code> check the next article ill go in depth with it. There are also other tools such as <a href=\"https://sourceforge.net/projects/gstm/\">gSTM</a> tool.</p>\n<p>So let’s begin to create our reverse tunnel</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ssh -R 19999:localhost:22 sourceuser@remoteip</span><br></pre></td></tr></table></figure>\n<p>You could use any other available localport</p>\n<h3 id=\"Install-plugin-in-Google-Chrome\"><a href=\"#Install-plugin-in-Google-Chrome\" class=\"headerlink\" title=\"Install plugin in Google Chrome\"></a>Install plugin in Google Chrome</h3><p>Go into your google chrome settings and search for <strong>Extensions</strong> .<br>I’m using <strong>Proxy SwitchySharp</strong> But you could use any other.</p>\n<p>Go to your Extension Options and let’s create a profile</p>\n<ul>\n<li><strong>Name:</strong> mylocaldomain.dev</li>\n<li><strong>SOCKS HOST:</strong> localhost</li>\n<li><strong>Port:</strong> 19999</li>\n<li><strong>SOCKS v5:</strong> (enable)</li>\n</ul>\n<p>Now let’s create a rule for our local domain in <strong>Switch Rules</strong></p>\n<p>Add the rule for our domain: </p>\n<ul>\n<li><strong>Rule Name:</strong> MyLocalDomain</li>\n<li><strong>URL Pattern:</strong> *.mylocaldomain.dev</li>\n<li><strong>Pattern Type:</strong> Wildcard</li>\n<li><strong>ProxyProfile:</strong> mylocaldomain.dev</li>\n</ul>\n<p><strong>NOTE:</strong> Don’t forget to active the option <strong>Enable Switch Rules</strong></p>\n<p>Also don’t forget to activate the <strong>Auto Switch Mode</strong> in the extension.</p>\n<p>And that’s it now when you access the browser to domain <a href=\"http://something.mylocaldomain.dev/\">http://something.mylocaldomain.dev</a> you be using your defined ssh tunnel for it.</p>\n<h2 id=\"Test\"><a href=\"#Test\" class=\"headerlink\" title=\"Test\"></a>Test</h2><p>Lets do a quick test :)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo sh -c &#39;echo &quot;127.0.0.1  test.mylocaldomain.dev&quot;  &gt;&gt; &#x2F;etc&#x2F;hosts&#39;</span><br><span class=\"line\">ssh -R 19999:localhost:22 localhost</span><br><span class=\"line\">python -m SimpleHTTPServer 8000</span><br></pre></td></tr></table></figure>\n<p>Now access on the browser to <a href=\"http://test.mylocaldomain.dev:8000/\">http://test.mylocaldomain.dev:8000</a></p>\n<p>You could now have different profiles configured and autoswitch will do the job for you of choosing the appropriate tunnel.</p>\n<p>Cheers,<br>RR </p>\n"},{"title":"Create Avro Tables for Google BigQuery","date":"2017-09-19T21:59:58.000Z","_content":"\n\nThis article describes one way to create tables in BigQuery From HDFS parquet Data.\n\n## Requirements\n\nFor this article assumes the following requisites are meet:\n\n* You have a Google Cloud Platform account\n* You have created a Google Cloud Storage bucket\n* You have HDFS configured\n* You have Parquet data that you could materialize in Hive\n* Must have [bq](https://cloud.google.com/sdk/docs/) util install \n\n## Definitions\n\n* **Hive Parquet Table:** parquet_table\n* **HDFS Parquet Location:** `/user/hive/warehouse/test.db/parquet_table`\n* **Hive Avro Table Name:** avro_table\n* **HDFS Avro Location:** `/user/hive/warehouse/test.db/avro_table`\n\n## Process\n\n1. Create a avro table from parquet data\n2. Copy avro files to GCS\n3. Create Bigquery Table from avro in gs bucket\n\n## Setup\n\n### Create the Avro Table\n\nLet's start by creating the new table based on the existing parquet data\n\n```\nSET hive.exec.compress.output=true;\nSET avro.output.codec=snappy;\n\nCREATE TABLE avro_table STORED AS AVRO\n  AS (SELECT * FROM parquet_table);\n```\n\nOne could specify in the `SELECT` statement the columns we would like that could be obtain using `parquet-tools` command  ex:\n\n```\nparquet-tools meta <parquet_file>\n```\n\nYou would still need to get the parquet file to obtain that.\n\n### Why Avro File and that format ?\n\nYou could update data to BigQuery by streaming or from Google Cloud Storage as a batch process. A bulk import from HDFS seems logical to use a batch process so why avro ?\nAcording to the latest info on the google BigQuery Site it's possible to:\n\n* Load from Google Cloud Storage, including CSV, JSON (newline-delimited), and Avro files, as well as Google Cloud Datastore backups.\n* Load directly from a readable data source.\n* Insert individual records using streaming inserts.\n\n> Compressed Avro files are not supported, but compressed data blocks are. BigQuery supports the DEFLATE and Snappy codecs.\n> \n\nAlso there is the following Avro mapping that could be usefull\n\n|Avro data type|  BigQuery data type |\n|--------------|---------------------|\n|null          | - ignored -         |   \n|boolean       |BOOLEAN              |\n|int           |INTEGER              |\n|long          |INTEGER              |\n|float         |FLOAT                |\n|double        |FLOAT                |\n|bytes         |BYTES                |\n|string        |STRING               |\n|record        |RECORD               |\n|enum          |STRING               |\n|array         | - repeated fields - |\n|map<T>        |RECORD               |\n|union         |RECORD               |\n|fixed         |BYTES                |\n\nCheck the full spec on GCP [Page](https://cloud.google.com/bigquery/data-formats#avro_format)\n\nThe other advantage of using avro is that BigQuery infers the schema so you don't have to describe the columns of you table.\n\n### Copy Avro file from HDFS to GCS\n\nThe best approach for this is to add the GCS connector to your HDFS config\n\nFollow the instructions in the following [link](https://github.com/GoogleCloudPlatform/bigdata-interop/tree/master/gcs) or download the jar for Hadoop 2.x [here](https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar)\n\n1. Add that jar on a valid location for you cluster `HADOOP_CLASSPATH`\n2. Generate a `service account` in the GCP console and get JSON key ([follow this instructions](https://cloud.google.com/storage/docs/authentication#service_accounts))\n3. Copy that JSON file to a location in your cluster\n4. Add the following properties to your cluster `core-site.xml`\n\n```\n  <property>\n    <name>fs.gs.project.id</name>\n    <value>your-project-name</value>\n    <description>\n      Required. Google Cloud Project ID with access to configured GCS buckets.\n    </description>\n  </property>\n  <property>\n  <name>google.cloud.auth.service.account.json.keyfile</name>\n  <value>/path/to/your/JSON-keyfile</value>\n  <description>\n    The JSON key file of the service account used for GCS\n    access when google.cloud.auth.service.account.enable is true.\n  </description>\n  </property>\n    <property>\n    <name>fs.gs.working.dir</name>\n    <value>/</value>\n    <description>\n      The directory relative gs: uris resolve in inside of the default bucket.\n    </description>\n  </property>\n```\n\nExtended options are available in [gcs-core-default](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml) example\n\n5. Create a new bucket on GCP and make sure you can access to with via `hdfs` command.\n\n```\nhdfs dfs -ls gs://my-bucket-name/\n```\n\nIf that works, you can now execute a distcp to sync the avro files directly to GCS.\n\n```\nhdfs mkdir gs://my-bucket-name/my_table\nhdfs distcp /user/hive/warehouse/test.db/avro_table/* gs://my-bucket-name/my_table/\n```\n\n### Load GCS avro data to a BigQuery table\n\nExecute\n\n```\nbq load ds.table gs://my-bucket-name/my_table/* --autodetect\n```\n\nAnd that's it you now have a table with data in BigQuery.\n\nIt is recommended to have this process managed by some type of orchestrator. There are several solutions for this. The next article i'll be writting  about one of those [Airflow](https://airflow.incubator.apache.org/)\n\nCheers,\nRR\n\n## References\n\n* https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive.html\n* https://github.com/apache/parquet-mr/tree/master/parquet-tools\n* https://cloud.google.com/bigquery/loading-data\n* https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md\n* https://cloud.google.com/bigquery/docs/loading-data-cloud-storage","source":"_posts/CreateAvroTablesForBigQuery.md","raw":"---\ntitle: Create Avro Tables For Google BigQuery\ntags:\n  - Hive\n  - BigQuery\n  - Avro\n  - HDFS\n  - Parquet\n  - GCS\ndate: 2017-09-19 22:59:58\n---\n\n\nThis article describes one way to create tables in BigQuery From HDFS parquet Data.\n\n## Requirements\n\nFor this article assumes the following requisites are meet:\n\n* You have a Google Cloud Platform account\n* You have created a Google Cloud Storage bucket\n* You have HDFS configured\n* You have Parquet data that you could materialize in Hive\n* Must have [bq](https://cloud.google.com/sdk/docs/) util install \n\n## Definitions\n\n* **Hive Parquet Table:** parquet_table\n* **HDFS Parquet Location:** `/user/hive/warehouse/test.db/parquet_table`\n* **Hive Avro Table Name:** avro_table\n* **HDFS Avro Location:** `/user/hive/warehouse/test.db/avro_table`\n\n## Process\n\n1. Create a avro table from parquet data\n2. Copy avro files to GCS\n3. Create Bigquery Table from avro in gs bucket\n\n## Setup\n\n### Create the Avro Table\n\nLet's start by creating the new table based on the existing parquet data\n\n```\nSET hive.exec.compress.output=true;\nSET avro.output.codec=snappy;\n\nCREATE TABLE avro_table STORED AS AVRO\n  AS (SELECT * FROM parquet_table);\n```\n\nOne could specify in the `SELECT` statement the columns we would like that could be obtain using `parquet-tools` command  ex:\n\n```\nparquet-tools meta <parquet_file>\n```\n\nYou would still need to get the parquet file to obtain that.\n\n### Why Avro File and that format ?\n\nYou could update data to BigQuery by streaming or from Google Cloud Storage as a batch process. A bulk import from HDFS seems logical to use a batch process so why avro ?\nAcording to the latest info on the google BigQuery Site it's possible to:\n\n* Load from Google Cloud Storage, including CSV, JSON (newline-delimited), and Avro files, as well as Google Cloud Datastore backups.\n* Load directly from a readable data source.\n* Insert individual records using streaming inserts.\n\n> Compressed Avro files are not supported, but compressed data blocks are. BigQuery supports the DEFLATE and Snappy codecs.\n> \n\nAlso there is the following Avro mapping that could be usefull\n\n|Avro data type|  BigQuery data type |\n|--------------|---------------------|\n|null          | - ignored -         |   \n|boolean       |BOOLEAN              |\n|int           |INTEGER              |\n|long          |INTEGER              |\n|float         |FLOAT                |\n|double        |FLOAT                |\n|bytes         |BYTES                |\n|string        |STRING               |\n|record        |RECORD               |\n|enum          |STRING               |\n|array         | - repeated fields - |\n|map<T>        |RECORD               |\n|union         |RECORD               |\n|fixed         |BYTES                |\n\nCheck the full spec on GCP [Page](https://cloud.google.com/bigquery/data-formats#avro_format)\n\nThe other advantage of using avro is that BigQuery infers the schema so you don't have to describe the columns of you table.\n\n### Copy Avro file from HDFS to GCS\n\nThe best approach for this is to add the GCS connector to your HDFS config\n\nFollow the instructions in the following [link](https://github.com/GoogleCloudPlatform/bigdata-interop/tree/master/gcs) or download the jar for Hadoop 2.x [here](https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar)\n\n1. Add that jar on a valid location for you cluster `HADOOP_CLASSPATH`\n2. Generate a `service account` in the GCP console and get JSON key ([follow this instructions](https://cloud.google.com/storage/docs/authentication#service_accounts))\n3. Copy that JSON file to a location in your cluster\n4. Add the following properties to your cluster `core-site.xml`\n\n```\n  <property>\n    <name>fs.gs.project.id</name>\n    <value>your-project-name</value>\n    <description>\n      Required. Google Cloud Project ID with access to configured GCS buckets.\n    </description>\n  </property>\n  <property>\n  <name>google.cloud.auth.service.account.json.keyfile</name>\n  <value>/path/to/your/JSON-keyfile</value>\n  <description>\n    The JSON key file of the service account used for GCS\n    access when google.cloud.auth.service.account.enable is true.\n  </description>\n  </property>\n    <property>\n    <name>fs.gs.working.dir</name>\n    <value>/</value>\n    <description>\n      The directory relative gs: uris resolve in inside of the default bucket.\n    </description>\n  </property>\n```\n\nExtended options are available in [gcs-core-default](https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml) example\n\n5. Create a new bucket on GCP and make sure you can access to with via `hdfs` command.\n\n```\nhdfs dfs -ls gs://my-bucket-name/\n```\n\nIf that works, you can now execute a distcp to sync the avro files directly to GCS.\n\n```\nhdfs mkdir gs://my-bucket-name/my_table\nhdfs distcp /user/hive/warehouse/test.db/avro_table/* gs://my-bucket-name/my_table/\n```\n\n### Load GCS avro data to a BigQuery table\n\nExecute\n\n```\nbq load ds.table gs://my-bucket-name/my_table/* --autodetect\n```\n\nAnd that's it you now have a table with data in BigQuery.\n\nIt is recommended to have this process managed by some type of orchestrator. There are several solutions for this. The next article i'll be writting  about one of those [Airflow](https://airflow.incubator.apache.org/)\n\nCheers,\nRR\n\n## References\n\n* https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive.html\n* https://github.com/apache/parquet-mr/tree/master/parquet-tools\n* https://cloud.google.com/bigquery/loading-data\n* https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md\n* https://cloud.google.com/bigquery/docs/loading-data-cloud-storage","slug":"CreateAvroTablesForBigQuery","published":1,"updated":"2020-09-06T14:38:37.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjg0005chpf654pf8ch","content":"<p>This article describes one way to create tables in BigQuery From HDFS parquet Data.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><p>For this article assumes the following requisites are meet:</p>\n<ul>\n<li>You have a Google Cloud Platform account</li>\n<li>You have created a Google Cloud Storage bucket</li>\n<li>You have HDFS configured</li>\n<li>You have Parquet data that you could materialize in Hive</li>\n<li>Must have <a href=\"https://cloud.google.com/sdk/docs/\">bq</a> util install </li>\n</ul>\n<h2 id=\"Definitions\"><a href=\"#Definitions\" class=\"headerlink\" title=\"Definitions\"></a>Definitions</h2><ul>\n<li><strong>Hive Parquet Table:</strong> parquet_table</li>\n<li><strong>HDFS Parquet Location:</strong> <code>/user/hive/warehouse/test.db/parquet_table</code></li>\n<li><strong>Hive Avro Table Name:</strong> avro_table</li>\n<li><strong>HDFS Avro Location:</strong> <code>/user/hive/warehouse/test.db/avro_table</code></li>\n</ul>\n<h2 id=\"Process\"><a href=\"#Process\" class=\"headerlink\" title=\"Process\"></a>Process</h2><ol>\n<li>Create a avro table from parquet data</li>\n<li>Copy avro files to GCS</li>\n<li>Create Bigquery Table from avro in gs bucket</li>\n</ol>\n<h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><h3 id=\"Create-the-Avro-Table\"><a href=\"#Create-the-Avro-Table\" class=\"headerlink\" title=\"Create the Avro Table\"></a>Create the Avro Table</h3><p>Let’s start by creating the new table based on the existing parquet data</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET hive.exec.compress.output&#x3D;true;</span><br><span class=\"line\">SET avro.output.codec&#x3D;snappy;</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE avro_table STORED AS AVRO</span><br><span class=\"line\">  AS (SELECT * FROM parquet_table);</span><br></pre></td></tr></table></figure>\n<p>One could specify in the <code>SELECT</code> statement the columns we would like that could be obtain using <code>parquet-tools</code> command  ex:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">parquet-tools meta &lt;parquet_file&gt;</span><br></pre></td></tr></table></figure>\n<p>You would still need to get the parquet file to obtain that.</p>\n<h3 id=\"Why-Avro-File-and-that-format\"><a href=\"#Why-Avro-File-and-that-format\" class=\"headerlink\" title=\"Why Avro File and that format ?\"></a>Why Avro File and that format ?</h3><p>You could update data to BigQuery by streaming or from Google Cloud Storage as a batch process. A bulk import from HDFS seems logical to use a batch process so why avro ?<br>Acording to the latest info on the google BigQuery Site it’s possible to:</p>\n<ul>\n<li>Load from Google Cloud Storage, including CSV, JSON (newline-delimited), and Avro files, as well as Google Cloud Datastore backups.</li>\n<li>Load directly from a readable data source.</li>\n<li>Insert individual records using streaming inserts.</li>\n</ul>\n<blockquote>\n<p>Compressed Avro files are not supported, but compressed data blocks are. BigQuery supports the DEFLATE and Snappy codecs.</p>\n</blockquote>\n<p>Also there is the following Avro mapping that could be usefull</p>\n<table>\n<thead>\n<tr>\n<th>Avro data type</th>\n<th>BigQuery data type</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>null</td>\n<td>- ignored -</td>\n</tr>\n<tr>\n<td>boolean</td>\n<td>BOOLEAN</td>\n</tr>\n<tr>\n<td>int</td>\n<td>INTEGER</td>\n</tr>\n<tr>\n<td>long</td>\n<td>INTEGER</td>\n</tr>\n<tr>\n<td>float</td>\n<td>FLOAT</td>\n</tr>\n<tr>\n<td>double</td>\n<td>FLOAT</td>\n</tr>\n<tr>\n<td>bytes</td>\n<td>BYTES</td>\n</tr>\n<tr>\n<td>string</td>\n<td>STRING</td>\n</tr>\n<tr>\n<td>record</td>\n<td>RECORD</td>\n</tr>\n<tr>\n<td>enum</td>\n<td>STRING</td>\n</tr>\n<tr>\n<td>array</td>\n<td>- repeated fields -</td>\n</tr>\n<tr>\n<td>map<T></td>\n<td>RECORD</td>\n</tr>\n<tr>\n<td>union</td>\n<td>RECORD</td>\n</tr>\n<tr>\n<td>fixed</td>\n<td>BYTES</td>\n</tr>\n</tbody></table>\n<p>Check the full spec on GCP <a href=\"https://cloud.google.com/bigquery/data-formats#avro_format\">Page</a></p>\n<p>The other advantage of using avro is that BigQuery infers the schema so you don’t have to describe the columns of you table.</p>\n<h3 id=\"Copy-Avro-file-from-HDFS-to-GCS\"><a href=\"#Copy-Avro-file-from-HDFS-to-GCS\" class=\"headerlink\" title=\"Copy Avro file from HDFS to GCS\"></a>Copy Avro file from HDFS to GCS</h3><p>The best approach for this is to add the GCS connector to your HDFS config</p>\n<p>Follow the instructions in the following <a href=\"https://github.com/GoogleCloudPlatform/bigdata-interop/tree/master/gcs\">link</a> or download the jar for Hadoop 2.x <a href=\"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar\">here</a></p>\n<ol>\n<li>Add that jar on a valid location for you cluster <code>HADOOP_CLASSPATH</code></li>\n<li>Generate a <code>service account</code> in the GCP console and get JSON key (<a href=\"https://cloud.google.com/storage/docs/authentication#service_accounts\">follow this instructions</a>)</li>\n<li>Copy that JSON file to a location in your cluster</li>\n<li>Add the following properties to your cluster <code>core-site.xml</code></li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;fs.gs.project.id&lt;&#x2F;name&gt;</span><br><span class=\"line\">  &lt;value&gt;your-project-name&lt;&#x2F;value&gt;</span><br><span class=\"line\">  &lt;description&gt;</span><br><span class=\"line\">    Required. Google Cloud Project ID with access to configured GCS buckets.</span><br><span class=\"line\">  &lt;&#x2F;description&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">&lt;name&gt;google.cloud.auth.service.account.json.keyfile&lt;&#x2F;name&gt;</span><br><span class=\"line\">&lt;value&gt;&#x2F;path&#x2F;to&#x2F;your&#x2F;JSON-keyfile&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;description&gt;</span><br><span class=\"line\">  The JSON key file of the service account used for GCS</span><br><span class=\"line\">  access when google.cloud.auth.service.account.enable is true.</span><br><span class=\"line\">&lt;&#x2F;description&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;fs.gs.working.dir&lt;&#x2F;name&gt;</span><br><span class=\"line\">  &lt;value&gt;&#x2F;&lt;&#x2F;value&gt;</span><br><span class=\"line\">  &lt;description&gt;</span><br><span class=\"line\">    The directory relative gs: uris resolve in inside of the default bucket.</span><br><span class=\"line\">  &lt;&#x2F;description&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>\n<p>Extended options are available in <a href=\"https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml\">gcs-core-default</a> example</p>\n<ol start=\"5\">\n<li>Create a new bucket on GCP and make sure you can access to with via <code>hdfs</code> command.</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs dfs -ls gs:&#x2F;&#x2F;my-bucket-name&#x2F;</span><br></pre></td></tr></table></figure>\n<p>If that works, you can now execute a distcp to sync the avro files directly to GCS.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs mkdir gs:&#x2F;&#x2F;my-bucket-name&#x2F;my_table</span><br><span class=\"line\">hdfs distcp &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;avro_table&#x2F;* gs:&#x2F;&#x2F;my-bucket-name&#x2F;my_table&#x2F;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Load-GCS-avro-data-to-a-BigQuery-table\"><a href=\"#Load-GCS-avro-data-to-a-BigQuery-table\" class=\"headerlink\" title=\"Load GCS avro data to a BigQuery table\"></a>Load GCS avro data to a BigQuery table</h3><p>Execute</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bq load ds.table gs:&#x2F;&#x2F;my-bucket-name&#x2F;my_table&#x2F;* --autodetect</span><br></pre></td></tr></table></figure>\n<p>And that’s it you now have a table with data in BigQuery.</p>\n<p>It is recommended to have this process managed by some type of orchestrator. There are several solutions for this. The next article i’ll be writting  about one of those <a href=\"https://airflow.incubator.apache.org/\">Airflow</a></p>\n<p>Cheers,<br>RR</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive.html\">https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive.html</a></li>\n<li><a href=\"https://github.com/apache/parquet-mr/tree/master/parquet-tools\">https://github.com/apache/parquet-mr/tree/master/parquet-tools</a></li>\n<li><a href=\"https://cloud.google.com/bigquery/loading-data\">https://cloud.google.com/bigquery/loading-data</a></li>\n<li><a href=\"https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md\">https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md</a></li>\n<li><a href=\"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage\">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>This article describes one way to create tables in BigQuery From HDFS parquet Data.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><p>For this article assumes the following requisites are meet:</p>\n<ul>\n<li>You have a Google Cloud Platform account</li>\n<li>You have created a Google Cloud Storage bucket</li>\n<li>You have HDFS configured</li>\n<li>You have Parquet data that you could materialize in Hive</li>\n<li>Must have <a href=\"https://cloud.google.com/sdk/docs/\">bq</a> util install </li>\n</ul>\n<h2 id=\"Definitions\"><a href=\"#Definitions\" class=\"headerlink\" title=\"Definitions\"></a>Definitions</h2><ul>\n<li><strong>Hive Parquet Table:</strong> parquet_table</li>\n<li><strong>HDFS Parquet Location:</strong> <code>/user/hive/warehouse/test.db/parquet_table</code></li>\n<li><strong>Hive Avro Table Name:</strong> avro_table</li>\n<li><strong>HDFS Avro Location:</strong> <code>/user/hive/warehouse/test.db/avro_table</code></li>\n</ul>\n<h2 id=\"Process\"><a href=\"#Process\" class=\"headerlink\" title=\"Process\"></a>Process</h2><ol>\n<li>Create a avro table from parquet data</li>\n<li>Copy avro files to GCS</li>\n<li>Create Bigquery Table from avro in gs bucket</li>\n</ol>\n<h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><h3 id=\"Create-the-Avro-Table\"><a href=\"#Create-the-Avro-Table\" class=\"headerlink\" title=\"Create the Avro Table\"></a>Create the Avro Table</h3><p>Let’s start by creating the new table based on the existing parquet data</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SET hive.exec.compress.output&#x3D;true;</span><br><span class=\"line\">SET avro.output.codec&#x3D;snappy;</span><br><span class=\"line\"></span><br><span class=\"line\">CREATE TABLE avro_table STORED AS AVRO</span><br><span class=\"line\">  AS (SELECT * FROM parquet_table);</span><br></pre></td></tr></table></figure>\n<p>One could specify in the <code>SELECT</code> statement the columns we would like that could be obtain using <code>parquet-tools</code> command  ex:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">parquet-tools meta &lt;parquet_file&gt;</span><br></pre></td></tr></table></figure>\n<p>You would still need to get the parquet file to obtain that.</p>\n<h3 id=\"Why-Avro-File-and-that-format\"><a href=\"#Why-Avro-File-and-that-format\" class=\"headerlink\" title=\"Why Avro File and that format ?\"></a>Why Avro File and that format ?</h3><p>You could update data to BigQuery by streaming or from Google Cloud Storage as a batch process. A bulk import from HDFS seems logical to use a batch process so why avro ?<br>Acording to the latest info on the google BigQuery Site it’s possible to:</p>\n<ul>\n<li>Load from Google Cloud Storage, including CSV, JSON (newline-delimited), and Avro files, as well as Google Cloud Datastore backups.</li>\n<li>Load directly from a readable data source.</li>\n<li>Insert individual records using streaming inserts.</li>\n</ul>\n<blockquote>\n<p>Compressed Avro files are not supported, but compressed data blocks are. BigQuery supports the DEFLATE and Snappy codecs.</p>\n</blockquote>\n<p>Also there is the following Avro mapping that could be usefull</p>\n<table>\n<thead>\n<tr>\n<th>Avro data type</th>\n<th>BigQuery data type</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>null</td>\n<td>- ignored -</td>\n</tr>\n<tr>\n<td>boolean</td>\n<td>BOOLEAN</td>\n</tr>\n<tr>\n<td>int</td>\n<td>INTEGER</td>\n</tr>\n<tr>\n<td>long</td>\n<td>INTEGER</td>\n</tr>\n<tr>\n<td>float</td>\n<td>FLOAT</td>\n</tr>\n<tr>\n<td>double</td>\n<td>FLOAT</td>\n</tr>\n<tr>\n<td>bytes</td>\n<td>BYTES</td>\n</tr>\n<tr>\n<td>string</td>\n<td>STRING</td>\n</tr>\n<tr>\n<td>record</td>\n<td>RECORD</td>\n</tr>\n<tr>\n<td>enum</td>\n<td>STRING</td>\n</tr>\n<tr>\n<td>array</td>\n<td>- repeated fields -</td>\n</tr>\n<tr>\n<td>map<T></td>\n<td>RECORD</td>\n</tr>\n<tr>\n<td>union</td>\n<td>RECORD</td>\n</tr>\n<tr>\n<td>fixed</td>\n<td>BYTES</td>\n</tr>\n</tbody></table>\n<p>Check the full spec on GCP <a href=\"https://cloud.google.com/bigquery/data-formats#avro_format\">Page</a></p>\n<p>The other advantage of using avro is that BigQuery infers the schema so you don’t have to describe the columns of you table.</p>\n<h3 id=\"Copy-Avro-file-from-HDFS-to-GCS\"><a href=\"#Copy-Avro-file-from-HDFS-to-GCS\" class=\"headerlink\" title=\"Copy Avro file from HDFS to GCS\"></a>Copy Avro file from HDFS to GCS</h3><p>The best approach for this is to add the GCS connector to your HDFS config</p>\n<p>Follow the instructions in the following <a href=\"https://github.com/GoogleCloudPlatform/bigdata-interop/tree/master/gcs\">link</a> or download the jar for Hadoop 2.x <a href=\"https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-latest-hadoop2.jar\">here</a></p>\n<ol>\n<li>Add that jar on a valid location for you cluster <code>HADOOP_CLASSPATH</code></li>\n<li>Generate a <code>service account</code> in the GCP console and get JSON key (<a href=\"https://cloud.google.com/storage/docs/authentication#service_accounts\">follow this instructions</a>)</li>\n<li>Copy that JSON file to a location in your cluster</li>\n<li>Add the following properties to your cluster <code>core-site.xml</code></li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;fs.gs.project.id&lt;&#x2F;name&gt;</span><br><span class=\"line\">  &lt;value&gt;your-project-name&lt;&#x2F;value&gt;</span><br><span class=\"line\">  &lt;description&gt;</span><br><span class=\"line\">    Required. Google Cloud Project ID with access to configured GCS buckets.</span><br><span class=\"line\">  &lt;&#x2F;description&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">&lt;property&gt;</span><br><span class=\"line\">&lt;name&gt;google.cloud.auth.service.account.json.keyfile&lt;&#x2F;name&gt;</span><br><span class=\"line\">&lt;value&gt;&#x2F;path&#x2F;to&#x2F;your&#x2F;JSON-keyfile&lt;&#x2F;value&gt;</span><br><span class=\"line\">&lt;description&gt;</span><br><span class=\"line\">  The JSON key file of the service account used for GCS</span><br><span class=\"line\">  access when google.cloud.auth.service.account.enable is true.</span><br><span class=\"line\">&lt;&#x2F;description&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br><span class=\"line\">  &lt;property&gt;</span><br><span class=\"line\">  &lt;name&gt;fs.gs.working.dir&lt;&#x2F;name&gt;</span><br><span class=\"line\">  &lt;value&gt;&#x2F;&lt;&#x2F;value&gt;</span><br><span class=\"line\">  &lt;description&gt;</span><br><span class=\"line\">    The directory relative gs: uris resolve in inside of the default bucket.</span><br><span class=\"line\">  &lt;&#x2F;description&gt;</span><br><span class=\"line\">&lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>\n<p>Extended options are available in <a href=\"https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/conf/gcs-core-default.xml\">gcs-core-default</a> example</p>\n<ol start=\"5\">\n<li>Create a new bucket on GCP and make sure you can access to with via <code>hdfs</code> command.</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs dfs -ls gs:&#x2F;&#x2F;my-bucket-name&#x2F;</span><br></pre></td></tr></table></figure>\n<p>If that works, you can now execute a distcp to sync the avro files directly to GCS.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hdfs mkdir gs:&#x2F;&#x2F;my-bucket-name&#x2F;my_table</span><br><span class=\"line\">hdfs distcp &#x2F;user&#x2F;hive&#x2F;warehouse&#x2F;test.db&#x2F;avro_table&#x2F;* gs:&#x2F;&#x2F;my-bucket-name&#x2F;my_table&#x2F;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Load-GCS-avro-data-to-a-BigQuery-table\"><a href=\"#Load-GCS-avro-data-to-a-BigQuery-table\" class=\"headerlink\" title=\"Load GCS avro data to a BigQuery table\"></a>Load GCS avro data to a BigQuery table</h3><p>Execute</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">bq load ds.table gs:&#x2F;&#x2F;my-bucket-name&#x2F;my_table&#x2F;* --autodetect</span><br></pre></td></tr></table></figure>\n<p>And that’s it you now have a table with data in BigQuery.</p>\n<p>It is recommended to have this process managed by some type of orchestrator. There are several solutions for this. The next article i’ll be writting  about one of those <a href=\"https://airflow.incubator.apache.org/\">Airflow</a></p>\n<p>Cheers,<br>RR</p>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive.html\">https://www.cloudera.com/documentation/enterprise/5-4-x/topics/cdh_ig_hive.html</a></li>\n<li><a href=\"https://github.com/apache/parquet-mr/tree/master/parquet-tools\">https://github.com/apache/parquet-mr/tree/master/parquet-tools</a></li>\n<li><a href=\"https://cloud.google.com/bigquery/loading-data\">https://cloud.google.com/bigquery/loading-data</a></li>\n<li><a href=\"https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md\">https://github.com/GoogleCloudPlatform/bigdata-interop/blob/master/gcs/INSTALL.md</a></li>\n<li><a href=\"https://cloud.google.com/bigquery/docs/loading-data-cloud-storage\">https://cloud.google.com/bigquery/docs/loading-data-cloud-storage</a></li>\n</ul>\n"},{"title":"Created rramos.github.io","date":"2016-09-17T22:57:27.000Z","comments":1,"_content":"# Started the github pages \n\n## Intro\n\nI had several blogs in the past that i normaly tend to ignore and stop writting, so this one will be a simple tech/personal blog with entries that i might find usefull in the future to search quicker.\n\nAnd as any normal blog i would start by writting the procedure of deploying the blog itself.\n\nThe only way i would keep this thing updated is if i have very quick way to edit via terminal a Markdown file and do a command like `post-it` and that's it.\n\nSo [hexo](https://hexo.io/) seems like a good solution for me.\n\nGona try this one out.\n\nSo first things first, hosting. I'm cheap bastard so i don't have cool domain yet, and i just need some git repo for the blog.\n\nLets stick with [GitHub Pages](https://pages.github.com/) there git service is pretty owesome so lets try this one out.\n\n**Mental-Note-To-Self:** Get some nice english corrector ;) \n\nSo i've just greated a new repo in github and from the settings i've selected to build a standard GitHub Page.\n\ncloned the dam thing to my computer\n\n`git clone git@github.com:rramos/rramos.github.io.git`\n\n## Installing Hexo\n\nSo first things first, let's install the requirements.\n\nIt requires:\n* Node\n* Git\n\nWell, git i allready have since a clonned the repo. \nNode must be installed, but since i'm using Ubuntu Xenial i need to install the legacy one.\n\n```\nsudo apt install nodejs-legacy\n```\n\nNow starting to install hexo via npm.\n\n```\nsudo npm install -g hexo-cli\nsudo npm install hexo-deployer-git --save\n```\n\nOk that's it.\n\n## Start writting\n\nSo i've run the following command on the repo, this initiates the hexo page structure.\n\n```\nhexo init\n```\n\nI've added the deploy configuration in _config.yml\n\n```\ndeploy:\n  type: git\n  repo: git@github.com:rramos/rramos.github.io.git\n  branch: master\n```\n\nI also downloaded a very simple theme, cas'm a simple guy. \n\n```\ncd themes\ngit clone https://github.com/lotabout/very-simple\n```\nThanks @lotabout for the theme by the way.\n\nAnd installed the theme requirements\n\n```\ngit clone https://github.com/lotabout/very-simple themes/very-simple\nsudo npm install hexo-renderer-sass --save\nsudo npm install hexo-renderer-jade --save\n```\n\n## Writing\n\nSo ... now i suppose i can start creating blog entries, let's start.\n\n```\nhexo new post \"Created rramos.github.io\"\n```\n\nI get the output where the md file is and start writting there.\n\nAfter i have made sure my ssh keys where registered on github i simply deployed with\n\n```\nhexo deploy\n```\n\nAnd voilá: https://rramos.github.io  is up and running.\n\n## Final touches \n\nSo hexo only dumps the public part of the blog which makes sense to the git repo. ~~But i want to include all the source and not have separeted repositories for that.~~\n\nI've created a src dir and copy all the source data there, now i can edit directly that repo, let's take that advantage and remove the default hello world page from the structure guess there migh exist a command for that.\n\n```\n$ hexo list post\nINFO  Start processing\nDate        Title                     Path                                Category  Tags\n2016-09-17  Created rramos.github.io  _posts/Created-rramos-github-io.md\n2016-09-18  Hello World               _posts/hello-world.md\n```\n\nThere you are you bastard hello-world, lets get ride of you.\n\n```\nhexo clean\nrm source/_posts/hello-world.md\nhexo generate\n```\n\nWell guess that's it. There is a lot to explore in hexo from what i can tell, need to check the documentation and understand the community envolvment. Also a quick way to add imagens and other objects and define the quick way to deploy.\n\n## Seperate public from source\n\nIt turns out that have the source and public data in the same repo causes some issues on the updates. The best aproach seems to have a separate repo for the source data and the offcial Github Pages or other hosting service with git for the public part.\n\nI've also added in this source repo a `.gitignore` with\n\n```\npublic\ndb.json\n.deploy_git\n```\n\nSo now i just have to edit MarkDown hexo generate and hexo deploy, seems quick enough let's see if this time i can keep this updated.\n\nCheers,\nRR\n","source":"_posts/Created-rramos-github-io.md","raw":"---\ntitle: Created rramos.github.io\ndate: 2016-09-17 23:57:27\ncomments: true\ntags:\n- github\n- hexo\n- blog\n- git\n- markdown\n\n---\n# Started the github pages \n\n## Intro\n\nI had several blogs in the past that i normaly tend to ignore and stop writting, so this one will be a simple tech/personal blog with entries that i might find usefull in the future to search quicker.\n\nAnd as any normal blog i would start by writting the procedure of deploying the blog itself.\n\nThe only way i would keep this thing updated is if i have very quick way to edit via terminal a Markdown file and do a command like `post-it` and that's it.\n\nSo [hexo](https://hexo.io/) seems like a good solution for me.\n\nGona try this one out.\n\nSo first things first, hosting. I'm cheap bastard so i don't have cool domain yet, and i just need some git repo for the blog.\n\nLets stick with [GitHub Pages](https://pages.github.com/) there git service is pretty owesome so lets try this one out.\n\n**Mental-Note-To-Self:** Get some nice english corrector ;) \n\nSo i've just greated a new repo in github and from the settings i've selected to build a standard GitHub Page.\n\ncloned the dam thing to my computer\n\n`git clone git@github.com:rramos/rramos.github.io.git`\n\n## Installing Hexo\n\nSo first things first, let's install the requirements.\n\nIt requires:\n* Node\n* Git\n\nWell, git i allready have since a clonned the repo. \nNode must be installed, but since i'm using Ubuntu Xenial i need to install the legacy one.\n\n```\nsudo apt install nodejs-legacy\n```\n\nNow starting to install hexo via npm.\n\n```\nsudo npm install -g hexo-cli\nsudo npm install hexo-deployer-git --save\n```\n\nOk that's it.\n\n## Start writting\n\nSo i've run the following command on the repo, this initiates the hexo page structure.\n\n```\nhexo init\n```\n\nI've added the deploy configuration in _config.yml\n\n```\ndeploy:\n  type: git\n  repo: git@github.com:rramos/rramos.github.io.git\n  branch: master\n```\n\nI also downloaded a very simple theme, cas'm a simple guy. \n\n```\ncd themes\ngit clone https://github.com/lotabout/very-simple\n```\nThanks @lotabout for the theme by the way.\n\nAnd installed the theme requirements\n\n```\ngit clone https://github.com/lotabout/very-simple themes/very-simple\nsudo npm install hexo-renderer-sass --save\nsudo npm install hexo-renderer-jade --save\n```\n\n## Writing\n\nSo ... now i suppose i can start creating blog entries, let's start.\n\n```\nhexo new post \"Created rramos.github.io\"\n```\n\nI get the output where the md file is and start writting there.\n\nAfter i have made sure my ssh keys where registered on github i simply deployed with\n\n```\nhexo deploy\n```\n\nAnd voilá: https://rramos.github.io  is up and running.\n\n## Final touches \n\nSo hexo only dumps the public part of the blog which makes sense to the git repo. ~~But i want to include all the source and not have separeted repositories for that.~~\n\nI've created a src dir and copy all the source data there, now i can edit directly that repo, let's take that advantage and remove the default hello world page from the structure guess there migh exist a command for that.\n\n```\n$ hexo list post\nINFO  Start processing\nDate        Title                     Path                                Category  Tags\n2016-09-17  Created rramos.github.io  _posts/Created-rramos-github-io.md\n2016-09-18  Hello World               _posts/hello-world.md\n```\n\nThere you are you bastard hello-world, lets get ride of you.\n\n```\nhexo clean\nrm source/_posts/hello-world.md\nhexo generate\n```\n\nWell guess that's it. There is a lot to explore in hexo from what i can tell, need to check the documentation and understand the community envolvment. Also a quick way to add imagens and other objects and define the quick way to deploy.\n\n## Seperate public from source\n\nIt turns out that have the source and public data in the same repo causes some issues on the updates. The best aproach seems to have a separate repo for the source data and the offcial Github Pages or other hosting service with git for the public part.\n\nI've also added in this source repo a `.gitignore` with\n\n```\npublic\ndb.json\n.deploy_git\n```\n\nSo now i just have to edit MarkDown hexo generate and hexo deploy, seems quick enough let's see if this time i can keep this updated.\n\nCheers,\nRR\n","slug":"Created-rramos-github-io","published":1,"updated":"2020-09-06T14:38:37.851Z","layout":"post","photos":[],"link":"","_id":"ckle9cxjh0006chpf15vlem3w","content":"<h1 id=\"Started-the-github-pages\"><a href=\"#Started-the-github-pages\" class=\"headerlink\" title=\"Started the github pages\"></a>Started the github pages</h1><h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><p>I had several blogs in the past that i normaly tend to ignore and stop writting, so this one will be a simple tech/personal blog with entries that i might find usefull in the future to search quicker.</p>\n<p>And as any normal blog i would start by writting the procedure of deploying the blog itself.</p>\n<p>The only way i would keep this thing updated is if i have very quick way to edit via terminal a Markdown file and do a command like <code>post-it</code> and that’s it.</p>\n<p>So <a href=\"https://hexo.io/\">hexo</a> seems like a good solution for me.</p>\n<p>Gona try this one out.</p>\n<p>So first things first, hosting. I’m cheap bastard so i don’t have cool domain yet, and i just need some git repo for the blog.</p>\n<p>Lets stick with <a href=\"https://pages.github.com/\">GitHub Pages</a> there git service is pretty owesome so lets try this one out.</p>\n<p><strong>Mental-Note-To-Self:</strong> Get some nice english corrector ;) </p>\n<p>So i’ve just greated a new repo in github and from the settings i’ve selected to build a standard GitHub Page.</p>\n<p>cloned the dam thing to my computer</p>\n<p><code>git clone git@github.com:rramos/rramos.github.io.git</code></p>\n<h2 id=\"Installing-Hexo\"><a href=\"#Installing-Hexo\" class=\"headerlink\" title=\"Installing Hexo\"></a>Installing Hexo</h2><p>So first things first, let’s install the requirements.</p>\n<p>It requires:</p>\n<ul>\n<li>Node</li>\n<li>Git</li>\n</ul>\n<p>Well, git i allready have since a clonned the repo.<br>Node must be installed, but since i’m using Ubuntu Xenial i need to install the legacy one.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install nodejs-legacy</span><br></pre></td></tr></table></figure>\n<p>Now starting to install hexo via npm.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo npm install -g hexo-cli</span><br><span class=\"line\">sudo npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>\n<p>Ok that’s it.</p>\n<h2 id=\"Start-writting\"><a href=\"#Start-writting\" class=\"headerlink\" title=\"Start writting\"></a>Start writting</h2><p>So i’ve run the following command on the repo, this initiates the hexo page structure.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo init</span><br></pre></td></tr></table></figure>\n<p>I’ve added the deploy configuration in _config.yml</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repo: git@github.com:rramos&#x2F;rramos.github.io.git</span><br><span class=\"line\">  branch: master</span><br></pre></td></tr></table></figure>\n<p>I also downloaded a very simple theme, cas’m a simple guy. </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd themes</span><br><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;lotabout&#x2F;very-simple</span><br></pre></td></tr></table></figure>\n<p>Thanks @lotabout for the theme by the way.</p>\n<p>And installed the theme requirements</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;lotabout&#x2F;very-simple themes&#x2F;very-simple</span><br><span class=\"line\">sudo npm install hexo-renderer-sass --save</span><br><span class=\"line\">sudo npm install hexo-renderer-jade --save</span><br></pre></td></tr></table></figure>\n<h2 id=\"Writing\"><a href=\"#Writing\" class=\"headerlink\" title=\"Writing\"></a>Writing</h2><p>So … now i suppose i can start creating blog entries, let’s start.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new post &quot;Created rramos.github.io&quot;</span><br></pre></td></tr></table></figure>\n<p>I get the output where the md file is and start writting there.</p>\n<p>After i have made sure my ssh keys where registered on github i simply deployed with</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo deploy</span><br></pre></td></tr></table></figure>\n<p>And voilá: <a href=\"https://rramos.github.io/\">https://rramos.github.io</a>  is up and running.</p>\n<h2 id=\"Final-touches\"><a href=\"#Final-touches\" class=\"headerlink\" title=\"Final touches\"></a>Final touches</h2><p>So hexo only dumps the public part of the blog which makes sense to the git repo. <del>But i want to include all the source and not have separeted repositories for that.</del></p>\n<p>I’ve created a src dir and copy all the source data there, now i can edit directly that repo, let’s take that advantage and remove the default hello world page from the structure guess there migh exist a command for that.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo list post</span><br><span class=\"line\">INFO  Start processing</span><br><span class=\"line\">Date        Title                     Path                                Category  Tags</span><br><span class=\"line\">2016-09-17  Created rramos.github.io  _posts&#x2F;Created-rramos-github-io.md</span><br><span class=\"line\">2016-09-18  Hello World               _posts&#x2F;hello-world.md</span><br></pre></td></tr></table></figure>\n<p>There you are you bastard hello-world, lets get ride of you.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo clean</span><br><span class=\"line\">rm source&#x2F;_posts&#x2F;hello-world.md</span><br><span class=\"line\">hexo generate</span><br></pre></td></tr></table></figure>\n<p>Well guess that’s it. There is a lot to explore in hexo from what i can tell, need to check the documentation and understand the community envolvment. Also a quick way to add imagens and other objects and define the quick way to deploy.</p>\n<h2 id=\"Seperate-public-from-source\"><a href=\"#Seperate-public-from-source\" class=\"headerlink\" title=\"Seperate public from source\"></a>Seperate public from source</h2><p>It turns out that have the source and public data in the same repo causes some issues on the updates. The best aproach seems to have a separate repo for the source data and the offcial Github Pages or other hosting service with git for the public part.</p>\n<p>I’ve also added in this source repo a <code>.gitignore</code> with</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public</span><br><span class=\"line\">db.json</span><br><span class=\"line\">.deploy_git</span><br></pre></td></tr></table></figure>\n<p>So now i just have to edit MarkDown hexo generate and hexo deploy, seems quick enough let’s see if this time i can keep this updated.</p>\n<p>Cheers,<br>RR</p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Started-the-github-pages\"><a href=\"#Started-the-github-pages\" class=\"headerlink\" title=\"Started the github pages\"></a>Started the github pages</h1><h2 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h2><p>I had several blogs in the past that i normaly tend to ignore and stop writting, so this one will be a simple tech/personal blog with entries that i might find usefull in the future to search quicker.</p>\n<p>And as any normal blog i would start by writting the procedure of deploying the blog itself.</p>\n<p>The only way i would keep this thing updated is if i have very quick way to edit via terminal a Markdown file and do a command like <code>post-it</code> and that’s it.</p>\n<p>So <a href=\"https://hexo.io/\">hexo</a> seems like a good solution for me.</p>\n<p>Gona try this one out.</p>\n<p>So first things first, hosting. I’m cheap bastard so i don’t have cool domain yet, and i just need some git repo for the blog.</p>\n<p>Lets stick with <a href=\"https://pages.github.com/\">GitHub Pages</a> there git service is pretty owesome so lets try this one out.</p>\n<p><strong>Mental-Note-To-Self:</strong> Get some nice english corrector ;) </p>\n<p>So i’ve just greated a new repo in github and from the settings i’ve selected to build a standard GitHub Page.</p>\n<p>cloned the dam thing to my computer</p>\n<p><code>git clone git@github.com:rramos/rramos.github.io.git</code></p>\n<h2 id=\"Installing-Hexo\"><a href=\"#Installing-Hexo\" class=\"headerlink\" title=\"Installing Hexo\"></a>Installing Hexo</h2><p>So first things first, let’s install the requirements.</p>\n<p>It requires:</p>\n<ul>\n<li>Node</li>\n<li>Git</li>\n</ul>\n<p>Well, git i allready have since a clonned the repo.<br>Node must be installed, but since i’m using Ubuntu Xenial i need to install the legacy one.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt install nodejs-legacy</span><br></pre></td></tr></table></figure>\n<p>Now starting to install hexo via npm.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo npm install -g hexo-cli</span><br><span class=\"line\">sudo npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure>\n<p>Ok that’s it.</p>\n<h2 id=\"Start-writting\"><a href=\"#Start-writting\" class=\"headerlink\" title=\"Start writting\"></a>Start writting</h2><p>So i’ve run the following command on the repo, this initiates the hexo page structure.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo init</span><br></pre></td></tr></table></figure>\n<p>I’ve added the deploy configuration in _config.yml</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">deploy:</span><br><span class=\"line\">  type: git</span><br><span class=\"line\">  repo: git@github.com:rramos&#x2F;rramos.github.io.git</span><br><span class=\"line\">  branch: master</span><br></pre></td></tr></table></figure>\n<p>I also downloaded a very simple theme, cas’m a simple guy. </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd themes</span><br><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;lotabout&#x2F;very-simple</span><br></pre></td></tr></table></figure>\n<p>Thanks @lotabout for the theme by the way.</p>\n<p>And installed the theme requirements</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;lotabout&#x2F;very-simple themes&#x2F;very-simple</span><br><span class=\"line\">sudo npm install hexo-renderer-sass --save</span><br><span class=\"line\">sudo npm install hexo-renderer-jade --save</span><br></pre></td></tr></table></figure>\n<h2 id=\"Writing\"><a href=\"#Writing\" class=\"headerlink\" title=\"Writing\"></a>Writing</h2><p>So … now i suppose i can start creating blog entries, let’s start.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo new post &quot;Created rramos.github.io&quot;</span><br></pre></td></tr></table></figure>\n<p>I get the output where the md file is and start writting there.</p>\n<p>After i have made sure my ssh keys where registered on github i simply deployed with</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo deploy</span><br></pre></td></tr></table></figure>\n<p>And voilá: <a href=\"https://rramos.github.io/\">https://rramos.github.io</a>  is up and running.</p>\n<h2 id=\"Final-touches\"><a href=\"#Final-touches\" class=\"headerlink\" title=\"Final touches\"></a>Final touches</h2><p>So hexo only dumps the public part of the blog which makes sense to the git repo. <del>But i want to include all the source and not have separeted repositories for that.</del></p>\n<p>I’ve created a src dir and copy all the source data there, now i can edit directly that repo, let’s take that advantage and remove the default hello world page from the structure guess there migh exist a command for that.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ hexo list post</span><br><span class=\"line\">INFO  Start processing</span><br><span class=\"line\">Date        Title                     Path                                Category  Tags</span><br><span class=\"line\">2016-09-17  Created rramos.github.io  _posts&#x2F;Created-rramos-github-io.md</span><br><span class=\"line\">2016-09-18  Hello World               _posts&#x2F;hello-world.md</span><br></pre></td></tr></table></figure>\n<p>There you are you bastard hello-world, lets get ride of you.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">hexo clean</span><br><span class=\"line\">rm source&#x2F;_posts&#x2F;hello-world.md</span><br><span class=\"line\">hexo generate</span><br></pre></td></tr></table></figure>\n<p>Well guess that’s it. There is a lot to explore in hexo from what i can tell, need to check the documentation and understand the community envolvment. Also a quick way to add imagens and other objects and define the quick way to deploy.</p>\n<h2 id=\"Seperate-public-from-source\"><a href=\"#Seperate-public-from-source\" class=\"headerlink\" title=\"Seperate public from source\"></a>Seperate public from source</h2><p>It turns out that have the source and public data in the same repo causes some issues on the updates. The best aproach seems to have a separate repo for the source data and the offcial Github Pages or other hosting service with git for the public part.</p>\n<p>I’ve also added in this source repo a <code>.gitignore</code> with</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public</span><br><span class=\"line\">db.json</span><br><span class=\"line\">.deploy_git</span><br></pre></td></tr></table></figure>\n<p>So now i just have to edit MarkDown hexo generate and hexo deploy, seems quick enough let’s see if this time i can keep this updated.</p>\n<p>Cheers,<br>RR</p>\n"},{"title":"How to Create a Google Analytics Custom Dimension","date":"2017-09-07T19:37:23.000Z","_content":"\nIn order to add a custom dimension in Google Analytics follow this steps:\n\n1. Click **admin** and navigate to the property you which to add a custom dimension\n\n{% asset_img \"Custom_Dimension.png\" \"Custom Dimension (step 1)\" %}\n\n2. Click new **Custom Dimension**\n\n3. Give it a **Name** and a **Scope** and you'l get the javascript to add on your website\n\n{% asset_img \"GA_CustomDimension_js.png\" \"Custom Dimension (step 2)\" %}\n\n4. After you should modify your tracking code adding for example\n\n```\nga('send', 'pageview', {\n  'dimension1':  'My Custom Dimension'\n});\n```\n\n## When we should use this ?\n\nLet's say your website does some sort of user classification, you can create a CustomDimension ex: `UserCategory` and send enrich the data you track.\n","source":"_posts/GA-CustomDimension.md","raw":"---\ntitle: How to Create a Google Analytics Custom Dimension\ndate: 2017-09-07 20:37:23\ntags:\n    - Google Analytics\n    - Analytics\n    - Metrics\n    - Dimensions\n---\n\nIn order to add a custom dimension in Google Analytics follow this steps:\n\n1. Click **admin** and navigate to the property you which to add a custom dimension\n\n{% asset_img \"Custom_Dimension.png\" \"Custom Dimension (step 1)\" %}\n\n2. Click new **Custom Dimension**\n\n3. Give it a **Name** and a **Scope** and you'l get the javascript to add on your website\n\n{% asset_img \"GA_CustomDimension_js.png\" \"Custom Dimension (step 2)\" %}\n\n4. After you should modify your tracking code adding for example\n\n```\nga('send', 'pageview', {\n  'dimension1':  'My Custom Dimension'\n});\n```\n\n## When we should use this ?\n\nLet's say your website does some sort of user classification, you can create a CustomDimension ex: `UserCategory` and send enrich the data you track.\n","slug":"GA-CustomDimension","published":1,"updated":"2020-09-06T14:38:37.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxji0007chpfgfiidg7d","content":"<p>In order to add a custom dimension in Google Analytics follow this steps:</p>\n<ol>\n<li>Click <strong>admin</strong> and navigate to the property you which to add a custom dimension</li>\n</ol>\n<img src=\"/2017/09/07/GA-CustomDimension/Custom_Dimension.png\" class=\"\" title=\"Custom Dimension (step 1)\">\n\n<ol start=\"2\">\n<li><p>Click new <strong>Custom Dimension</strong></p>\n</li>\n<li><p>Give it a <strong>Name</strong> and a <strong>Scope</strong> and you’l get the javascript to add on your website</p>\n</li>\n</ol>\n<img src=\"/2017/09/07/GA-CustomDimension/GA_CustomDimension_js.png\" class=\"\" title=\"Custom Dimension (step 2)\">\n\n<ol start=\"4\">\n<li>After you should modify your tracking code adding for example</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ga(&#39;send&#39;, &#39;pageview&#39;, &#123;</span><br><span class=\"line\">  &#39;dimension1&#39;:  &#39;My Custom Dimension&#39;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<h2 id=\"When-we-should-use-this\"><a href=\"#When-we-should-use-this\" class=\"headerlink\" title=\"When we should use this ?\"></a>When we should use this ?</h2><p>Let’s say your website does some sort of user classification, you can create a CustomDimension ex: <code>UserCategory</code> and send enrich the data you track.</p>\n","site":{"data":{}},"excerpt":"","more":"<p>In order to add a custom dimension in Google Analytics follow this steps:</p>\n<ol>\n<li>Click <strong>admin</strong> and navigate to the property you which to add a custom dimension</li>\n</ol>\n<img src=\"/2017/09/07/GA-CustomDimension/Custom_Dimension.png\" class=\"\" title=\"Custom Dimension (step 1)\">\n\n<ol start=\"2\">\n<li><p>Click new <strong>Custom Dimension</strong></p>\n</li>\n<li><p>Give it a <strong>Name</strong> and a <strong>Scope</strong> and you’l get the javascript to add on your website</p>\n</li>\n</ol>\n<img src=\"/2017/09/07/GA-CustomDimension/GA_CustomDimension_js.png\" class=\"\" title=\"Custom Dimension (step 2)\">\n\n<ol start=\"4\">\n<li>After you should modify your tracking code adding for example</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ga(&#39;send&#39;, &#39;pageview&#39;, &#123;</span><br><span class=\"line\">  &#39;dimension1&#39;:  &#39;My Custom Dimension&#39;</span><br><span class=\"line\">&#125;);</span><br></pre></td></tr></table></figure>\n<h2 id=\"When-we-should-use-this\"><a href=\"#When-we-should-use-this\" class=\"headerlink\" title=\"When we should use this ?\"></a>When we should use this ?</h2><p>Let’s say your website does some sort of user classification, you can create a CustomDimension ex: <code>UserCategory</code> and send enrich the data you track.</p>\n"},{"title":"Polybase Configuration With Cloudera 5","date":"2017-09-09T00:06:30.000Z","_content":"\n\nIn this article i'll try to describe the required configuration steps to setup Polybase in SQLServer 2016 for [Cloudera](https://www.cloudera.com) CDH5. \n\n# Intro\n\nBefore starting the the configuration steps, let's just try to understand why this is being done.\n\n## Sqoop\n\nSqoop is one of the most used tools to transfer data from the Relational World to BigData infrastructures. It relies on JDBC connectors to transfer data between SQLServer and HDFS.\n\n## Performance\n\nThe transfer process to SQLServer via sqoop is taking quite a lot, so the objective of this PoC is to verify if PolyBase alternative for dumping data in/out the cluster and understand if there is a improvement on existing processes.\n\n# Setup Process\n\n## Obtain the cluster configuration files\n\nIn order to configure Polybase for you Cloudera cluster one should first gather from CM the client configurations for HDFS and YARN.\n\nAfter downloading the configs you need to update the following files:\n\n* yarn-site.xml\n* mapred-site.xml\n* hdfs-site.xml\n\nCopy this files to your SQLServer instance where Polybase will be installed.\n\nThe usual path is\n\n```\nC:\\Program Files\\Microsoft SQL Server\\MSSQL13.MSSQLSERVER\\MSSQL\\Binn\\Polybase\\Hadoop\\conf  \n```\n\n**Note:** Please note, that when PolyBase authenticates to a Kerberos secured cluster, we require the `hadoop.rpc.protection` setting to be set to authentication. This will leave the data communication between Hadoop nodes unencrypted.\n\n##  Activate the required Feature\n\nOn the SQLServer you are going to activate Polybase make sure you have the required pre-requisites\n\n* 64-bit SQL Server Evaluation edition\n* Microsoft .NET Framework 4.5.\n* Oracle Java SE RunTime Environment (JRE) version 7.51 or higher (64-bit) \n* Minimum memory: 4GB\n* Minimum hard disk space: 2GB\n* TCP/IP must be enabled for Polybase to function correctly. \n\nFollow Microsoft guide to activate the feature [PolyBase Install Guide](https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation)\n\nOne could test if Polybase is correctly installed by running the following command\n\n{% codeblock lang:SQL %}\nSELECT SERVERPROPERTY ('IsPolybaseInstalled') AS IsPolybaseInstalled;  \n{% endcodeblock %}\n\n## Configure External data source\n\nExecute the following T-SQL to create Hadoop connectivity to CDH5\n\n{% codeblock lang:SQL %}\n-- Values map to various external data sources.  \n--  Option 6: Cloudera 5.1, 5.2, 5.3, 5.4, 5.5, 5.9, 5.10, 5.11, and 5.12 on Linux\nsp_configure @configname = 'hadoop connectivity', @configvalue = 6;   \nGO   \n\nRECONFIGURE   \nGO  \n{% endcodeblock %}\n\nYou could change the option in case you use a different Hadoop Cluster check the [Option Mapping](https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/polybase-connectivity-configuration-transact-sql)\n\n**Note:** After running RECONFIGURE, you must stop and restart the SQL Server service.\n\n## Create the T-SQL objects\n\nFollow the example configuration described in [Getting Started with Polybase](https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase)\n\n{% codeblock lang:SQL %}\n-- 1: Create a database scoped credential.  \n-- Create a master key on the database. This is required to encrypt the credential secret.  \n\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'S0me!nfo';  \n\n-- 2: Create a database scoped credential  for Kerberos-secured Hadoop clusters.  \n-- IDENTITY: the Kerberos user name.  \n-- SECRET: the Kerberos password  \n\nCREATE DATABASE SCOPED CREDENTIAL HadoopUser1   \nWITH IDENTITY = '<hadoop_user_name>', Secret = '<hadoop_password>';  \n\n-- 3:  Create an external data source.  \n-- LOCATION (Required) : Hadoop Name Node IP address and port.  \n-- RESOURCE MANAGER LOCATION (Optional): Hadoop Resource Manager location to enable pushdown computation.  \n-- CREDENTIAL (Optional):  the database scoped credential, created above.  \n\nCREATE EXTERNAL DATA SOURCE MyHadoopCluster WITH (  \n        TYPE = HADOOP,   \n        LOCATION ='hdfs://10.xxx.xx.xxx:xxxx',   \n        RESOURCE_MANAGER_LOCATION = '10.xxx.xx.xxx:xxxx',   \n        CREDENTIAL = HadoopUser1        \n);  \n\n-- 4: Create an external file format.  \n-- FORMAT TYPE: Type of format in Hadoop (DELIMITEDTEXT,  RCFILE, ORC, PARQUET).    \nCREATE EXTERNAL FILE FORMAT TextFileFormat WITH (  \n        FORMAT_TYPE = DELIMITEDTEXT,   \n        FORMAT_OPTIONS (FIELD_TERMINATOR ='|',   \n                USE_TYPE_DEFAULT = TRUE)  \n\n-- 5:  Create an external table pointing to data stored in Hadoop.  \n-- LOCATION: path to file or directory that contains the data (relative to HDFS root).  \n\nCREATE EXTERNAL TABLE [dbo].[CarSensor_Data] (  \n        [SensorKey] int NOT NULL,   \n        [CustomerKey] int NOT NULL,   \n        [GeographyKey] int NULL,   \n        [Speed] float NOT NULL,   \n        [YearMeasured] int NOT NULL  \n)  \nWITH (LOCATION='/Demo/',   \n        DATA_SOURCE = MyHadoopCluster,  \n        FILE_FORMAT = TextFileFormat  \n);  \n\n-- 6:  Create statistics on an external table.   \nCREATE STATISTICS StatsForSensors on CarSensor_Data(CustomerKey, Speed)  \n{% endcodeblock %}\n\n## Example Queries\n\n* Import external Data\n\n{% codeblock lang:SQL %}\n-- PolyBase Scenario 2: Import external data into SQL Server.  \n-- Import data for fast drivers into SQL Server to do more in-depth analysis and  \n-- leverage Columnstore technology.  \n\nSELECT DISTINCT   \n        Insured_Customers.FirstName, Insured_Customers.LastName,   \n        Insured_Customers.YearlyIncome, Insured_Customers.MaritalStatus  \nINTO Fast_Customers from Insured_Customers INNER JOIN   \n(  \n        SELECT * FROM CarSensor_Data where Speed > 35   \n) AS SensorD  \nON Insured_Customers.CustomerKey = SensorD.CustomerKey  \nORDER BY YearlyIncome  \n\nCREATE CLUSTERED COLUMNSTORE INDEX CCI_FastCustomers ON Fast_Customers;  \n{% endcodeblock %}\n\n* Export External Data\n\n{% codeblock lang:SQL %}\n-- PolyBase Scenario 3: Export data from SQL Server to Hadoop.  \n\n-- Enable INSERT into external table  \nsp_configure ‘allow polybase export’, 1;  \nreconfigure  \n\n-- Create an external table.   \nCREATE EXTERNAL TABLE [dbo].[FastCustomers2009] (  \n        [FirstName] char(25) NOT NULL,   \n        [LastName] char(25) NOT NULL,   \n        [YearlyIncome] float NULL,   \n        [MaritalStatus] char(1) NOT NULL  \n)  \nWITH (  \n        LOCATION='/old_data/2009/customerdata',  \n        DATA_SOURCE = HadoopHDP2,  \n        FILE_FORMAT = TextFileFormat,  \n        REJECT_TYPE = VALUE,  \n        REJECT_VALUE = 0  \n);  \n\n-- Export data: Move old data to Hadoop while keeping it query-able via an external table.  \nINSERT INTO dbo.FastCustomer2009  \nSELECT T.* FROM Insured_Customers T1 JOIN CarSensor_Data T2  \nON (T1.CustomerKey = T2.CustomerKey)  \nWHERE T2.YearMeasured = 2009 and T2.Speed > 40;  \n{% endcodeblock %}\n\n# Tests\n\nInitial tests are quite good actually, even with the identified issues. Polybase seems quite limited but for the objective in hands migth present like a very viable solution.\n\nSome more tests would be required.\n\n\n# Issues\n\n* It seems one cannot truncate external tables so an extra process would be required if you plan to use this as part of an ETL process that should support re-runs\n* It seems that `hadoop_user_name` is being ignored and polybase still uses `pwc_user` account in cluster.\n* Take care on the compression levels you choose as they consume quite a lot CPU on your SQLServer\n* The metadata of the tables is allways stored on SQLServer. And when you choose parquet files has source format it stores in parquet meta the colunms as `col-0,col-1,col-3,...` if you map thoose files to a Hive table would require a view with the respective column name mapping.\n* Not sure if this can be change but the dumped files to HDFS are splitted in 8, for the initial tests, only bad for small tables.\n\n\n\n# Conclusion\n\n- Work in progress\n\n# References\n\n* https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide\n* https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase\n* https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation","source":"_posts/PolybaseCDH.md","raw":"---\ntitle: Polybase Configuration with Cloudera 5\ntags:\n  - Cloudera\n  - Polybase\n  - HDFS\n  - Sqoop\n  - SQLServer\n  - Data Ingestion\ndate: 2017-09-09 01:06:30\n---\n\n\nIn this article i'll try to describe the required configuration steps to setup Polybase in SQLServer 2016 for [Cloudera](https://www.cloudera.com) CDH5. \n\n# Intro\n\nBefore starting the the configuration steps, let's just try to understand why this is being done.\n\n## Sqoop\n\nSqoop is one of the most used tools to transfer data from the Relational World to BigData infrastructures. It relies on JDBC connectors to transfer data between SQLServer and HDFS.\n\n## Performance\n\nThe transfer process to SQLServer via sqoop is taking quite a lot, so the objective of this PoC is to verify if PolyBase alternative for dumping data in/out the cluster and understand if there is a improvement on existing processes.\n\n# Setup Process\n\n## Obtain the cluster configuration files\n\nIn order to configure Polybase for you Cloudera cluster one should first gather from CM the client configurations for HDFS and YARN.\n\nAfter downloading the configs you need to update the following files:\n\n* yarn-site.xml\n* mapred-site.xml\n* hdfs-site.xml\n\nCopy this files to your SQLServer instance where Polybase will be installed.\n\nThe usual path is\n\n```\nC:\\Program Files\\Microsoft SQL Server\\MSSQL13.MSSQLSERVER\\MSSQL\\Binn\\Polybase\\Hadoop\\conf  \n```\n\n**Note:** Please note, that when PolyBase authenticates to a Kerberos secured cluster, we require the `hadoop.rpc.protection` setting to be set to authentication. This will leave the data communication between Hadoop nodes unencrypted.\n\n##  Activate the required Feature\n\nOn the SQLServer you are going to activate Polybase make sure you have the required pre-requisites\n\n* 64-bit SQL Server Evaluation edition\n* Microsoft .NET Framework 4.5.\n* Oracle Java SE RunTime Environment (JRE) version 7.51 or higher (64-bit) \n* Minimum memory: 4GB\n* Minimum hard disk space: 2GB\n* TCP/IP must be enabled for Polybase to function correctly. \n\nFollow Microsoft guide to activate the feature [PolyBase Install Guide](https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation)\n\nOne could test if Polybase is correctly installed by running the following command\n\n{% codeblock lang:SQL %}\nSELECT SERVERPROPERTY ('IsPolybaseInstalled') AS IsPolybaseInstalled;  \n{% endcodeblock %}\n\n## Configure External data source\n\nExecute the following T-SQL to create Hadoop connectivity to CDH5\n\n{% codeblock lang:SQL %}\n-- Values map to various external data sources.  \n--  Option 6: Cloudera 5.1, 5.2, 5.3, 5.4, 5.5, 5.9, 5.10, 5.11, and 5.12 on Linux\nsp_configure @configname = 'hadoop connectivity', @configvalue = 6;   \nGO   \n\nRECONFIGURE   \nGO  \n{% endcodeblock %}\n\nYou could change the option in case you use a different Hadoop Cluster check the [Option Mapping](https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/polybase-connectivity-configuration-transact-sql)\n\n**Note:** After running RECONFIGURE, you must stop and restart the SQL Server service.\n\n## Create the T-SQL objects\n\nFollow the example configuration described in [Getting Started with Polybase](https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase)\n\n{% codeblock lang:SQL %}\n-- 1: Create a database scoped credential.  \n-- Create a master key on the database. This is required to encrypt the credential secret.  \n\nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'S0me!nfo';  \n\n-- 2: Create a database scoped credential  for Kerberos-secured Hadoop clusters.  \n-- IDENTITY: the Kerberos user name.  \n-- SECRET: the Kerberos password  \n\nCREATE DATABASE SCOPED CREDENTIAL HadoopUser1   \nWITH IDENTITY = '<hadoop_user_name>', Secret = '<hadoop_password>';  \n\n-- 3:  Create an external data source.  \n-- LOCATION (Required) : Hadoop Name Node IP address and port.  \n-- RESOURCE MANAGER LOCATION (Optional): Hadoop Resource Manager location to enable pushdown computation.  \n-- CREDENTIAL (Optional):  the database scoped credential, created above.  \n\nCREATE EXTERNAL DATA SOURCE MyHadoopCluster WITH (  \n        TYPE = HADOOP,   \n        LOCATION ='hdfs://10.xxx.xx.xxx:xxxx',   \n        RESOURCE_MANAGER_LOCATION = '10.xxx.xx.xxx:xxxx',   \n        CREDENTIAL = HadoopUser1        \n);  \n\n-- 4: Create an external file format.  \n-- FORMAT TYPE: Type of format in Hadoop (DELIMITEDTEXT,  RCFILE, ORC, PARQUET).    \nCREATE EXTERNAL FILE FORMAT TextFileFormat WITH (  \n        FORMAT_TYPE = DELIMITEDTEXT,   \n        FORMAT_OPTIONS (FIELD_TERMINATOR ='|',   \n                USE_TYPE_DEFAULT = TRUE)  \n\n-- 5:  Create an external table pointing to data stored in Hadoop.  \n-- LOCATION: path to file or directory that contains the data (relative to HDFS root).  \n\nCREATE EXTERNAL TABLE [dbo].[CarSensor_Data] (  \n        [SensorKey] int NOT NULL,   \n        [CustomerKey] int NOT NULL,   \n        [GeographyKey] int NULL,   \n        [Speed] float NOT NULL,   \n        [YearMeasured] int NOT NULL  \n)  \nWITH (LOCATION='/Demo/',   \n        DATA_SOURCE = MyHadoopCluster,  \n        FILE_FORMAT = TextFileFormat  \n);  \n\n-- 6:  Create statistics on an external table.   \nCREATE STATISTICS StatsForSensors on CarSensor_Data(CustomerKey, Speed)  \n{% endcodeblock %}\n\n## Example Queries\n\n* Import external Data\n\n{% codeblock lang:SQL %}\n-- PolyBase Scenario 2: Import external data into SQL Server.  \n-- Import data for fast drivers into SQL Server to do more in-depth analysis and  \n-- leverage Columnstore technology.  \n\nSELECT DISTINCT   \n        Insured_Customers.FirstName, Insured_Customers.LastName,   \n        Insured_Customers.YearlyIncome, Insured_Customers.MaritalStatus  \nINTO Fast_Customers from Insured_Customers INNER JOIN   \n(  \n        SELECT * FROM CarSensor_Data where Speed > 35   \n) AS SensorD  \nON Insured_Customers.CustomerKey = SensorD.CustomerKey  \nORDER BY YearlyIncome  \n\nCREATE CLUSTERED COLUMNSTORE INDEX CCI_FastCustomers ON Fast_Customers;  \n{% endcodeblock %}\n\n* Export External Data\n\n{% codeblock lang:SQL %}\n-- PolyBase Scenario 3: Export data from SQL Server to Hadoop.  \n\n-- Enable INSERT into external table  \nsp_configure ‘allow polybase export’, 1;  \nreconfigure  \n\n-- Create an external table.   \nCREATE EXTERNAL TABLE [dbo].[FastCustomers2009] (  \n        [FirstName] char(25) NOT NULL,   \n        [LastName] char(25) NOT NULL,   \n        [YearlyIncome] float NULL,   \n        [MaritalStatus] char(1) NOT NULL  \n)  \nWITH (  \n        LOCATION='/old_data/2009/customerdata',  \n        DATA_SOURCE = HadoopHDP2,  \n        FILE_FORMAT = TextFileFormat,  \n        REJECT_TYPE = VALUE,  \n        REJECT_VALUE = 0  \n);  \n\n-- Export data: Move old data to Hadoop while keeping it query-able via an external table.  \nINSERT INTO dbo.FastCustomer2009  \nSELECT T.* FROM Insured_Customers T1 JOIN CarSensor_Data T2  \nON (T1.CustomerKey = T2.CustomerKey)  \nWHERE T2.YearMeasured = 2009 and T2.Speed > 40;  \n{% endcodeblock %}\n\n# Tests\n\nInitial tests are quite good actually, even with the identified issues. Polybase seems quite limited but for the objective in hands migth present like a very viable solution.\n\nSome more tests would be required.\n\n\n# Issues\n\n* It seems one cannot truncate external tables so an extra process would be required if you plan to use this as part of an ETL process that should support re-runs\n* It seems that `hadoop_user_name` is being ignored and polybase still uses `pwc_user` account in cluster.\n* Take care on the compression levels you choose as they consume quite a lot CPU on your SQLServer\n* The metadata of the tables is allways stored on SQLServer. And when you choose parquet files has source format it stores in parquet meta the colunms as `col-0,col-1,col-3,...` if you map thoose files to a Hive table would require a view with the respective column name mapping.\n* Not sure if this can be change but the dumped files to HDFS are splitted in 8, for the initial tests, only bad for small tables.\n\n\n\n# Conclusion\n\n- Work in progress\n\n# References\n\n* https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide\n* https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase\n* https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation","slug":"PolybaseCDH","published":1,"updated":"2020-09-06T14:38:37.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjj0008chpf909rfhtb","content":"<p>In this article i’ll try to describe the required configuration steps to setup Polybase in SQLServer 2016 for <a href=\"https://www.cloudera.com/\">Cloudera</a> CDH5. </p>\n<h1 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h1><p>Before starting the the configuration steps, let’s just try to understand why this is being done.</p>\n<h2 id=\"Sqoop\"><a href=\"#Sqoop\" class=\"headerlink\" title=\"Sqoop\"></a>Sqoop</h2><p>Sqoop is one of the most used tools to transfer data from the Relational World to BigData infrastructures. It relies on JDBC connectors to transfer data between SQLServer and HDFS.</p>\n<h2 id=\"Performance\"><a href=\"#Performance\" class=\"headerlink\" title=\"Performance\"></a>Performance</h2><p>The transfer process to SQLServer via sqoop is taking quite a lot, so the objective of this PoC is to verify if PolyBase alternative for dumping data in/out the cluster and understand if there is a improvement on existing processes.</p>\n<h1 id=\"Setup-Process\"><a href=\"#Setup-Process\" class=\"headerlink\" title=\"Setup Process\"></a>Setup Process</h1><h2 id=\"Obtain-the-cluster-configuration-files\"><a href=\"#Obtain-the-cluster-configuration-files\" class=\"headerlink\" title=\"Obtain the cluster configuration files\"></a>Obtain the cluster configuration files</h2><p>In order to configure Polybase for you Cloudera cluster one should first gather from CM the client configurations for HDFS and YARN.</p>\n<p>After downloading the configs you need to update the following files:</p>\n<ul>\n<li>yarn-site.xml</li>\n<li>mapred-site.xml</li>\n<li>hdfs-site.xml</li>\n</ul>\n<p>Copy this files to your SQLServer instance where Polybase will be installed.</p>\n<p>The usual path is</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C:\\Program Files\\Microsoft SQL Server\\MSSQL13.MSSQLSERVER\\MSSQL\\Binn\\Polybase\\Hadoop\\conf  </span><br></pre></td></tr></table></figure>\n<p><strong>Note:</strong> Please note, that when PolyBase authenticates to a Kerberos secured cluster, we require the <code>hadoop.rpc.protection</code> setting to be set to authentication. This will leave the data communication between Hadoop nodes unencrypted.</p>\n<h2 id=\"Activate-the-required-Feature\"><a href=\"#Activate-the-required-Feature\" class=\"headerlink\" title=\"Activate the required Feature\"></a>Activate the required Feature</h2><p>On the SQLServer you are going to activate Polybase make sure you have the required pre-requisites</p>\n<ul>\n<li>64-bit SQL Server Evaluation edition</li>\n<li>Microsoft .NET Framework 4.5.</li>\n<li>Oracle Java SE RunTime Environment (JRE) version 7.51 or higher (64-bit) </li>\n<li>Minimum memory: 4GB</li>\n<li>Minimum hard disk space: 2GB</li>\n<li>TCP/IP must be enabled for Polybase to function correctly. </li>\n</ul>\n<p>Follow Microsoft guide to activate the feature <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation\">PolyBase Install Guide</a></p>\n<p>One could test if Polybase is correctly installed by running the following command</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> SERVERPROPERTY (<span class=\"string\">&#x27;IsPolybaseInstalled&#x27;</span>) <span class=\"keyword\">AS</span> IsPolybaseInstalled;  </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Configure-External-data-source\"><a href=\"#Configure-External-data-source\" class=\"headerlink\" title=\"Configure External data source\"></a>Configure External data source</h2><p>Execute the following T-SQL to create Hadoop connectivity to CDH5</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- Values map to various external data sources.  </span></span><br><span class=\"line\"><span class=\"comment\">--  Option 6: Cloudera 5.1, 5.2, 5.3, 5.4, 5.5, 5.9, 5.10, 5.11, and 5.12 on Linux</span></span><br><span class=\"line\">sp_configure <span class=\"variable\">@configname</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;hadoop connectivity&#x27;</span>, <span class=\"variable\">@configvalue</span> <span class=\"operator\">=</span> <span class=\"number\">6</span>;   </span><br><span class=\"line\">GO   </span><br><span class=\"line\"></span><br><span class=\"line\">RECONFIGURE   </span><br><span class=\"line\">GO  </span><br></pre></td></tr></table></figure>\n\n<p>You could change the option in case you use a different Hadoop Cluster check the <a href=\"https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/polybase-connectivity-configuration-transact-sql\">Option Mapping</a></p>\n<p><strong>Note:</strong> After running RECONFIGURE, you must stop and restart the SQL Server service.</p>\n<h2 id=\"Create-the-T-SQL-objects\"><a href=\"#Create-the-T-SQL-objects\" class=\"headerlink\" title=\"Create the T-SQL objects\"></a>Create the T-SQL objects</h2><p>Follow the example configuration described in <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase\">Getting Started with Polybase</a></p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- 1: Create a database scoped credential.  </span></span><br><span class=\"line\"><span class=\"comment\">-- Create a master key on the database. This is required to encrypt the credential secret.  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> MASTER KEY ENCRYPTION <span class=\"keyword\">BY</span> PASSWORD <span class=\"operator\">=</span> <span class=\"string\">&#x27;S0me!nfo&#x27;</span>;  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- 2: Create a database scoped credential  for Kerberos-secured Hadoop clusters.  </span></span><br><span class=\"line\"><span class=\"comment\">-- IDENTITY: the Kerberos user name.  </span></span><br><span class=\"line\"><span class=\"comment\">-- SECRET: the Kerberos password  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> DATABASE SCOPED CREDENTIAL HadoopUser1   </span><br><span class=\"line\"><span class=\"keyword\">WITH</span> <span class=\"keyword\">IDENTITY</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;&lt;hadoop_user_name&gt;&#x27;</span>, Secret <span class=\"operator\">=</span> <span class=\"string\">&#x27;&lt;hadoop_password&gt;&#x27;</span>;  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- 3:  Create an external data source.  </span></span><br><span class=\"line\"><span class=\"comment\">-- LOCATION (Required) : Hadoop Name Node IP address and port.  </span></span><br><span class=\"line\"><span class=\"comment\">-- RESOURCE MANAGER LOCATION (Optional): Hadoop Resource Manager location to enable pushdown computation.  </span></span><br><span class=\"line\"><span class=\"comment\">-- CREDENTIAL (Optional):  the database scoped credential, created above.  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">EXTERNAL</span> DATA SOURCE MyHadoopCluster <span class=\"keyword\">WITH</span> (  </span><br><span class=\"line\">        TYPE <span class=\"operator\">=</span> HADOOP,   </span><br><span class=\"line\">        LOCATION <span class=\"operator\">=</span><span class=\"string\">&#x27;hdfs://10.xxx.xx.xxx:xxxx&#x27;</span>,   </span><br><span class=\"line\">        RESOURCE_MANAGER_LOCATION <span class=\"operator\">=</span> <span class=\"string\">&#x27;10.xxx.xx.xxx:xxxx&#x27;</span>,   </span><br><span class=\"line\">        CREDENTIAL <span class=\"operator\">=</span> HadoopUser1        </span><br><span class=\"line\">);  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- 4: Create an external file format.  </span></span><br><span class=\"line\"><span class=\"comment\">-- FORMAT TYPE: Type of format in Hadoop (DELIMITEDTEXT,  RCFILE, ORC, PARQUET).    </span></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">EXTERNAL</span> FILE FORMAT TextFileFormat <span class=\"keyword\">WITH</span> (  </span><br><span class=\"line\">        FORMAT_TYPE <span class=\"operator\">=</span> DELIMITEDTEXT,   </span><br><span class=\"line\">        FORMAT_OPTIONS (FIELD_TERMINATOR <span class=\"operator\">=</span><span class=\"string\">&#x27;|&#x27;</span>,   </span><br><span class=\"line\">                USE_TYPE_DEFAULT <span class=\"operator\">=</span> <span class=\"literal\">TRUE</span>)  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- 5:  Create an external table pointing to data stored in Hadoop.  </span></span><br><span class=\"line\"><span class=\"comment\">-- LOCATION: path to file or directory that contains the data (relative to HDFS root).  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">EXTERNAL</span> <span class=\"keyword\">TABLE</span> [dbo].[CarSensor_Data] (  </span><br><span class=\"line\">        [SensorKey] <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [CustomerKey] <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [GeographyKey] <span class=\"type\">int</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [Speed] <span class=\"type\">float</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [YearMeasured] <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>  </span><br><span class=\"line\">)  </span><br><span class=\"line\"><span class=\"keyword\">WITH</span> (LOCATION<span class=\"operator\">=</span><span class=\"string\">&#x27;/Demo/&#x27;</span>,   </span><br><span class=\"line\">        DATA_SOURCE <span class=\"operator\">=</span> MyHadoopCluster,  </span><br><span class=\"line\">        FILE_FORMAT <span class=\"operator\">=</span> TextFileFormat  </span><br><span class=\"line\">);  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- 6:  Create statistics on an external table.   </span></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> STATISTICS StatsForSensors <span class=\"keyword\">on</span> CarSensor_Data(CustomerKey, Speed)  </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Example-Queries\"><a href=\"#Example-Queries\" class=\"headerlink\" title=\"Example Queries\"></a>Example Queries</h2><ul>\n<li>Import external Data</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- PolyBase Scenario 2: Import external data into SQL Server.  </span></span><br><span class=\"line\"><span class=\"comment\">-- Import data for fast drivers into SQL Server to do more in-depth analysis and  </span></span><br><span class=\"line\"><span class=\"comment\">-- leverage Columnstore technology.  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"keyword\">DISTINCT</span>   </span><br><span class=\"line\">        Insured_Customers.FirstName, Insured_Customers.LastName,   </span><br><span class=\"line\">        Insured_Customers.YearlyIncome, Insured_Customers.MaritalStatus  </span><br><span class=\"line\"><span class=\"keyword\">INTO</span> Fast_Customers <span class=\"keyword\">from</span> Insured_Customers <span class=\"keyword\">INNER</span> <span class=\"keyword\">JOIN</span>   </span><br><span class=\"line\">(  </span><br><span class=\"line\">        <span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> CarSensor_Data <span class=\"keyword\">where</span> Speed <span class=\"operator\">&gt;</span> <span class=\"number\">35</span>   </span><br><span class=\"line\">) <span class=\"keyword\">AS</span> SensorD  </span><br><span class=\"line\"><span class=\"keyword\">ON</span> Insured_Customers.CustomerKey <span class=\"operator\">=</span> SensorD.CustomerKey  </span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> YearlyIncome  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> CLUSTERED COLUMNSTORE INDEX CCI_FastCustomers <span class=\"keyword\">ON</span> Fast_Customers;  </span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>Export External Data</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- PolyBase Scenario 3: Export data from SQL Server to Hadoop.  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Enable INSERT into external table  </span></span><br><span class=\"line\">sp_configure ‘allow polybase export’, <span class=\"number\">1</span>;  </span><br><span class=\"line\">reconfigure  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Create an external table.   </span></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">EXTERNAL</span> <span class=\"keyword\">TABLE</span> [dbo].[FastCustomers2009] (  </span><br><span class=\"line\">        [FirstName] <span class=\"type\">char</span>(<span class=\"number\">25</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [LastName] <span class=\"type\">char</span>(<span class=\"number\">25</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [YearlyIncome] <span class=\"type\">float</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [MaritalStatus] <span class=\"type\">char</span>(<span class=\"number\">1</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>  </span><br><span class=\"line\">)  </span><br><span class=\"line\"><span class=\"keyword\">WITH</span> (  </span><br><span class=\"line\">        LOCATION<span class=\"operator\">=</span><span class=\"string\">&#x27;/old_data/2009/customerdata&#x27;</span>,  </span><br><span class=\"line\">        DATA_SOURCE <span class=\"operator\">=</span> HadoopHDP2,  </span><br><span class=\"line\">        FILE_FORMAT <span class=\"operator\">=</span> TextFileFormat,  </span><br><span class=\"line\">        REJECT_TYPE <span class=\"operator\">=</span> <span class=\"keyword\">VALUE</span>,  </span><br><span class=\"line\">        REJECT_VALUE <span class=\"operator\">=</span> <span class=\"number\">0</span>  </span><br><span class=\"line\">);  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Export data: Move old data to Hadoop while keeping it query-able via an external table.  </span></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> dbo.FastCustomer2009  </span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> T.<span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> Insured_Customers T1 <span class=\"keyword\">JOIN</span> CarSensor_Data T2  </span><br><span class=\"line\"><span class=\"keyword\">ON</span> (T1.CustomerKey <span class=\"operator\">=</span> T2.CustomerKey)  </span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> T2.YearMeasured <span class=\"operator\">=</span> <span class=\"number\">2009</span> <span class=\"keyword\">and</span> T2.Speed <span class=\"operator\">&gt;</span> <span class=\"number\">40</span>;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Tests\"><a href=\"#Tests\" class=\"headerlink\" title=\"Tests\"></a>Tests</h1><p>Initial tests are quite good actually, even with the identified issues. Polybase seems quite limited but for the objective in hands migth present like a very viable solution.</p>\n<p>Some more tests would be required.</p>\n<h1 id=\"Issues\"><a href=\"#Issues\" class=\"headerlink\" title=\"Issues\"></a>Issues</h1><ul>\n<li>It seems one cannot truncate external tables so an extra process would be required if you plan to use this as part of an ETL process that should support re-runs</li>\n<li>It seems that <code>hadoop_user_name</code> is being ignored and polybase still uses <code>pwc_user</code> account in cluster.</li>\n<li>Take care on the compression levels you choose as they consume quite a lot CPU on your SQLServer</li>\n<li>The metadata of the tables is allways stored on SQLServer. And when you choose parquet files has source format it stores in parquet meta the colunms as <code>col-0,col-1,col-3,...</code> if you map thoose files to a Hive table would require a view with the respective column name mapping.</li>\n<li>Not sure if this can be change but the dumped files to HDFS are splitted in 8, for the initial tests, only bad for small tables.</li>\n</ul>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><ul>\n<li>Work in progress</li>\n</ul>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide\">https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase\">https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation\">https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>In this article i’ll try to describe the required configuration steps to setup Polybase in SQLServer 2016 for <a href=\"https://www.cloudera.com/\">Cloudera</a> CDH5. </p>\n<h1 id=\"Intro\"><a href=\"#Intro\" class=\"headerlink\" title=\"Intro\"></a>Intro</h1><p>Before starting the the configuration steps, let’s just try to understand why this is being done.</p>\n<h2 id=\"Sqoop\"><a href=\"#Sqoop\" class=\"headerlink\" title=\"Sqoop\"></a>Sqoop</h2><p>Sqoop is one of the most used tools to transfer data from the Relational World to BigData infrastructures. It relies on JDBC connectors to transfer data between SQLServer and HDFS.</p>\n<h2 id=\"Performance\"><a href=\"#Performance\" class=\"headerlink\" title=\"Performance\"></a>Performance</h2><p>The transfer process to SQLServer via sqoop is taking quite a lot, so the objective of this PoC is to verify if PolyBase alternative for dumping data in/out the cluster and understand if there is a improvement on existing processes.</p>\n<h1 id=\"Setup-Process\"><a href=\"#Setup-Process\" class=\"headerlink\" title=\"Setup Process\"></a>Setup Process</h1><h2 id=\"Obtain-the-cluster-configuration-files\"><a href=\"#Obtain-the-cluster-configuration-files\" class=\"headerlink\" title=\"Obtain the cluster configuration files\"></a>Obtain the cluster configuration files</h2><p>In order to configure Polybase for you Cloudera cluster one should first gather from CM the client configurations for HDFS and YARN.</p>\n<p>After downloading the configs you need to update the following files:</p>\n<ul>\n<li>yarn-site.xml</li>\n<li>mapred-site.xml</li>\n<li>hdfs-site.xml</li>\n</ul>\n<p>Copy this files to your SQLServer instance where Polybase will be installed.</p>\n<p>The usual path is</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">C:\\Program Files\\Microsoft SQL Server\\MSSQL13.MSSQLSERVER\\MSSQL\\Binn\\Polybase\\Hadoop\\conf  </span><br></pre></td></tr></table></figure>\n<p><strong>Note:</strong> Please note, that when PolyBase authenticates to a Kerberos secured cluster, we require the <code>hadoop.rpc.protection</code> setting to be set to authentication. This will leave the data communication between Hadoop nodes unencrypted.</p>\n<h2 id=\"Activate-the-required-Feature\"><a href=\"#Activate-the-required-Feature\" class=\"headerlink\" title=\"Activate the required Feature\"></a>Activate the required Feature</h2><p>On the SQLServer you are going to activate Polybase make sure you have the required pre-requisites</p>\n<ul>\n<li>64-bit SQL Server Evaluation edition</li>\n<li>Microsoft .NET Framework 4.5.</li>\n<li>Oracle Java SE RunTime Environment (JRE) version 7.51 or higher (64-bit) </li>\n<li>Minimum memory: 4GB</li>\n<li>Minimum hard disk space: 2GB</li>\n<li>TCP/IP must be enabled for Polybase to function correctly. </li>\n</ul>\n<p>Follow Microsoft guide to activate the feature <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation\">PolyBase Install Guide</a></p>\n<p>One could test if Polybase is correctly installed by running the following command</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">SELECT</span> SERVERPROPERTY (<span class=\"string\">&#x27;IsPolybaseInstalled&#x27;</span>) <span class=\"keyword\">AS</span> IsPolybaseInstalled;  </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Configure-External-data-source\"><a href=\"#Configure-External-data-source\" class=\"headerlink\" title=\"Configure External data source\"></a>Configure External data source</h2><p>Execute the following T-SQL to create Hadoop connectivity to CDH5</p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- Values map to various external data sources.  </span></span><br><span class=\"line\"><span class=\"comment\">--  Option 6: Cloudera 5.1, 5.2, 5.3, 5.4, 5.5, 5.9, 5.10, 5.11, and 5.12 on Linux</span></span><br><span class=\"line\">sp_configure <span class=\"variable\">@configname</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;hadoop connectivity&#x27;</span>, <span class=\"variable\">@configvalue</span> <span class=\"operator\">=</span> <span class=\"number\">6</span>;   </span><br><span class=\"line\">GO   </span><br><span class=\"line\"></span><br><span class=\"line\">RECONFIGURE   </span><br><span class=\"line\">GO  </span><br></pre></td></tr></table></figure>\n\n<p>You could change the option in case you use a different Hadoop Cluster check the <a href=\"https://docs.microsoft.com/en-us/sql/database-engine/configure-windows/polybase-connectivity-configuration-transact-sql\">Option Mapping</a></p>\n<p><strong>Note:</strong> After running RECONFIGURE, you must stop and restart the SQL Server service.</p>\n<h2 id=\"Create-the-T-SQL-objects\"><a href=\"#Create-the-T-SQL-objects\" class=\"headerlink\" title=\"Create the T-SQL objects\"></a>Create the T-SQL objects</h2><p>Follow the example configuration described in <a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase\">Getting Started with Polybase</a></p>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- 1: Create a database scoped credential.  </span></span><br><span class=\"line\"><span class=\"comment\">-- Create a master key on the database. This is required to encrypt the credential secret.  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> MASTER KEY ENCRYPTION <span class=\"keyword\">BY</span> PASSWORD <span class=\"operator\">=</span> <span class=\"string\">&#x27;S0me!nfo&#x27;</span>;  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- 2: Create a database scoped credential  for Kerberos-secured Hadoop clusters.  </span></span><br><span class=\"line\"><span class=\"comment\">-- IDENTITY: the Kerberos user name.  </span></span><br><span class=\"line\"><span class=\"comment\">-- SECRET: the Kerberos password  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> DATABASE SCOPED CREDENTIAL HadoopUser1   </span><br><span class=\"line\"><span class=\"keyword\">WITH</span> <span class=\"keyword\">IDENTITY</span> <span class=\"operator\">=</span> <span class=\"string\">&#x27;&lt;hadoop_user_name&gt;&#x27;</span>, Secret <span class=\"operator\">=</span> <span class=\"string\">&#x27;&lt;hadoop_password&gt;&#x27;</span>;  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- 3:  Create an external data source.  </span></span><br><span class=\"line\"><span class=\"comment\">-- LOCATION (Required) : Hadoop Name Node IP address and port.  </span></span><br><span class=\"line\"><span class=\"comment\">-- RESOURCE MANAGER LOCATION (Optional): Hadoop Resource Manager location to enable pushdown computation.  </span></span><br><span class=\"line\"><span class=\"comment\">-- CREDENTIAL (Optional):  the database scoped credential, created above.  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">EXTERNAL</span> DATA SOURCE MyHadoopCluster <span class=\"keyword\">WITH</span> (  </span><br><span class=\"line\">        TYPE <span class=\"operator\">=</span> HADOOP,   </span><br><span class=\"line\">        LOCATION <span class=\"operator\">=</span><span class=\"string\">&#x27;hdfs://10.xxx.xx.xxx:xxxx&#x27;</span>,   </span><br><span class=\"line\">        RESOURCE_MANAGER_LOCATION <span class=\"operator\">=</span> <span class=\"string\">&#x27;10.xxx.xx.xxx:xxxx&#x27;</span>,   </span><br><span class=\"line\">        CREDENTIAL <span class=\"operator\">=</span> HadoopUser1        </span><br><span class=\"line\">);  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- 4: Create an external file format.  </span></span><br><span class=\"line\"><span class=\"comment\">-- FORMAT TYPE: Type of format in Hadoop (DELIMITEDTEXT,  RCFILE, ORC, PARQUET).    </span></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">EXTERNAL</span> FILE FORMAT TextFileFormat <span class=\"keyword\">WITH</span> (  </span><br><span class=\"line\">        FORMAT_TYPE <span class=\"operator\">=</span> DELIMITEDTEXT,   </span><br><span class=\"line\">        FORMAT_OPTIONS (FIELD_TERMINATOR <span class=\"operator\">=</span><span class=\"string\">&#x27;|&#x27;</span>,   </span><br><span class=\"line\">                USE_TYPE_DEFAULT <span class=\"operator\">=</span> <span class=\"literal\">TRUE</span>)  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- 5:  Create an external table pointing to data stored in Hadoop.  </span></span><br><span class=\"line\"><span class=\"comment\">-- LOCATION: path to file or directory that contains the data (relative to HDFS root).  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">EXTERNAL</span> <span class=\"keyword\">TABLE</span> [dbo].[CarSensor_Data] (  </span><br><span class=\"line\">        [SensorKey] <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [CustomerKey] <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [GeographyKey] <span class=\"type\">int</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [Speed] <span class=\"type\">float</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [YearMeasured] <span class=\"type\">int</span> <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>  </span><br><span class=\"line\">)  </span><br><span class=\"line\"><span class=\"keyword\">WITH</span> (LOCATION<span class=\"operator\">=</span><span class=\"string\">&#x27;/Demo/&#x27;</span>,   </span><br><span class=\"line\">        DATA_SOURCE <span class=\"operator\">=</span> MyHadoopCluster,  </span><br><span class=\"line\">        FILE_FORMAT <span class=\"operator\">=</span> TextFileFormat  </span><br><span class=\"line\">);  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- 6:  Create statistics on an external table.   </span></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> STATISTICS StatsForSensors <span class=\"keyword\">on</span> CarSensor_Data(CustomerKey, Speed)  </span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Example-Queries\"><a href=\"#Example-Queries\" class=\"headerlink\" title=\"Example Queries\"></a>Example Queries</h2><ul>\n<li>Import external Data</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- PolyBase Scenario 2: Import external data into SQL Server.  </span></span><br><span class=\"line\"><span class=\"comment\">-- Import data for fast drivers into SQL Server to do more in-depth analysis and  </span></span><br><span class=\"line\"><span class=\"comment\">-- leverage Columnstore technology.  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> <span class=\"keyword\">DISTINCT</span>   </span><br><span class=\"line\">        Insured_Customers.FirstName, Insured_Customers.LastName,   </span><br><span class=\"line\">        Insured_Customers.YearlyIncome, Insured_Customers.MaritalStatus  </span><br><span class=\"line\"><span class=\"keyword\">INTO</span> Fast_Customers <span class=\"keyword\">from</span> Insured_Customers <span class=\"keyword\">INNER</span> <span class=\"keyword\">JOIN</span>   </span><br><span class=\"line\">(  </span><br><span class=\"line\">        <span class=\"keyword\">SELECT</span> <span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> CarSensor_Data <span class=\"keyword\">where</span> Speed <span class=\"operator\">&gt;</span> <span class=\"number\">35</span>   </span><br><span class=\"line\">) <span class=\"keyword\">AS</span> SensorD  </span><br><span class=\"line\"><span class=\"keyword\">ON</span> Insured_Customers.CustomerKey <span class=\"operator\">=</span> SensorD.CustomerKey  </span><br><span class=\"line\"><span class=\"keyword\">ORDER</span> <span class=\"keyword\">BY</span> YearlyIncome  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> CLUSTERED COLUMNSTORE INDEX CCI_FastCustomers <span class=\"keyword\">ON</span> Fast_Customers;  </span><br></pre></td></tr></table></figure>\n\n<ul>\n<li>Export External Data</li>\n</ul>\n<figure class=\"highlight sql\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">-- PolyBase Scenario 3: Export data from SQL Server to Hadoop.  </span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Enable INSERT into external table  </span></span><br><span class=\"line\">sp_configure ‘allow polybase export’, <span class=\"number\">1</span>;  </span><br><span class=\"line\">reconfigure  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Create an external table.   </span></span><br><span class=\"line\"><span class=\"keyword\">CREATE</span> <span class=\"keyword\">EXTERNAL</span> <span class=\"keyword\">TABLE</span> [dbo].[FastCustomers2009] (  </span><br><span class=\"line\">        [FirstName] <span class=\"type\">char</span>(<span class=\"number\">25</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [LastName] <span class=\"type\">char</span>(<span class=\"number\">25</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [YearlyIncome] <span class=\"type\">float</span> <span class=\"keyword\">NULL</span>,   </span><br><span class=\"line\">        [MaritalStatus] <span class=\"type\">char</span>(<span class=\"number\">1</span>) <span class=\"keyword\">NOT</span> <span class=\"keyword\">NULL</span>  </span><br><span class=\"line\">)  </span><br><span class=\"line\"><span class=\"keyword\">WITH</span> (  </span><br><span class=\"line\">        LOCATION<span class=\"operator\">=</span><span class=\"string\">&#x27;/old_data/2009/customerdata&#x27;</span>,  </span><br><span class=\"line\">        DATA_SOURCE <span class=\"operator\">=</span> HadoopHDP2,  </span><br><span class=\"line\">        FILE_FORMAT <span class=\"operator\">=</span> TextFileFormat,  </span><br><span class=\"line\">        REJECT_TYPE <span class=\"operator\">=</span> <span class=\"keyword\">VALUE</span>,  </span><br><span class=\"line\">        REJECT_VALUE <span class=\"operator\">=</span> <span class=\"number\">0</span>  </span><br><span class=\"line\">);  </span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\">-- Export data: Move old data to Hadoop while keeping it query-able via an external table.  </span></span><br><span class=\"line\"><span class=\"keyword\">INSERT</span> <span class=\"keyword\">INTO</span> dbo.FastCustomer2009  </span><br><span class=\"line\"><span class=\"keyword\">SELECT</span> T.<span class=\"operator\">*</span> <span class=\"keyword\">FROM</span> Insured_Customers T1 <span class=\"keyword\">JOIN</span> CarSensor_Data T2  </span><br><span class=\"line\"><span class=\"keyword\">ON</span> (T1.CustomerKey <span class=\"operator\">=</span> T2.CustomerKey)  </span><br><span class=\"line\"><span class=\"keyword\">WHERE</span> T2.YearMeasured <span class=\"operator\">=</span> <span class=\"number\">2009</span> <span class=\"keyword\">and</span> T2.Speed <span class=\"operator\">&gt;</span> <span class=\"number\">40</span>;  </span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Tests\"><a href=\"#Tests\" class=\"headerlink\" title=\"Tests\"></a>Tests</h1><p>Initial tests are quite good actually, even with the identified issues. Polybase seems quite limited but for the objective in hands migth present like a very viable solution.</p>\n<p>Some more tests would be required.</p>\n<h1 id=\"Issues\"><a href=\"#Issues\" class=\"headerlink\" title=\"Issues\"></a>Issues</h1><ul>\n<li>It seems one cannot truncate external tables so an extra process would be required if you plan to use this as part of an ETL process that should support re-runs</li>\n<li>It seems that <code>hadoop_user_name</code> is being ignored and polybase still uses <code>pwc_user</code> account in cluster.</li>\n<li>Take care on the compression levels you choose as they consume quite a lot CPU on your SQLServer</li>\n<li>The metadata of the tables is allways stored on SQLServer. And when you choose parquet files has source format it stores in parquet meta the colunms as <code>col-0,col-1,col-3,...</code> if you map thoose files to a Hive table would require a view with the respective column name mapping.</li>\n<li>Not sure if this can be change but the dumped files to HDFS are splitted in 8, for the initial tests, only bad for small tables.</li>\n</ul>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><ul>\n<li>Work in progress</li>\n</ul>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide\">https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase\">https://docs.microsoft.com/en-us/sql/relational-databases/polybase/get-started-with-polybase</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation\">https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-installation</a></li>\n</ul>\n"},{"title":"Spark Summit 2016","date":"2016-11-03T22:09:40.000Z","_content":"\n\nSo i've attended 25-27 OCT in Brussels to SparkSummit 2016 in Europe. So it kind makes sense to write down some impressions regarding the event.\n\nThe Event it self was pretty well organized, and the distribution of spaces was well planned.\n\nRegarding Spark it-self, is one of the ecosystem players (https://hadoopecosystemtable.github.io/) that's been getting more traction from the industry, partially because of the Enterprise support from Databricks and other major BigData contributors.\n\nThis Summit, had huge increase of participants, compared to last year, what guaranties that this is really here to stay.\n\nI always try to make of this conferences with two key points:\n\n* What's new ?\n* What are the other Companies doing, that's working for them ?\n\n## What new \n\nAMPLab, the creators of Spark from UC Berkeley presented some interesting projects for the future that would increase SparkEngine to a new level. They presented some comparation charts with execution times of this new engine compared with project Flink which uses real streaming and not micro-batching, which may come as a very good news, because there are more and more Streaming applications being implemented. Also some new layer of secure, the projects are called Drizzle and Opaque, let's hope they get this ready soon :)\n\nAlso one of the big announcements, was the new TensorFrames object. this output come's from a join work of TensorFlow and Databricks in order to bring DeepLearning to Spark. What this means is that in the Future one could simple use the Spark Framework in order to take advantage of GPU's, also that comes into alignment as you could expect with the new feature in Databricks Notebooks to support this.\n\nIt is still in Beta, but if you are using both TensorFlow and Spark, this is obviously good news.\n\nIBM is also entering the race with a SaaS solution (https://console.ng.bluemix.net/catalog/services/apache-spark) similar to Databricks, in terms of Notebook offering but it runs on their own infrastructure, which could be good and have good stability and all, but when dealing with bigdata and moving all that data from provider to provider, you better to the maths.\n\nFor that matter still prefer the Databricks model, and they have good news that until the end of the year they are planning to support a new CloudProvider.\n\n\n## Use Cases\n\nThere where lot's of use cases, in the several industries, even in the [Milk Industry](https://spark-summit.org/eu-2016/events/mmmooogle-from-big-data-to-decisions-for-dairy-cows/) that's right :)\n\nBut looking on the overall presented cases, is clear that more and more companies are adopting Streaming solutions for they business, as [Jacek Laskowski](https://youtu.be/mVP9sZ6K__Y) referred in his presentation. (Really recommend that one, a really appealing one)\n\nKafka is becoming a key-role figure in the several presented architectures.\n\nAnd it seems HDFS is becoming mostly used as a DataLake only solution, and starting using more high-speed memory layer as DataGrid, Alluxio solutions.\n\nThey also presented some comparations regarding direct access to Cloud Blob Storage compared with traditional FS [Steve Loughran](https://spark-summit.org/eu-2016/events/spark-and-object-stores-what-you-need-to-know/). Interesting values presented regarding Microsoft Cloud Solution vs AWS.\n\nOn other aspect that call my attention, was the lack of monitoring solutions for Spark workloads, and there where in fact several presentations regarding this subject, mostly use cases of in-house developments that fill this gap. [Simon Whitear](https://spark-summit.org/eu-2016/events/sparklint-a-tool-for-monitoring-identifying-and-tuning-inefficient-spark-jobs-across-your-cluster/) \nwas one of the best in my opinion.\n\nLambda,Gamma and Omega. With all these Streaming high-speed architectures showing on, one has to play it safe right. That's also one of the tendencies for the Industry to have these types of architectures with a speed layer and slow-layer. [William Benton](https://spark-summit.org/eu-2016/speakers/william-benton/) made a very enjoyable presentation regarding this topic.\n\n# Slides and Videos\n\nIt seems they updated the site and the videos and slides seem available, which is good news. \n\nCheck them if you like on the site Schedule page:\n\nhttps://spark-summit.org/eu-2016/schedule/\n\n\nCheers and keep Sparking","source":"_posts/SparkSummit2016.md","raw":"---\ntitle: Spark Summit 2016\ntags:\n  - Conferences\n  - Spark\n  - SparkSummit\n  - Hadoop\n  - Streamming\n  - Kafka\ndate: 2016-11-03 22:09:40\n---\n\n\nSo i've attended 25-27 OCT in Brussels to SparkSummit 2016 in Europe. So it kind makes sense to write down some impressions regarding the event.\n\nThe Event it self was pretty well organized, and the distribution of spaces was well planned.\n\nRegarding Spark it-self, is one of the ecosystem players (https://hadoopecosystemtable.github.io/) that's been getting more traction from the industry, partially because of the Enterprise support from Databricks and other major BigData contributors.\n\nThis Summit, had huge increase of participants, compared to last year, what guaranties that this is really here to stay.\n\nI always try to make of this conferences with two key points:\n\n* What's new ?\n* What are the other Companies doing, that's working for them ?\n\n## What new \n\nAMPLab, the creators of Spark from UC Berkeley presented some interesting projects for the future that would increase SparkEngine to a new level. They presented some comparation charts with execution times of this new engine compared with project Flink which uses real streaming and not micro-batching, which may come as a very good news, because there are more and more Streaming applications being implemented. Also some new layer of secure, the projects are called Drizzle and Opaque, let's hope they get this ready soon :)\n\nAlso one of the big announcements, was the new TensorFrames object. this output come's from a join work of TensorFlow and Databricks in order to bring DeepLearning to Spark. What this means is that in the Future one could simple use the Spark Framework in order to take advantage of GPU's, also that comes into alignment as you could expect with the new feature in Databricks Notebooks to support this.\n\nIt is still in Beta, but if you are using both TensorFlow and Spark, this is obviously good news.\n\nIBM is also entering the race with a SaaS solution (https://console.ng.bluemix.net/catalog/services/apache-spark) similar to Databricks, in terms of Notebook offering but it runs on their own infrastructure, which could be good and have good stability and all, but when dealing with bigdata and moving all that data from provider to provider, you better to the maths.\n\nFor that matter still prefer the Databricks model, and they have good news that until the end of the year they are planning to support a new CloudProvider.\n\n\n## Use Cases\n\nThere where lot's of use cases, in the several industries, even in the [Milk Industry](https://spark-summit.org/eu-2016/events/mmmooogle-from-big-data-to-decisions-for-dairy-cows/) that's right :)\n\nBut looking on the overall presented cases, is clear that more and more companies are adopting Streaming solutions for they business, as [Jacek Laskowski](https://youtu.be/mVP9sZ6K__Y) referred in his presentation. (Really recommend that one, a really appealing one)\n\nKafka is becoming a key-role figure in the several presented architectures.\n\nAnd it seems HDFS is becoming mostly used as a DataLake only solution, and starting using more high-speed memory layer as DataGrid, Alluxio solutions.\n\nThey also presented some comparations regarding direct access to Cloud Blob Storage compared with traditional FS [Steve Loughran](https://spark-summit.org/eu-2016/events/spark-and-object-stores-what-you-need-to-know/). Interesting values presented regarding Microsoft Cloud Solution vs AWS.\n\nOn other aspect that call my attention, was the lack of monitoring solutions for Spark workloads, and there where in fact several presentations regarding this subject, mostly use cases of in-house developments that fill this gap. [Simon Whitear](https://spark-summit.org/eu-2016/events/sparklint-a-tool-for-monitoring-identifying-and-tuning-inefficient-spark-jobs-across-your-cluster/) \nwas one of the best in my opinion.\n\nLambda,Gamma and Omega. With all these Streaming high-speed architectures showing on, one has to play it safe right. That's also one of the tendencies for the Industry to have these types of architectures with a speed layer and slow-layer. [William Benton](https://spark-summit.org/eu-2016/speakers/william-benton/) made a very enjoyable presentation regarding this topic.\n\n# Slides and Videos\n\nIt seems they updated the site and the videos and slides seem available, which is good news. \n\nCheck them if you like on the site Schedule page:\n\nhttps://spark-summit.org/eu-2016/schedule/\n\n\nCheers and keep Sparking","slug":"SparkSummit2016","published":1,"updated":"2020-09-06T14:38:37.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjj0009chpfe1hddzec","content":"<p>So i’ve attended 25-27 OCT in Brussels to SparkSummit 2016 in Europe. So it kind makes sense to write down some impressions regarding the event.</p>\n<p>The Event it self was pretty well organized, and the distribution of spaces was well planned.</p>\n<p>Regarding Spark it-self, is one of the ecosystem players (<a href=\"https://hadoopecosystemtable.github.io/\">https://hadoopecosystemtable.github.io/</a>) that’s been getting more traction from the industry, partially because of the Enterprise support from Databricks and other major BigData contributors.</p>\n<p>This Summit, had huge increase of participants, compared to last year, what guaranties that this is really here to stay.</p>\n<p>I always try to make of this conferences with two key points:</p>\n<ul>\n<li>What’s new ?</li>\n<li>What are the other Companies doing, that’s working for them ?</li>\n</ul>\n<h2 id=\"What-new\"><a href=\"#What-new\" class=\"headerlink\" title=\"What new\"></a>What new</h2><p>AMPLab, the creators of Spark from UC Berkeley presented some interesting projects for the future that would increase SparkEngine to a new level. They presented some comparation charts with execution times of this new engine compared with project Flink which uses real streaming and not micro-batching, which may come as a very good news, because there are more and more Streaming applications being implemented. Also some new layer of secure, the projects are called Drizzle and Opaque, let’s hope they get this ready soon :)</p>\n<p>Also one of the big announcements, was the new TensorFrames object. this output come’s from a join work of TensorFlow and Databricks in order to bring DeepLearning to Spark. What this means is that in the Future one could simple use the Spark Framework in order to take advantage of GPU’s, also that comes into alignment as you could expect with the new feature in Databricks Notebooks to support this.</p>\n<p>It is still in Beta, but if you are using both TensorFlow and Spark, this is obviously good news.</p>\n<p>IBM is also entering the race with a SaaS solution (<a href=\"https://console.ng.bluemix.net/catalog/services/apache-spark\">https://console.ng.bluemix.net/catalog/services/apache-spark</a>) similar to Databricks, in terms of Notebook offering but it runs on their own infrastructure, which could be good and have good stability and all, but when dealing with bigdata and moving all that data from provider to provider, you better to the maths.</p>\n<p>For that matter still prefer the Databricks model, and they have good news that until the end of the year they are planning to support a new CloudProvider.</p>\n<h2 id=\"Use-Cases\"><a href=\"#Use-Cases\" class=\"headerlink\" title=\"Use Cases\"></a>Use Cases</h2><p>There where lot’s of use cases, in the several industries, even in the <a href=\"https://spark-summit.org/eu-2016/events/mmmooogle-from-big-data-to-decisions-for-dairy-cows/\">Milk Industry</a> that’s right :)</p>\n<p>But looking on the overall presented cases, is clear that more and more companies are adopting Streaming solutions for they business, as <a href=\"https://youtu.be/mVP9sZ6K__Y\">Jacek Laskowski</a> referred in his presentation. (Really recommend that one, a really appealing one)</p>\n<p>Kafka is becoming a key-role figure in the several presented architectures.</p>\n<p>And it seems HDFS is becoming mostly used as a DataLake only solution, and starting using more high-speed memory layer as DataGrid, Alluxio solutions.</p>\n<p>They also presented some comparations regarding direct access to Cloud Blob Storage compared with traditional FS <a href=\"https://spark-summit.org/eu-2016/events/spark-and-object-stores-what-you-need-to-know/\">Steve Loughran</a>. Interesting values presented regarding Microsoft Cloud Solution vs AWS.</p>\n<p>On other aspect that call my attention, was the lack of monitoring solutions for Spark workloads, and there where in fact several presentations regarding this subject, mostly use cases of in-house developments that fill this gap. <a href=\"https://spark-summit.org/eu-2016/events/sparklint-a-tool-for-monitoring-identifying-and-tuning-inefficient-spark-jobs-across-your-cluster/\">Simon Whitear</a><br>was one of the best in my opinion.</p>\n<p>Lambda,Gamma and Omega. With all these Streaming high-speed architectures showing on, one has to play it safe right. That’s also one of the tendencies for the Industry to have these types of architectures with a speed layer and slow-layer. <a href=\"https://spark-summit.org/eu-2016/speakers/william-benton/\">William Benton</a> made a very enjoyable presentation regarding this topic.</p>\n<h1 id=\"Slides-and-Videos\"><a href=\"#Slides-and-Videos\" class=\"headerlink\" title=\"Slides and Videos\"></a>Slides and Videos</h1><p>It seems they updated the site and the videos and slides seem available, which is good news. </p>\n<p>Check them if you like on the site Schedule page:</p>\n<p><a href=\"https://spark-summit.org/eu-2016/schedule/\">https://spark-summit.org/eu-2016/schedule/</a></p>\n<p>Cheers and keep Sparking</p>\n","site":{"data":{}},"excerpt":"","more":"<p>So i’ve attended 25-27 OCT in Brussels to SparkSummit 2016 in Europe. So it kind makes sense to write down some impressions regarding the event.</p>\n<p>The Event it self was pretty well organized, and the distribution of spaces was well planned.</p>\n<p>Regarding Spark it-self, is one of the ecosystem players (<a href=\"https://hadoopecosystemtable.github.io/\">https://hadoopecosystemtable.github.io/</a>) that’s been getting more traction from the industry, partially because of the Enterprise support from Databricks and other major BigData contributors.</p>\n<p>This Summit, had huge increase of participants, compared to last year, what guaranties that this is really here to stay.</p>\n<p>I always try to make of this conferences with two key points:</p>\n<ul>\n<li>What’s new ?</li>\n<li>What are the other Companies doing, that’s working for them ?</li>\n</ul>\n<h2 id=\"What-new\"><a href=\"#What-new\" class=\"headerlink\" title=\"What new\"></a>What new</h2><p>AMPLab, the creators of Spark from UC Berkeley presented some interesting projects for the future that would increase SparkEngine to a new level. They presented some comparation charts with execution times of this new engine compared with project Flink which uses real streaming and not micro-batching, which may come as a very good news, because there are more and more Streaming applications being implemented. Also some new layer of secure, the projects are called Drizzle and Opaque, let’s hope they get this ready soon :)</p>\n<p>Also one of the big announcements, was the new TensorFrames object. this output come’s from a join work of TensorFlow and Databricks in order to bring DeepLearning to Spark. What this means is that in the Future one could simple use the Spark Framework in order to take advantage of GPU’s, also that comes into alignment as you could expect with the new feature in Databricks Notebooks to support this.</p>\n<p>It is still in Beta, but if you are using both TensorFlow and Spark, this is obviously good news.</p>\n<p>IBM is also entering the race with a SaaS solution (<a href=\"https://console.ng.bluemix.net/catalog/services/apache-spark\">https://console.ng.bluemix.net/catalog/services/apache-spark</a>) similar to Databricks, in terms of Notebook offering but it runs on their own infrastructure, which could be good and have good stability and all, but when dealing with bigdata and moving all that data from provider to provider, you better to the maths.</p>\n<p>For that matter still prefer the Databricks model, and they have good news that until the end of the year they are planning to support a new CloudProvider.</p>\n<h2 id=\"Use-Cases\"><a href=\"#Use-Cases\" class=\"headerlink\" title=\"Use Cases\"></a>Use Cases</h2><p>There where lot’s of use cases, in the several industries, even in the <a href=\"https://spark-summit.org/eu-2016/events/mmmooogle-from-big-data-to-decisions-for-dairy-cows/\">Milk Industry</a> that’s right :)</p>\n<p>But looking on the overall presented cases, is clear that more and more companies are adopting Streaming solutions for they business, as <a href=\"https://youtu.be/mVP9sZ6K__Y\">Jacek Laskowski</a> referred in his presentation. (Really recommend that one, a really appealing one)</p>\n<p>Kafka is becoming a key-role figure in the several presented architectures.</p>\n<p>And it seems HDFS is becoming mostly used as a DataLake only solution, and starting using more high-speed memory layer as DataGrid, Alluxio solutions.</p>\n<p>They also presented some comparations regarding direct access to Cloud Blob Storage compared with traditional FS <a href=\"https://spark-summit.org/eu-2016/events/spark-and-object-stores-what-you-need-to-know/\">Steve Loughran</a>. Interesting values presented regarding Microsoft Cloud Solution vs AWS.</p>\n<p>On other aspect that call my attention, was the lack of monitoring solutions for Spark workloads, and there where in fact several presentations regarding this subject, mostly use cases of in-house developments that fill this gap. <a href=\"https://spark-summit.org/eu-2016/events/sparklint-a-tool-for-monitoring-identifying-and-tuning-inefficient-spark-jobs-across-your-cluster/\">Simon Whitear</a><br>was one of the best in my opinion.</p>\n<p>Lambda,Gamma and Omega. With all these Streaming high-speed architectures showing on, one has to play it safe right. That’s also one of the tendencies for the Industry to have these types of architectures with a speed layer and slow-layer. <a href=\"https://spark-summit.org/eu-2016/speakers/william-benton/\">William Benton</a> made a very enjoyable presentation regarding this topic.</p>\n<h1 id=\"Slides-and-Videos\"><a href=\"#Slides-and-Videos\" class=\"headerlink\" title=\"Slides and Videos\"></a>Slides and Videos</h1><p>It seems they updated the site and the videos and slides seem available, which is good news. </p>\n<p>Check them if you like on the site Schedule page:</p>\n<p><a href=\"https://spark-summit.org/eu-2016/schedule/\">https://spark-summit.org/eu-2016/schedule/</a></p>\n<p>Cheers and keep Sparking</p>\n"},{"title":"Ansible-Cmdb","date":"2017-10-09T21:17:35.000Z","_content":"\n\nThis article is about [ansible-cmdb](https://github.com/fboender/ansible-cmdb) a very nice solution for gathering your ansible facts into a visual format.\n\n{% blockquote Offical Website Definition%}\nAnsible-cmdb takes the output of Ansible's fact gathering and converts it into a static HTML overview page (and other things) containing system configuration information.\n\nIt supports multiple types of output (html, csv, sql, etc) and extending information gathered by Ansible with custom data. For each host it also shows the groups, host variables, custom variables and machine-local facts.\n{% endblockquote %}\n\n# Supported output formats / templates:\n\n* Fancy HTML (--template html_fancy), as seen in the screenshots above.\n* Fancy HTML Split (--template html_fancy_split), with each host's details in a separate file (for large number of hosts).\n* CSV (--template csv), the trustworthy and flexible comma-separated format.\n* JSON (--template json), a dump of all facts in JSON format.\n* Markdown (--template markdown), useful for copy-pasting into Wiki's and such.\n* Markdown Split ('--template markdown_split'), with each host's details in a seperate file (for large number of hosts).\n* SQL (--template sql), for importing host facts into a (My)SQL database.\nPlain Text table (--template txt_table), for the console gurus.\nand of course, any custom template you're willing to make.\n\n# Setup\n\nAnd it's so simple to setup :D\n\n```\nsudo pip install ansible-cmdb\n```\n\n**Note:** You could also use your distribution prefeered method. check the offical site.\n\n# Creating Reports\n\nSimple console generation\n\n```\nansible-cmdb -t txt_table --columns name,os,ip,mem,cpus out\n```\n\nThe HTML Fancy one\n\n```\nansible-cmdb --template txt_table out  --template html_fancy\n```\n\nOfficial Screenshoot\n\n{% asset_img \"screenshot-overview.png\"  %}\n\n\n# Conclusion\n\nI found this software quite simple and very usefull. I found that `--template txt_table` is quite handy to use on Documentation for small enviroments. For bigger projects the CSV could be a better choice. Also the fancy HTML templates are very usefull on playbook development phases where you can confirm your facts very easily. Ansible-cmdb started as a short Python script, but many people made contributions so far.\n\nCheers,\nRR\n\n# References\n* https://github.com/fboender/ansible-cmdb","source":"_posts/ansible-cmdb.md","raw":"---\ntitle: ansible-cmdb\ntags:\n  - Ansible\ndate: 2017-10-09 22:17:35\n---\n\n\nThis article is about [ansible-cmdb](https://github.com/fboender/ansible-cmdb) a very nice solution for gathering your ansible facts into a visual format.\n\n{% blockquote Offical Website Definition%}\nAnsible-cmdb takes the output of Ansible's fact gathering and converts it into a static HTML overview page (and other things) containing system configuration information.\n\nIt supports multiple types of output (html, csv, sql, etc) and extending information gathered by Ansible with custom data. For each host it also shows the groups, host variables, custom variables and machine-local facts.\n{% endblockquote %}\n\n# Supported output formats / templates:\n\n* Fancy HTML (--template html_fancy), as seen in the screenshots above.\n* Fancy HTML Split (--template html_fancy_split), with each host's details in a separate file (for large number of hosts).\n* CSV (--template csv), the trustworthy and flexible comma-separated format.\n* JSON (--template json), a dump of all facts in JSON format.\n* Markdown (--template markdown), useful for copy-pasting into Wiki's and such.\n* Markdown Split ('--template markdown_split'), with each host's details in a seperate file (for large number of hosts).\n* SQL (--template sql), for importing host facts into a (My)SQL database.\nPlain Text table (--template txt_table), for the console gurus.\nand of course, any custom template you're willing to make.\n\n# Setup\n\nAnd it's so simple to setup :D\n\n```\nsudo pip install ansible-cmdb\n```\n\n**Note:** You could also use your distribution prefeered method. check the offical site.\n\n# Creating Reports\n\nSimple console generation\n\n```\nansible-cmdb -t txt_table --columns name,os,ip,mem,cpus out\n```\n\nThe HTML Fancy one\n\n```\nansible-cmdb --template txt_table out  --template html_fancy\n```\n\nOfficial Screenshoot\n\n{% asset_img \"screenshot-overview.png\"  %}\n\n\n# Conclusion\n\nI found this software quite simple and very usefull. I found that `--template txt_table` is quite handy to use on Documentation for small enviroments. For bigger projects the CSV could be a better choice. Also the fancy HTML templates are very usefull on playbook development phases where you can confirm your facts very easily. Ansible-cmdb started as a short Python script, but many people made contributions so far.\n\nCheers,\nRR\n\n# References\n* https://github.com/fboender/ansible-cmdb","slug":"ansible-cmdb","published":1,"updated":"2020-09-06T14:38:37.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjk000achpf0xsn4xkr","content":"<p>This article is about <a href=\"https://github.com/fboender/ansible-cmdb\">ansible-cmdb</a> a very nice solution for gathering your ansible facts into a visual format.</p>\n<blockquote><p>Ansible-cmdb takes the output of Ansible’s fact gathering and converts it into a static HTML overview page (and other things) containing system configuration information.</p>\n<p>It supports multiple types of output (html, csv, sql, etc) and extending information gathered by Ansible with custom data. For each host it also shows the groups, host variables, custom variables and machine-local facts.</p>\n<footer><strong>Offical Website Definition</strong></footer></blockquote>\n\n<h1 id=\"Supported-output-formats-templates\"><a href=\"#Supported-output-formats-templates\" class=\"headerlink\" title=\"Supported output formats / templates:\"></a>Supported output formats / templates:</h1><ul>\n<li>Fancy HTML (–template html_fancy), as seen in the screenshots above.</li>\n<li>Fancy HTML Split (–template html_fancy_split), with each host’s details in a separate file (for large number of hosts).</li>\n<li>CSV (–template csv), the trustworthy and flexible comma-separated format.</li>\n<li>JSON (–template json), a dump of all facts in JSON format.</li>\n<li>Markdown (–template markdown), useful for copy-pasting into Wiki’s and such.</li>\n<li>Markdown Split (‘–template markdown_split’), with each host’s details in a seperate file (for large number of hosts).</li>\n<li>SQL (–template sql), for importing host facts into a (My)SQL database.<br>Plain Text table (–template txt_table), for the console gurus.<br>and of course, any custom template you’re willing to make.</li>\n</ul>\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><p>And it’s so simple to setup :D</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pip install ansible-cmdb</span><br></pre></td></tr></table></figure>\n<p><strong>Note:</strong> You could also use your distribution prefeered method. check the offical site.</p>\n<h1 id=\"Creating-Reports\"><a href=\"#Creating-Reports\" class=\"headerlink\" title=\"Creating Reports\"></a>Creating Reports</h1><p>Simple console generation</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ansible-cmdb -t txt_table --columns name,os,ip,mem,cpus out</span><br></pre></td></tr></table></figure>\n<p>The HTML Fancy one</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ansible-cmdb --template txt_table out  --template html_fancy</span><br></pre></td></tr></table></figure>\n<p>Official Screenshoot</p>\n<img src=\"/2017/10/09/ansible-cmdb/screenshot-overview.png\" class=\"\">\n\n\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>I found this software quite simple and very usefull. I found that <code>--template txt_table</code> is quite handy to use on Documentation for small enviroments. For bigger projects the CSV could be a better choice. Also the fancy HTML templates are very usefull on playbook development phases where you can confirm your facts very easily. Ansible-cmdb started as a short Python script, but many people made contributions so far.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://github.com/fboender/ansible-cmdb\">https://github.com/fboender/ansible-cmdb</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>This article is about <a href=\"https://github.com/fboender/ansible-cmdb\">ansible-cmdb</a> a very nice solution for gathering your ansible facts into a visual format.</p>\n<blockquote><p>Ansible-cmdb takes the output of Ansible’s fact gathering and converts it into a static HTML overview page (and other things) containing system configuration information.</p>\n<p>It supports multiple types of output (html, csv, sql, etc) and extending information gathered by Ansible with custom data. For each host it also shows the groups, host variables, custom variables and machine-local facts.</p>\n<footer><strong>Offical Website Definition</strong></footer></blockquote>\n\n<h1 id=\"Supported-output-formats-templates\"><a href=\"#Supported-output-formats-templates\" class=\"headerlink\" title=\"Supported output formats / templates:\"></a>Supported output formats / templates:</h1><ul>\n<li>Fancy HTML (–template html_fancy), as seen in the screenshots above.</li>\n<li>Fancy HTML Split (–template html_fancy_split), with each host’s details in a separate file (for large number of hosts).</li>\n<li>CSV (–template csv), the trustworthy and flexible comma-separated format.</li>\n<li>JSON (–template json), a dump of all facts in JSON format.</li>\n<li>Markdown (–template markdown), useful for copy-pasting into Wiki’s and such.</li>\n<li>Markdown Split (‘–template markdown_split’), with each host’s details in a seperate file (for large number of hosts).</li>\n<li>SQL (–template sql), for importing host facts into a (My)SQL database.<br>Plain Text table (–template txt_table), for the console gurus.<br>and of course, any custom template you’re willing to make.</li>\n</ul>\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><p>And it’s so simple to setup :D</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo pip install ansible-cmdb</span><br></pre></td></tr></table></figure>\n<p><strong>Note:</strong> You could also use your distribution prefeered method. check the offical site.</p>\n<h1 id=\"Creating-Reports\"><a href=\"#Creating-Reports\" class=\"headerlink\" title=\"Creating Reports\"></a>Creating Reports</h1><p>Simple console generation</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ansible-cmdb -t txt_table --columns name,os,ip,mem,cpus out</span><br></pre></td></tr></table></figure>\n<p>The HTML Fancy one</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ansible-cmdb --template txt_table out  --template html_fancy</span><br></pre></td></tr></table></figure>\n<p>Official Screenshoot</p>\n<img src=\"/2017/10/09/ansible-cmdb/screenshot-overview.png\" class=\"\">\n\n\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>I found this software quite simple and very usefull. I found that <code>--template txt_table</code> is quite handy to use on Documentation for small enviroments. For bigger projects the CSV could be a better choice. Also the fancy HTML templates are very usefull on playbook development phases where you can confirm your facts very easily. Ansible-cmdb started as a short Python script, but many people made contributions so far.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://github.com/fboender/ansible-cmdb\">https://github.com/fboender/ansible-cmdb</a></li>\n</ul>\n"},{"title":"Clean All Dockers and Configure With Btrfs","date":"2017-09-19T07:37:29.000Z","_content":"\n\nSo dockers tend to start growing like weeds. And cost me space i don't have. But one could change the storage driver. You could choose LVM and take advantage of the snapshots or btrfs which i allready have.\n\n## Btrfs  \n\nBtrfs is a next generation copy-on-write filesystem that supports many advanced storage technologies that make it a good fit for Docker. Btrfs is included in the mainline Linux kernel.\n\nDocker’s btrfs storage driver leverages many Btrfs features for image and container management. Among these features are block-level operations, thin provisioning, copy-on-write snapshots, and ease of administration. You can easily combine multiple physical block devices into a single Btrfs filesystem.\n\n## Requirements\n\n* Install btrfs \n* Make sure you have a volume formated as btrfs (not part of the article)\n\n## Clean all dockers and images\n\n```\n# Delete all containers\nsudo docker rm $(sudo docker ps -a -q)\n# Delete all images\nsudo docker rmi $(sudo docker images -q)\n```\n\n\n## Setup\n\nMake sure you have btrfs on your system\n\n```\nsudo cat /proc/filesystems | grep btrfs\n```\n\n1. Stop docker\n\n```\nsudo service docker stop\n```\n\n2. Create a backup of you docker settings\n\n```\nsudo mv /var/lib/docker/ /var/lib/docker.bak\n```\n\n3. create a subvolume from an existing btrfs FS\n\n```\nbtrfs subvolume create /archive/dockers\n```\n\n**NOTE:** It's better to use a dedicate disk for it\n\n4. Create the symlink\n\n```\nln -s /archive/dockers /var/lib/docker\n```\n\n5. Copy the backup data to the new location\n\n```\ncp -rp /var/lib/docker.bak/* /var/lib/docker/\n```\n\n6. Configure Docker to use the btrfs storage driver. This is required even though `/var/lib/docker/` is now using a Btrfs filesystem. Edit or create the file `/etc/docker/daemon.json`.\n\n```\n{\n  \"storage-driver\": \"btrfs\"\n}\n```\n\n7. Start docker service\n\n```\nsudo service docker start\n```\n\n8. Check if docker is running with docker support\n\n```\nsudo docker info|grep  \"Storage Driver\"\n```\n\nYou could check the volumes being created with the command\n\n```\nsudo btrfs subvolume list /var/liv/docker |grep dockers\n```\n\n\n## References\n\n* https://docs.docker.com/engine/userguide/storagedriver/btrfs-driver/\n\n","source":"_posts/dockerclean.md","raw":"---\ntitle: Clean all Dockers and configure with btrfs\ntags:\n  - Docker\n  - Linux\n  - btrfs\ndate: 2017-09-19 08:37:29\n---\n\n\nSo dockers tend to start growing like weeds. And cost me space i don't have. But one could change the storage driver. You could choose LVM and take advantage of the snapshots or btrfs which i allready have.\n\n## Btrfs  \n\nBtrfs is a next generation copy-on-write filesystem that supports many advanced storage technologies that make it a good fit for Docker. Btrfs is included in the mainline Linux kernel.\n\nDocker’s btrfs storage driver leverages many Btrfs features for image and container management. Among these features are block-level operations, thin provisioning, copy-on-write snapshots, and ease of administration. You can easily combine multiple physical block devices into a single Btrfs filesystem.\n\n## Requirements\n\n* Install btrfs \n* Make sure you have a volume formated as btrfs (not part of the article)\n\n## Clean all dockers and images\n\n```\n# Delete all containers\nsudo docker rm $(sudo docker ps -a -q)\n# Delete all images\nsudo docker rmi $(sudo docker images -q)\n```\n\n\n## Setup\n\nMake sure you have btrfs on your system\n\n```\nsudo cat /proc/filesystems | grep btrfs\n```\n\n1. Stop docker\n\n```\nsudo service docker stop\n```\n\n2. Create a backup of you docker settings\n\n```\nsudo mv /var/lib/docker/ /var/lib/docker.bak\n```\n\n3. create a subvolume from an existing btrfs FS\n\n```\nbtrfs subvolume create /archive/dockers\n```\n\n**NOTE:** It's better to use a dedicate disk for it\n\n4. Create the symlink\n\n```\nln -s /archive/dockers /var/lib/docker\n```\n\n5. Copy the backup data to the new location\n\n```\ncp -rp /var/lib/docker.bak/* /var/lib/docker/\n```\n\n6. Configure Docker to use the btrfs storage driver. This is required even though `/var/lib/docker/` is now using a Btrfs filesystem. Edit or create the file `/etc/docker/daemon.json`.\n\n```\n{\n  \"storage-driver\": \"btrfs\"\n}\n```\n\n7. Start docker service\n\n```\nsudo service docker start\n```\n\n8. Check if docker is running with docker support\n\n```\nsudo docker info|grep  \"Storage Driver\"\n```\n\nYou could check the volumes being created with the command\n\n```\nsudo btrfs subvolume list /var/liv/docker |grep dockers\n```\n\n\n## References\n\n* https://docs.docker.com/engine/userguide/storagedriver/btrfs-driver/\n\n","slug":"dockerclean","published":1,"updated":"2020-09-06T14:38:37.851Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjl000bchpf1i8v4pyn","content":"<p>So dockers tend to start growing like weeds. And cost me space i don’t have. But one could change the storage driver. You could choose LVM and take advantage of the snapshots or btrfs which i allready have.</p>\n<h2 id=\"Btrfs\"><a href=\"#Btrfs\" class=\"headerlink\" title=\"Btrfs\"></a>Btrfs</h2><p>Btrfs is a next generation copy-on-write filesystem that supports many advanced storage technologies that make it a good fit for Docker. Btrfs is included in the mainline Linux kernel.</p>\n<p>Docker’s btrfs storage driver leverages many Btrfs features for image and container management. Among these features are block-level operations, thin provisioning, copy-on-write snapshots, and ease of administration. You can easily combine multiple physical block devices into a single Btrfs filesystem.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><ul>\n<li>Install btrfs </li>\n<li>Make sure you have a volume formated as btrfs (not part of the article)</li>\n</ul>\n<h2 id=\"Clean-all-dockers-and-images\"><a href=\"#Clean-all-dockers-and-images\" class=\"headerlink\" title=\"Clean all dockers and images\"></a>Clean all dockers and images</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Delete all containers</span><br><span class=\"line\">sudo docker rm $(sudo docker ps -a -q)</span><br><span class=\"line\"># Delete all images</span><br><span class=\"line\">sudo docker rmi $(sudo docker images -q)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><p>Make sure you have btrfs on your system</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo cat &#x2F;proc&#x2F;filesystems | grep btrfs</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Stop docker</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo service docker stop</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>Create a backup of you docker settings</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo mv &#x2F;var&#x2F;lib&#x2F;docker&#x2F; &#x2F;var&#x2F;lib&#x2F;docker.bak</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>create a subvolume from an existing btrfs FS</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">btrfs subvolume create &#x2F;archive&#x2F;dockers</span><br></pre></td></tr></table></figure>\n<p><strong>NOTE:</strong> It’s better to use a dedicate disk for it</p>\n<ol start=\"4\">\n<li>Create the symlink</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln -s &#x2F;archive&#x2F;dockers &#x2F;var&#x2F;lib&#x2F;docker</span><br></pre></td></tr></table></figure>\n<ol start=\"5\">\n<li>Copy the backup data to the new location</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp -rp &#x2F;var&#x2F;lib&#x2F;docker.bak&#x2F;* &#x2F;var&#x2F;lib&#x2F;docker&#x2F;</span><br></pre></td></tr></table></figure>\n<ol start=\"6\">\n<li>Configure Docker to use the btrfs storage driver. This is required even though <code>/var/lib/docker/</code> is now using a Btrfs filesystem. Edit or create the file <code>/etc/docker/daemon.json</code>.</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;storage-driver&quot;: &quot;btrfs&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ol start=\"7\">\n<li>Start docker service</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo service docker start</span><br></pre></td></tr></table></figure>\n<ol start=\"8\">\n<li>Check if docker is running with docker support</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker info|grep  &quot;Storage Driver&quot;</span><br></pre></td></tr></table></figure>\n<p>You could check the volumes being created with the command</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo btrfs subvolume list &#x2F;var&#x2F;liv&#x2F;docker |grep dockers</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://docs.docker.com/engine/userguide/storagedriver/btrfs-driver/\">https://docs.docker.com/engine/userguide/storagedriver/btrfs-driver/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>So dockers tend to start growing like weeds. And cost me space i don’t have. But one could change the storage driver. You could choose LVM and take advantage of the snapshots or btrfs which i allready have.</p>\n<h2 id=\"Btrfs\"><a href=\"#Btrfs\" class=\"headerlink\" title=\"Btrfs\"></a>Btrfs</h2><p>Btrfs is a next generation copy-on-write filesystem that supports many advanced storage technologies that make it a good fit for Docker. Btrfs is included in the mainline Linux kernel.</p>\n<p>Docker’s btrfs storage driver leverages many Btrfs features for image and container management. Among these features are block-level operations, thin provisioning, copy-on-write snapshots, and ease of administration. You can easily combine multiple physical block devices into a single Btrfs filesystem.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><ul>\n<li>Install btrfs </li>\n<li>Make sure you have a volume formated as btrfs (not part of the article)</li>\n</ul>\n<h2 id=\"Clean-all-dockers-and-images\"><a href=\"#Clean-all-dockers-and-images\" class=\"headerlink\" title=\"Clean all dockers and images\"></a>Clean all dockers and images</h2><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"># Delete all containers</span><br><span class=\"line\">sudo docker rm $(sudo docker ps -a -q)</span><br><span class=\"line\"># Delete all images</span><br><span class=\"line\">sudo docker rmi $(sudo docker images -q)</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><p>Make sure you have btrfs on your system</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo cat &#x2F;proc&#x2F;filesystems | grep btrfs</span><br></pre></td></tr></table></figure>\n<ol>\n<li>Stop docker</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo service docker stop</span><br></pre></td></tr></table></figure>\n<ol start=\"2\">\n<li>Create a backup of you docker settings</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo mv &#x2F;var&#x2F;lib&#x2F;docker&#x2F; &#x2F;var&#x2F;lib&#x2F;docker.bak</span><br></pre></td></tr></table></figure>\n<ol start=\"3\">\n<li>create a subvolume from an existing btrfs FS</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">btrfs subvolume create &#x2F;archive&#x2F;dockers</span><br></pre></td></tr></table></figure>\n<p><strong>NOTE:</strong> It’s better to use a dedicate disk for it</p>\n<ol start=\"4\">\n<li>Create the symlink</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ln -s &#x2F;archive&#x2F;dockers &#x2F;var&#x2F;lib&#x2F;docker</span><br></pre></td></tr></table></figure>\n<ol start=\"5\">\n<li>Copy the backup data to the new location</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp -rp &#x2F;var&#x2F;lib&#x2F;docker.bak&#x2F;* &#x2F;var&#x2F;lib&#x2F;docker&#x2F;</span><br></pre></td></tr></table></figure>\n<ol start=\"6\">\n<li>Configure Docker to use the btrfs storage driver. This is required even though <code>/var/lib/docker/</code> is now using a Btrfs filesystem. Edit or create the file <code>/etc/docker/daemon.json</code>.</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#123;</span><br><span class=\"line\">  &quot;storage-driver&quot;: &quot;btrfs&quot;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n<ol start=\"7\">\n<li>Start docker service</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo service docker start</span><br></pre></td></tr></table></figure>\n<ol start=\"8\">\n<li>Check if docker is running with docker support</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker info|grep  &quot;Storage Driver&quot;</span><br></pre></td></tr></table></figure>\n<p>You could check the volumes being created with the command</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo btrfs subvolume list &#x2F;var&#x2F;liv&#x2F;docker |grep dockers</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://docs.docker.com/engine/userguide/storagedriver/btrfs-driver/\">https://docs.docker.com/engine/userguide/storagedriver/btrfs-driver/</a></li>\n</ul>\n"},{"title":"Dockers","date":"2017-10-18T00:26:17.000Z","_content":"\n\nThis article is just to share a list of usefull dockers, in case one wants to speed test some feature and required some of this software to be up and running.\n\n# Why ?\n\nThe main ideia is that i can fireup quickly without having to worry on the ports that are allready mapped on my local machine, so the several dockers should have colision ports taken care of.\n\nOne should just need to execute `docker-compose up` and that's it.\n\nYou could also use other repo, but i'll try to make my local configurations in a way that i could interuse several dockers.\n\n# Dockers repo\n\nYou just need to clone the repo https://github.com/rramos/dockers\n\n```\ngit clone https://github.com/rramos/dockers.git\n```\n\n\n# Requirements\n\nMake sure you have docker and docker compose properly configured\n\n# Setup\n\nJust fire-up `docker-compose` and that's it \n\n```\ncd docker-portainer\ndocker-compose up -d\n```\n\nEach docker has it's own README file with more instructions.\n\n# Docker List\n\n* docker-grafana  \n* docker-hdp      \n* docker-ksql    \n* docker-nifi         \n* docker-portainer  \n* docker-zeppelin  \n* docker-hbase    \n* docker-jenkins  \n* docker-moodle  \n* docker-openproject  \n* docker-rundeck \n\nThis is the available list at the moment of this article, i will add up more as i'm using and testing new stuff.\n\n**NOTE:** Use this containers at your own risk \n\n\nCheers,\nRR\n","source":"_posts/dockers.md","raw":"---\ntitle: Dockers\ntags:\n  - Docker\n  - docker-compose\n  - utils\ndate: 2017-10-18 01:26:17\n---\n\n\nThis article is just to share a list of usefull dockers, in case one wants to speed test some feature and required some of this software to be up and running.\n\n# Why ?\n\nThe main ideia is that i can fireup quickly without having to worry on the ports that are allready mapped on my local machine, so the several dockers should have colision ports taken care of.\n\nOne should just need to execute `docker-compose up` and that's it.\n\nYou could also use other repo, but i'll try to make my local configurations in a way that i could interuse several dockers.\n\n# Dockers repo\n\nYou just need to clone the repo https://github.com/rramos/dockers\n\n```\ngit clone https://github.com/rramos/dockers.git\n```\n\n\n# Requirements\n\nMake sure you have docker and docker compose properly configured\n\n# Setup\n\nJust fire-up `docker-compose` and that's it \n\n```\ncd docker-portainer\ndocker-compose up -d\n```\n\nEach docker has it's own README file with more instructions.\n\n# Docker List\n\n* docker-grafana  \n* docker-hdp      \n* docker-ksql    \n* docker-nifi         \n* docker-portainer  \n* docker-zeppelin  \n* docker-hbase    \n* docker-jenkins  \n* docker-moodle  \n* docker-openproject  \n* docker-rundeck \n\nThis is the available list at the moment of this article, i will add up more as i'm using and testing new stuff.\n\n**NOTE:** Use this containers at your own risk \n\n\nCheers,\nRR\n","slug":"dockers","published":1,"updated":"2020-09-06T14:38:37.855Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjl000cchpfakiggudq","content":"<p>This article is just to share a list of usefull dockers, in case one wants to speed test some feature and required some of this software to be up and running.</p>\n<h1 id=\"Why\"><a href=\"#Why\" class=\"headerlink\" title=\"Why ?\"></a>Why ?</h1><p>The main ideia is that i can fireup quickly without having to worry on the ports that are allready mapped on my local machine, so the several dockers should have colision ports taken care of.</p>\n<p>One should just need to execute <code>docker-compose up</code> and that’s it.</p>\n<p>You could also use other repo, but i’ll try to make my local configurations in a way that i could interuse several dockers.</p>\n<h1 id=\"Dockers-repo\"><a href=\"#Dockers-repo\" class=\"headerlink\" title=\"Dockers repo\"></a>Dockers repo</h1><p>You just need to clone the repo <a href=\"https://github.com/rramos/dockers\">https://github.com/rramos/dockers</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;rramos&#x2F;dockers.git</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h1><p>Make sure you have docker and docker compose properly configured</p>\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><p>Just fire-up <code>docker-compose</code> and that’s it </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd docker-portainer</span><br><span class=\"line\">docker-compose up -d</span><br></pre></td></tr></table></figure>\n<p>Each docker has it’s own README file with more instructions.</p>\n<h1 id=\"Docker-List\"><a href=\"#Docker-List\" class=\"headerlink\" title=\"Docker List\"></a>Docker List</h1><ul>\n<li>docker-grafana  </li>\n<li>docker-hdp      </li>\n<li>docker-ksql    </li>\n<li>docker-nifi         </li>\n<li>docker-portainer  </li>\n<li>docker-zeppelin  </li>\n<li>docker-hbase    </li>\n<li>docker-jenkins  </li>\n<li>docker-moodle  </li>\n<li>docker-openproject  </li>\n<li>docker-rundeck </li>\n</ul>\n<p>This is the available list at the moment of this article, i will add up more as i’m using and testing new stuff.</p>\n<p><strong>NOTE:</strong> Use this containers at your own risk </p>\n<p>Cheers,<br>RR</p>\n","site":{"data":{}},"excerpt":"","more":"<p>This article is just to share a list of usefull dockers, in case one wants to speed test some feature and required some of this software to be up and running.</p>\n<h1 id=\"Why\"><a href=\"#Why\" class=\"headerlink\" title=\"Why ?\"></a>Why ?</h1><p>The main ideia is that i can fireup quickly without having to worry on the ports that are allready mapped on my local machine, so the several dockers should have colision ports taken care of.</p>\n<p>One should just need to execute <code>docker-compose up</code> and that’s it.</p>\n<p>You could also use other repo, but i’ll try to make my local configurations in a way that i could interuse several dockers.</p>\n<h1 id=\"Dockers-repo\"><a href=\"#Dockers-repo\" class=\"headerlink\" title=\"Dockers repo\"></a>Dockers repo</h1><p>You just need to clone the repo <a href=\"https://github.com/rramos/dockers\">https://github.com/rramos/dockers</a></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone https:&#x2F;&#x2F;github.com&#x2F;rramos&#x2F;dockers.git</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h1><p>Make sure you have docker and docker compose properly configured</p>\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><p>Just fire-up <code>docker-compose</code> and that’s it </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cd docker-portainer</span><br><span class=\"line\">docker-compose up -d</span><br></pre></td></tr></table></figure>\n<p>Each docker has it’s own README file with more instructions.</p>\n<h1 id=\"Docker-List\"><a href=\"#Docker-List\" class=\"headerlink\" title=\"Docker List\"></a>Docker List</h1><ul>\n<li>docker-grafana  </li>\n<li>docker-hdp      </li>\n<li>docker-ksql    </li>\n<li>docker-nifi         </li>\n<li>docker-portainer  </li>\n<li>docker-zeppelin  </li>\n<li>docker-hbase    </li>\n<li>docker-jenkins  </li>\n<li>docker-moodle  </li>\n<li>docker-openproject  </li>\n<li>docker-rundeck </li>\n</ul>\n<p>This is the available list at the moment of this article, i will add up more as i’m using and testing new stuff.</p>\n<p><strong>NOTE:</strong> Use this containers at your own risk </p>\n<p>Cheers,<br>RR</p>\n"},{"title":"Dr. Elephant Overview","date":"2017-10-13T01:09:12.000Z","_content":"\n\nThis article would be about [dr-elephant](https://github.com/linkedin/dr-elephant) A Performance and Monitoring tool for Hadoop and Spark.\n\n{% blockquote Offical Website Definition%}\n\nDr. Elephant is a performance monitoring and tuning tool for Hadoop and Spark. It automatically gathers all the metrics, runs analysis on them, and presents them in a simple way for easy consumption. Its goal is to improve developer productivity and increase cluster efficiency by making it easier to tune the jobs. It analyzes the Hadoop and Spark jobs using a set of pluggable, configurable, rule-based heuristics that provide insights on how a job performed, and then uses the results to make suggestions about how to tune the job to make it perform more efficiently.\n\n{% endblockquote %}\n\n# Requirements\n\n* Install mysql-server and create a BD for dr-elephant\n```sh\nsudo apt-get install mysql-server \n```\n\n* MySQl preparation\n\n```mysql\nmysql> create database drelephant;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> grant all on drelephant.* to drelephant@localhost identified by 'drelephant';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> flush privileges;\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n* Install zip command\n```sh\nsudo apt-get install zip\n```\n\n* Install sbt\n```sh\necho \"deb https://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\nsudo apt-get update\nsudo apt-get install sbt\n```\n\n\n# Setup\n\n* Clone the repo\n\n```sh\ngit https://github.com/linkedin/dr-elephant.git\n```\n\n* Compile\n\n```sh\nsbt package\nsbt dist\ncp ./target/universal/dr-elephant-2.0.3-SNAPSHOT.zip .\n./compile.sh\ncd dist\nunzip dr-elephant-2.0.3-SNAPSHOT.zip\n```\n\n* Starting the service\n\n```sh\nexport ELEPHANT_CONF_DIR=../../app-conf\n./bin/start.sh \n```\n\nOne can now access the web interface at: http://localhost:8080\n\n{% asset_img DrElephant-2-dashboard.jpg [Dashboard] %}\n\n# Conclusion\n\nThis tool seems very powerfull. At the moment i haven't tested changing the recommendations it provided, but will try them soon. Spark 2.x applications don't seem to be working at the momment\n\n# Extended tests\n\n* Test Oozie Scheduler integration\n* Test Airflow integration\n* Define a deployment strategy\n* Test recommended changes\n\n\n# References\n \n  * https://github.com/linkedin/dr-elephant\n  * https://github.com/linkedin/dr-elephant/commit/7d9e34ecdbe36a652ca4ad2db852df08da57050a\n  * https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hadoop-spark\n","source":"_posts/drelephat.md","raw":"---\ntitle: Dr. Elephant Overview\ntags:\n  - dr-elephant\n  - Hadoop\n  - Performance\n  - Monitoring\n  - Tunning\n  - Spark\ndate: 2017-10-13 02:09:12\n---\n\n\nThis article would be about [dr-elephant](https://github.com/linkedin/dr-elephant) A Performance and Monitoring tool for Hadoop and Spark.\n\n{% blockquote Offical Website Definition%}\n\nDr. Elephant is a performance monitoring and tuning tool for Hadoop and Spark. It automatically gathers all the metrics, runs analysis on them, and presents them in a simple way for easy consumption. Its goal is to improve developer productivity and increase cluster efficiency by making it easier to tune the jobs. It analyzes the Hadoop and Spark jobs using a set of pluggable, configurable, rule-based heuristics that provide insights on how a job performed, and then uses the results to make suggestions about how to tune the job to make it perform more efficiently.\n\n{% endblockquote %}\n\n# Requirements\n\n* Install mysql-server and create a BD for dr-elephant\n```sh\nsudo apt-get install mysql-server \n```\n\n* MySQl preparation\n\n```mysql\nmysql> create database drelephant;\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> grant all on drelephant.* to drelephant@localhost identified by 'drelephant';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> flush privileges;\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n* Install zip command\n```sh\nsudo apt-get install zip\n```\n\n* Install sbt\n```sh\necho \"deb https://dl.bintray.com/sbt/debian /\" | sudo tee -a /etc/apt/sources.list.d/sbt.list\nsudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823\nsudo apt-get update\nsudo apt-get install sbt\n```\n\n\n# Setup\n\n* Clone the repo\n\n```sh\ngit https://github.com/linkedin/dr-elephant.git\n```\n\n* Compile\n\n```sh\nsbt package\nsbt dist\ncp ./target/universal/dr-elephant-2.0.3-SNAPSHOT.zip .\n./compile.sh\ncd dist\nunzip dr-elephant-2.0.3-SNAPSHOT.zip\n```\n\n* Starting the service\n\n```sh\nexport ELEPHANT_CONF_DIR=../../app-conf\n./bin/start.sh \n```\n\nOne can now access the web interface at: http://localhost:8080\n\n{% asset_img DrElephant-2-dashboard.jpg [Dashboard] %}\n\n# Conclusion\n\nThis tool seems very powerfull. At the moment i haven't tested changing the recommendations it provided, but will try them soon. Spark 2.x applications don't seem to be working at the momment\n\n# Extended tests\n\n* Test Oozie Scheduler integration\n* Test Airflow integration\n* Define a deployment strategy\n* Test recommended changes\n\n\n# References\n \n  * https://github.com/linkedin/dr-elephant\n  * https://github.com/linkedin/dr-elephant/commit/7d9e34ecdbe36a652ca4ad2db852df08da57050a\n  * https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hadoop-spark\n","slug":"drelephat","published":1,"updated":"2020-09-06T14:38:37.855Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjn000dchpf89zbbote","content":"<p>This article would be about <a href=\"https://github.com/linkedin/dr-elephant\">dr-elephant</a> A Performance and Monitoring tool for Hadoop and Spark.</p>\n<blockquote><p>Dr. Elephant is a performance monitoring and tuning tool for Hadoop and Spark. It automatically gathers all the metrics, runs analysis on them, and presents them in a simple way for easy consumption. Its goal is to improve developer productivity and increase cluster efficiency by making it easier to tune the jobs. It analyzes the Hadoop and Spark jobs using a set of pluggable, configurable, rule-based heuristics that provide insights on how a job performed, and then uses the results to make suggestions about how to tune the job to make it perform more efficiently.</p>\n<footer><strong>Offical Website Definition</strong></footer></blockquote>\n\n<h1 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h1><ul>\n<li>Install mysql-server and create a BD for dr-elephant<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install mysql-server </span><br></pre></td></tr></table></figure></li>\n<li>MySQl preparation</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&gt; create database drelephant;</span><br><span class=\"line\">Query OK, 1 row affected (0.00 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">mysql&gt; grant all on drelephant.* to drelephant@localhost identified by &#39;drelephant&#39;;</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">mysql&gt; flush privileges;</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Install zip command<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install zip</span><br></pre></td></tr></table></figure></li>\n<li>Install sbt<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">&quot;deb https://dl.bintray.com/sbt/debian /&quot;</span> | sudo tee -a /etc/apt/sources.list.d/sbt.list</span><br><span class=\"line\">sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823</span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get install sbt</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><ul>\n<li>Clone the repo</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git https://github.com/linkedin/dr-elephant.git</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Compile</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sbt package</span><br><span class=\"line\">sbt dist</span><br><span class=\"line\">cp ./target/universal/dr-elephant-2.0.3-SNAPSHOT.zip .</span><br><span class=\"line\">./compile.sh</span><br><span class=\"line\"><span class=\"built_in\">cd</span> dist</span><br><span class=\"line\">unzip dr-elephant-2.0.3-SNAPSHOT.zip</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Starting the service</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> ELEPHANT_CONF_DIR=../../app-conf</span><br><span class=\"line\">./bin/start.sh </span><br></pre></td></tr></table></figure>\n<p>One can now access the web interface at: <a href=\"http://localhost:8080/\">http://localhost:8080</a></p>\n<img src=\"/2017/10/13/drelephat/DrElephant-2-dashboard.jpg\" class=\"\" title=\"[Dashboard]\">\n\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>This tool seems very powerfull. At the moment i haven’t tested changing the recommendations it provided, but will try them soon. Spark 2.x applications don’t seem to be working at the momment</p>\n<h1 id=\"Extended-tests\"><a href=\"#Extended-tests\" class=\"headerlink\" title=\"Extended tests\"></a>Extended tests</h1><ul>\n<li>Test Oozie Scheduler integration</li>\n<li>Test Airflow integration</li>\n<li>Define a deployment strategy</li>\n<li>Test recommended changes</li>\n</ul>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://github.com/linkedin/dr-elephant\">https://github.com/linkedin/dr-elephant</a></li>\n<li><a href=\"https://github.com/linkedin/dr-elephant/commit/7d9e34ecdbe36a652ca4ad2db852df08da57050a\">https://github.com/linkedin/dr-elephant/commit/7d9e34ecdbe36a652ca4ad2db852df08da57050a</a></li>\n<li><a href=\"https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hadoop-spark\">https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hadoop-spark</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>This article would be about <a href=\"https://github.com/linkedin/dr-elephant\">dr-elephant</a> A Performance and Monitoring tool for Hadoop and Spark.</p>\n<blockquote><p>Dr. Elephant is a performance monitoring and tuning tool for Hadoop and Spark. It automatically gathers all the metrics, runs analysis on them, and presents them in a simple way for easy consumption. Its goal is to improve developer productivity and increase cluster efficiency by making it easier to tune the jobs. It analyzes the Hadoop and Spark jobs using a set of pluggable, configurable, rule-based heuristics that provide insights on how a job performed, and then uses the results to make suggestions about how to tune the job to make it perform more efficiently.</p>\n<footer><strong>Offical Website Definition</strong></footer></blockquote>\n\n<h1 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h1><ul>\n<li>Install mysql-server and create a BD for dr-elephant<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install mysql-server </span><br></pre></td></tr></table></figure></li>\n<li>MySQl preparation</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mysql&gt; create database drelephant;</span><br><span class=\"line\">Query OK, 1 row affected (0.00 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">mysql&gt; grant all on drelephant.* to drelephant@localhost identified by &#39;drelephant&#39;;</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)</span><br><span class=\"line\"></span><br><span class=\"line\">mysql&gt; flush privileges;</span><br><span class=\"line\">Query OK, 0 rows affected (0.00 sec)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Install zip command<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install zip</span><br></pre></td></tr></table></figure></li>\n<li>Install sbt<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">echo</span> <span class=\"string\">&quot;deb https://dl.bintray.com/sbt/debian /&quot;</span> | sudo tee -a /etc/apt/sources.list.d/sbt.list</span><br><span class=\"line\">sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823</span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get install sbt</span><br></pre></td></tr></table></figure>\n\n</li>\n</ul>\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><ul>\n<li>Clone the repo</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git https://github.com/linkedin/dr-elephant.git</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Compile</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sbt package</span><br><span class=\"line\">sbt dist</span><br><span class=\"line\">cp ./target/universal/dr-elephant-2.0.3-SNAPSHOT.zip .</span><br><span class=\"line\">./compile.sh</span><br><span class=\"line\"><span class=\"built_in\">cd</span> dist</span><br><span class=\"line\">unzip dr-elephant-2.0.3-SNAPSHOT.zip</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Starting the service</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">export</span> ELEPHANT_CONF_DIR=../../app-conf</span><br><span class=\"line\">./bin/start.sh </span><br></pre></td></tr></table></figure>\n<p>One can now access the web interface at: <a href=\"http://localhost:8080/\">http://localhost:8080</a></p>\n<img src=\"/2017/10/13/drelephat/DrElephant-2-dashboard.jpg\" class=\"\" title=\"[Dashboard]\">\n\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>This tool seems very powerfull. At the moment i haven’t tested changing the recommendations it provided, but will try them soon. Spark 2.x applications don’t seem to be working at the momment</p>\n<h1 id=\"Extended-tests\"><a href=\"#Extended-tests\" class=\"headerlink\" title=\"Extended tests\"></a>Extended tests</h1><ul>\n<li>Test Oozie Scheduler integration</li>\n<li>Test Airflow integration</li>\n<li>Define a deployment strategy</li>\n<li>Test recommended changes</li>\n</ul>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://github.com/linkedin/dr-elephant\">https://github.com/linkedin/dr-elephant</a></li>\n<li><a href=\"https://github.com/linkedin/dr-elephant/commit/7d9e34ecdbe36a652ca4ad2db852df08da57050a\">https://github.com/linkedin/dr-elephant/commit/7d9e34ecdbe36a652ca4ad2db852df08da57050a</a></li>\n<li><a href=\"https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hadoop-spark\">https://engineering.linkedin.com/blog/2016/04/dr-elephant-open-source-self-serve-performance-tuning-hadoop-spark</a></li>\n</ul>\n"},{"title":"GIT Cheat Sheet","date":"2016-09-18T12:31:09.000Z","_content":"\n\nAs i'm allways forgetting how to reset my local changes from origin HEAD. This cheat sheet might help.\n\n\n## Create ##\n\nClone an existing repository\n```\ngit clone ssh://user@domain.com/repo.git\n```\n\nCreate a new local repository\n```\ngit init\n```\n\n## Local Changes ##\n\nChanged files in your working directory\n```\ngit status\n```\n\nChanges to tracked files\n```\ngit diff\n```\n\nAdd all current changes to the next commit\n```\ngit add .\n```\n\nAdd some changes in <file> to the next commit\n```\ngit add -p <file>\n```\n\nCommit all local changes in tracked files\n```\ngit commit -a\n```\n\nCommit previously staged changes\n```\ngit commmit\n```\n\nChange the last commit (Don't amend publish commits)\n```\ngit commit --amend\n```\n\n## Branches & Tags ##\n\nList all existing branches\n```\ngit branch -av\n```\n\nSwitch HEAD branch\n```\ngit checkout <branch>\n```\n\nCreate new banch based on your current HEAD\n```\ngit branch <new-branch>\n```\n\nCreate new tracking branch based on a remote branch\n```\ngit checkout --track <remote/branch>\n```\n\nDelete a local branch\n```\ngit branch -d <branch>\n```\n\nMark the current commit with a tag\n```\ngit tag <tag-name>\n```\n\n## Update & Publish ##\n\nList all currently configured remotes\n```\ngit remote -v\n```\n\nShow information about a remote\n```\ngit remote show <remote>\n```\n\nAdd new remote repository, named <remote>\n```\ngit remote add <shortname> <url>\n```\n\nDownload all changes from <remote>, but don't integrate into HEAD\n```\ngit fetch <remote>\n```\n\nDownload changes and directly merge/integrate into HEAD\n```\ngit pull <remote> <branch>\n```\n\nPublish local changes on a remote\n```\ngit push <remote> <branch>\n```\n\nDelete a branch on the remote\n```\ngit branch -dr <remote/branch>\n```\n\nPublish your tags\n```\ngit push --tags\n```\n\n## Merge & Rebase ##\n\nMerge <branch> into your current HEAD, without fast-forward\n```\ngit merge --no-ff <branch>\n```\n\nRebase your current HEAD onto <branch>\n```\ngit rebase <branch>\n```\n\nAbort a rebase\n```\ngit rebase --abort\n```\n\nContinue a rebase after resolving conflits\n```\ngit rebase --continue\n```\n\nUse your configured merge tool to sove conflits\n```\ngit mergetool\n```\n\nUse your editor to manually solve conflits and (after resolving) mark file as resolved\n```\ngit add <resolved-file>\ngit rm <resolved-file>\n```\n\n## Undo ##\n\nDiscard all local changes in your working directory\n```\ngit reset --hard HEAD\n```\n\nDiscard local changes in a specific file\n```\ngit checkout HEAD <file>\n```\n\nRevert a commit (by producing a new commit with contrary changes)\n```\ngit revert <commit>\n```\n\nReset your HEAD pointer to a previous commit, and discard all changes since then\n```\ngit reset --hard <commit>\n```\n\n...and preserve all changes as unstaged changes\n```\ngit reset <commit>\n```\n\n...and preserve uncommitted local changes\n```\ngit reset --keep <commit>\n```\n\n## Commit History\n\nShow all commits, starting with newest\n```\ngit log\n```\n\nShow changes over time for a specific file\n```\ngit log -p <file>\n```\n\nWho changed what and when in <file>\n```\ngit blame <file>\n```\n\n## Git plugins\n\nStore credential in manager\n```\ngit config credential.helper 'store'\n```\n\nActivate gpg signature\n```\ngit config commit.gpgsign true\n``` \n\n## Reference ##\n* https://www.git-tower.com/blog/git-cheat-sheet/","source":"_posts/git-cheat-sheet.md","raw":"---\ntitle: GIT cheat sheet\ntags:\n  - git\n  - cheat\n  - sheet\ndate: 2016-09-18 13:31:09\n---\n\n\nAs i'm allways forgetting how to reset my local changes from origin HEAD. This cheat sheet might help.\n\n\n## Create ##\n\nClone an existing repository\n```\ngit clone ssh://user@domain.com/repo.git\n```\n\nCreate a new local repository\n```\ngit init\n```\n\n## Local Changes ##\n\nChanged files in your working directory\n```\ngit status\n```\n\nChanges to tracked files\n```\ngit diff\n```\n\nAdd all current changes to the next commit\n```\ngit add .\n```\n\nAdd some changes in <file> to the next commit\n```\ngit add -p <file>\n```\n\nCommit all local changes in tracked files\n```\ngit commit -a\n```\n\nCommit previously staged changes\n```\ngit commmit\n```\n\nChange the last commit (Don't amend publish commits)\n```\ngit commit --amend\n```\n\n## Branches & Tags ##\n\nList all existing branches\n```\ngit branch -av\n```\n\nSwitch HEAD branch\n```\ngit checkout <branch>\n```\n\nCreate new banch based on your current HEAD\n```\ngit branch <new-branch>\n```\n\nCreate new tracking branch based on a remote branch\n```\ngit checkout --track <remote/branch>\n```\n\nDelete a local branch\n```\ngit branch -d <branch>\n```\n\nMark the current commit with a tag\n```\ngit tag <tag-name>\n```\n\n## Update & Publish ##\n\nList all currently configured remotes\n```\ngit remote -v\n```\n\nShow information about a remote\n```\ngit remote show <remote>\n```\n\nAdd new remote repository, named <remote>\n```\ngit remote add <shortname> <url>\n```\n\nDownload all changes from <remote>, but don't integrate into HEAD\n```\ngit fetch <remote>\n```\n\nDownload changes and directly merge/integrate into HEAD\n```\ngit pull <remote> <branch>\n```\n\nPublish local changes on a remote\n```\ngit push <remote> <branch>\n```\n\nDelete a branch on the remote\n```\ngit branch -dr <remote/branch>\n```\n\nPublish your tags\n```\ngit push --tags\n```\n\n## Merge & Rebase ##\n\nMerge <branch> into your current HEAD, without fast-forward\n```\ngit merge --no-ff <branch>\n```\n\nRebase your current HEAD onto <branch>\n```\ngit rebase <branch>\n```\n\nAbort a rebase\n```\ngit rebase --abort\n```\n\nContinue a rebase after resolving conflits\n```\ngit rebase --continue\n```\n\nUse your configured merge tool to sove conflits\n```\ngit mergetool\n```\n\nUse your editor to manually solve conflits and (after resolving) mark file as resolved\n```\ngit add <resolved-file>\ngit rm <resolved-file>\n```\n\n## Undo ##\n\nDiscard all local changes in your working directory\n```\ngit reset --hard HEAD\n```\n\nDiscard local changes in a specific file\n```\ngit checkout HEAD <file>\n```\n\nRevert a commit (by producing a new commit with contrary changes)\n```\ngit revert <commit>\n```\n\nReset your HEAD pointer to a previous commit, and discard all changes since then\n```\ngit reset --hard <commit>\n```\n\n...and preserve all changes as unstaged changes\n```\ngit reset <commit>\n```\n\n...and preserve uncommitted local changes\n```\ngit reset --keep <commit>\n```\n\n## Commit History\n\nShow all commits, starting with newest\n```\ngit log\n```\n\nShow changes over time for a specific file\n```\ngit log -p <file>\n```\n\nWho changed what and when in <file>\n```\ngit blame <file>\n```\n\n## Git plugins\n\nStore credential in manager\n```\ngit config credential.helper 'store'\n```\n\nActivate gpg signature\n```\ngit config commit.gpgsign true\n``` \n\n## Reference ##\n* https://www.git-tower.com/blog/git-cheat-sheet/","slug":"git-cheat-sheet","published":1,"updated":"2020-09-06T14:38:37.855Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjn000echpfe6ug7jfs","content":"<p>As i’m allways forgetting how to reset my local changes from origin HEAD. This cheat sheet might help.</p>\n<h2 id=\"Create\"><a href=\"#Create\" class=\"headerlink\" title=\"Create\"></a>Create</h2><p>Clone an existing repository</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone ssh:&#x2F;&#x2F;user@domain.com&#x2F;repo.git</span><br></pre></td></tr></table></figure>\n<p>Create a new local repository</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git init</span><br></pre></td></tr></table></figure>\n<h2 id=\"Local-Changes\"><a href=\"#Local-Changes\" class=\"headerlink\" title=\"Local Changes\"></a>Local Changes</h2><p>Changed files in your working directory</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git status</span><br></pre></td></tr></table></figure>\n<p>Changes to tracked files</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git diff</span><br></pre></td></tr></table></figure>\n<p>Add all current changes to the next commit</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add .</span><br></pre></td></tr></table></figure>\n<p>Add some changes in <file> to the next commit</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add -p &lt;file&gt;</span><br></pre></td></tr></table></figure>\n<p>Commit all local changes in tracked files</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git commit -a</span><br></pre></td></tr></table></figure>\n<p>Commit previously staged changes</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git commmit</span><br></pre></td></tr></table></figure>\n<p>Change the last commit (Don’t amend publish commits)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git commit --amend</span><br></pre></td></tr></table></figure>\n<h2 id=\"Branches-amp-Tags\"><a href=\"#Branches-amp-Tags\" class=\"headerlink\" title=\"Branches &amp; Tags\"></a>Branches &amp; Tags</h2><p>List all existing branches</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -av</span><br></pre></td></tr></table></figure>\n<p>Switch HEAD branch</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Create new banch based on your current HEAD</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch &lt;new-branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Create new tracking branch based on a remote branch</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout --track &lt;remote&#x2F;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Delete a local branch</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -d &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Mark the current commit with a tag</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git tag &lt;tag-name&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Update-amp-Publish\"><a href=\"#Update-amp-Publish\" class=\"headerlink\" title=\"Update &amp; Publish\"></a>Update &amp; Publish</h2><p>List all currently configured remotes</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git remote -v</span><br></pre></td></tr></table></figure>\n<p>Show information about a remote</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git remote show &lt;remote&gt;</span><br></pre></td></tr></table></figure>\n<p>Add new remote repository, named <remote></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git remote add &lt;shortname&gt; &lt;url&gt;</span><br></pre></td></tr></table></figure>\n<p>Download all changes from <remote>, but don’t integrate into HEAD</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch &lt;remote&gt;</span><br></pre></td></tr></table></figure>\n<p>Download changes and directly merge/integrate into HEAD</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull &lt;remote&gt; &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Publish local changes on a remote</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push &lt;remote&gt; &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Delete a branch on the remote</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -dr &lt;remote&#x2F;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Publish your tags</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push --tags</span><br></pre></td></tr></table></figure>\n<h2 id=\"Merge-amp-Rebase\"><a href=\"#Merge-amp-Rebase\" class=\"headerlink\" title=\"Merge &amp; Rebase\"></a>Merge &amp; Rebase</h2><p>Merge <branch> into your current HEAD, without fast-forward</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git merge --no-ff &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Rebase your current HEAD onto <branch></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rebase &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Abort a rebase</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rebase --abort</span><br></pre></td></tr></table></figure>\n<p>Continue a rebase after resolving conflits</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rebase --continue</span><br></pre></td></tr></table></figure>\n<p>Use your configured merge tool to sove conflits</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git mergetool</span><br></pre></td></tr></table></figure>\n<p>Use your editor to manually solve conflits and (after resolving) mark file as resolved</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add &lt;resolved-file&gt;</span><br><span class=\"line\">git rm &lt;resolved-file&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Undo\"><a href=\"#Undo\" class=\"headerlink\" title=\"Undo\"></a>Undo</h2><p>Discard all local changes in your working directory</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --hard HEAD</span><br></pre></td></tr></table></figure>\n<p>Discard local changes in a specific file</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout HEAD &lt;file&gt;</span><br></pre></td></tr></table></figure>\n<p>Revert a commit (by producing a new commit with contrary changes)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git revert &lt;commit&gt;</span><br></pre></td></tr></table></figure>\n<p>Reset your HEAD pointer to a previous commit, and discard all changes since then</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --hard &lt;commit&gt;</span><br></pre></td></tr></table></figure>\n<p>…and preserve all changes as unstaged changes</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset &lt;commit&gt;</span><br></pre></td></tr></table></figure>\n<p>…and preserve uncommitted local changes</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --keep &lt;commit&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Commit-History\"><a href=\"#Commit-History\" class=\"headerlink\" title=\"Commit History\"></a>Commit History</h2><p>Show all commits, starting with newest</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git log</span><br></pre></td></tr></table></figure>\n<p>Show changes over time for a specific file</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git log -p &lt;file&gt;</span><br></pre></td></tr></table></figure>\n<p>Who changed what and when in <file></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git blame &lt;file&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Git-plugins\"><a href=\"#Git-plugins\" class=\"headerlink\" title=\"Git plugins\"></a>Git plugins</h2><p>Store credential in manager</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config credential.helper &#39;store&#39;</span><br></pre></td></tr></table></figure>\n<p>Activate gpg signature</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config commit.gpgsign true</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.git-tower.com/blog/git-cheat-sheet/\">https://www.git-tower.com/blog/git-cheat-sheet/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>As i’m allways forgetting how to reset my local changes from origin HEAD. This cheat sheet might help.</p>\n<h2 id=\"Create\"><a href=\"#Create\" class=\"headerlink\" title=\"Create\"></a>Create</h2><p>Clone an existing repository</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git clone ssh:&#x2F;&#x2F;user@domain.com&#x2F;repo.git</span><br></pre></td></tr></table></figure>\n<p>Create a new local repository</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git init</span><br></pre></td></tr></table></figure>\n<h2 id=\"Local-Changes\"><a href=\"#Local-Changes\" class=\"headerlink\" title=\"Local Changes\"></a>Local Changes</h2><p>Changed files in your working directory</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git status</span><br></pre></td></tr></table></figure>\n<p>Changes to tracked files</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git diff</span><br></pre></td></tr></table></figure>\n<p>Add all current changes to the next commit</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add .</span><br></pre></td></tr></table></figure>\n<p>Add some changes in <file> to the next commit</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add -p &lt;file&gt;</span><br></pre></td></tr></table></figure>\n<p>Commit all local changes in tracked files</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git commit -a</span><br></pre></td></tr></table></figure>\n<p>Commit previously staged changes</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git commmit</span><br></pre></td></tr></table></figure>\n<p>Change the last commit (Don’t amend publish commits)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git commit --amend</span><br></pre></td></tr></table></figure>\n<h2 id=\"Branches-amp-Tags\"><a href=\"#Branches-amp-Tags\" class=\"headerlink\" title=\"Branches &amp; Tags\"></a>Branches &amp; Tags</h2><p>List all existing branches</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -av</span><br></pre></td></tr></table></figure>\n<p>Switch HEAD branch</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Create new banch based on your current HEAD</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch &lt;new-branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Create new tracking branch based on a remote branch</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout --track &lt;remote&#x2F;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Delete a local branch</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -d &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Mark the current commit with a tag</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git tag &lt;tag-name&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Update-amp-Publish\"><a href=\"#Update-amp-Publish\" class=\"headerlink\" title=\"Update &amp; Publish\"></a>Update &amp; Publish</h2><p>List all currently configured remotes</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git remote -v</span><br></pre></td></tr></table></figure>\n<p>Show information about a remote</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git remote show &lt;remote&gt;</span><br></pre></td></tr></table></figure>\n<p>Add new remote repository, named <remote></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git remote add &lt;shortname&gt; &lt;url&gt;</span><br></pre></td></tr></table></figure>\n<p>Download all changes from <remote>, but don’t integrate into HEAD</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git fetch &lt;remote&gt;</span><br></pre></td></tr></table></figure>\n<p>Download changes and directly merge/integrate into HEAD</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git pull &lt;remote&gt; &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Publish local changes on a remote</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push &lt;remote&gt; &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Delete a branch on the remote</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git branch -dr &lt;remote&#x2F;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Publish your tags</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git push --tags</span><br></pre></td></tr></table></figure>\n<h2 id=\"Merge-amp-Rebase\"><a href=\"#Merge-amp-Rebase\" class=\"headerlink\" title=\"Merge &amp; Rebase\"></a>Merge &amp; Rebase</h2><p>Merge <branch> into your current HEAD, without fast-forward</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git merge --no-ff &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Rebase your current HEAD onto <branch></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rebase &lt;branch&gt;</span><br></pre></td></tr></table></figure>\n<p>Abort a rebase</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rebase --abort</span><br></pre></td></tr></table></figure>\n<p>Continue a rebase after resolving conflits</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git rebase --continue</span><br></pre></td></tr></table></figure>\n<p>Use your configured merge tool to sove conflits</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git mergetool</span><br></pre></td></tr></table></figure>\n<p>Use your editor to manually solve conflits and (after resolving) mark file as resolved</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git add &lt;resolved-file&gt;</span><br><span class=\"line\">git rm &lt;resolved-file&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Undo\"><a href=\"#Undo\" class=\"headerlink\" title=\"Undo\"></a>Undo</h2><p>Discard all local changes in your working directory</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --hard HEAD</span><br></pre></td></tr></table></figure>\n<p>Discard local changes in a specific file</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git checkout HEAD &lt;file&gt;</span><br></pre></td></tr></table></figure>\n<p>Revert a commit (by producing a new commit with contrary changes)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git revert &lt;commit&gt;</span><br></pre></td></tr></table></figure>\n<p>Reset your HEAD pointer to a previous commit, and discard all changes since then</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --hard &lt;commit&gt;</span><br></pre></td></tr></table></figure>\n<p>…and preserve all changes as unstaged changes</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset &lt;commit&gt;</span><br></pre></td></tr></table></figure>\n<p>…and preserve uncommitted local changes</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git reset --keep &lt;commit&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Commit-History\"><a href=\"#Commit-History\" class=\"headerlink\" title=\"Commit History\"></a>Commit History</h2><p>Show all commits, starting with newest</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git log</span><br></pre></td></tr></table></figure>\n<p>Show changes over time for a specific file</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git log -p &lt;file&gt;</span><br></pre></td></tr></table></figure>\n<p>Who changed what and when in <file></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git blame &lt;file&gt;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Git-plugins\"><a href=\"#Git-plugins\" class=\"headerlink\" title=\"Git plugins\"></a>Git plugins</h2><p>Store credential in manager</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config credential.helper &#39;store&#39;</span><br></pre></td></tr></table></figure>\n<p>Activate gpg signature</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">git config commit.gpgsign true</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Reference\"><a href=\"#Reference\" class=\"headerlink\" title=\"Reference\"></a>Reference</h2><ul>\n<li><a href=\"https://www.git-tower.com/blog/git-cheat-sheet/\">https://www.git-tower.com/blog/git-cheat-sheet/</a></li>\n</ul>\n"},{"title":"Hive UDFS Functions","date":"2017-09-27T20:16:37.000Z","_content":"\n\nToday i was going to use a simple sha256 funtion in [Hive](https://hive.apache.org/) in order to mask a colunm and aparently in the latest [Cloudera](https://www.cloudera.com/) distribution the Shipped hive version doesn't have that native function.\n\nThis article will explain how you can build a sha256 or other udfs function and add it in Hive.\n\n# Checking Cloudera Packages Version\n\nCheck the following [URL](https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_package_tarball.html) in order to see the latest shipped package versions in Cloudera.\n\n> CDH 5.12 -> hive-1.1.0+cdh5.12.1+1197\n\n| Return Type| Name(Signature)           | Description          |\n|------------|---------------------------|----------------------|\n| string    | sha2(string/binary, int)   | Calculates the SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512) (as of Hive 1.3.0). The first argument is the string or binary to be hashed. The second argument indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). SHA-224 is supported starting from Java 8. If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL. Example: sha2('ABC', 256) = 'b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78'.|\n\n[Full Version](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)\n\n# Implement UDFS\n\nthis will server more as an exercise, one could create a more complex udf funtion. For the time being let's create a GenericUDFSha2 based on existing hive 1.3.0 version\n\nYou code clone my repo with some udfs-utils [here](git@github.com:rramos/udfs-utils.git)\n\nThe original code for hive version 1.3.0 is available in the repo\n\nhttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java\n\nLet's create the building structure\n\n```\nmkdir -p GenericUDFSha2/org/apache/hadoop/hive/ql/udf/generic\ncd GenericUDFSha2\n```\n\nLet's create a `pom.xml` file\n\n```\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>com.rramos.bigdata.utils</groupId>\n  <artifactId>GenericUDFSha2</artifactId>\n  <packaging>jar</packaging>\n  <version>1.0-SNAPSHOT</version>\n  <name>GenericUDFSha2</name>\n  <url>http://maven.apache.org</url>\n\n  <!-- properties -->\n  <properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n  </properties>\n     \n  <!-- prerequisitesprerequisites -->\n  <prerequisites>\n     <maven>3.0</maven>\n  </prerequisites>\n\n   <!-- Dependencies -->\n   <dependencies>\n\n    <dependency>\n      <groupId>junit</groupId>\n      <artifactId>junit</artifactId>\n      <version>4.12</version>\n      <scope>test</scope>\n    </dependency>\n\n    <dependency>\n       <groupId>org.apache.hive</groupId>\n       <artifactId>hive-exec</artifactId>\n       <version>2.0.0</version>\n    </dependency>\n\n    <dependency>\n    <groupId>joda-time</groupId>\n    <artifactId>joda-time</artifactId>\n    <version>2.9.3</version>\n    </dependency>\n\n  </dependencies>\n\n  <!-- Build options -->\n  <build>\n   <plugins>\n    <plugin>\n     <groupId>org.apache.maven.plugins</groupId>\n      <artifactId>maven-jar-plugin</artifactId>\n      <version>2.6</version>\n       <configuration>\n        <archive>\n          <manifest>\n           <addClasspath>true</addClasspath>\n           <mainClass>com.rramos.bigdata.utils.GenericUDFSha2</mainClass>\n          </manifest>\n        </archive>\n       </configuration>\n      </plugin>\n\n      <plugin>\n          <artifactId>maven-assembly-plugin</artifactId>\n          <configuration>\n                <archive>\n                    <manifest>\n                        <mainClass>\n                            com.rramos.bigdata.utils.GenericUDFSha2\n                        </mainClass>\n                    </manifest>\n                </archive>\n                <descriptorRefs>\n                    <descriptorRef>jar-with-dependencies</descriptorRef>\n                </descriptorRefs>\n           </configuration>\n      </plugin>\n\n    </plugins>\n  </build>\n\n</project>\n```\n\nYou should obviously change for you packaging namespace, i'm just using `com.rramos.bigdata.utils` to be simpler.\n\nNext, let's create the following file\n\n`org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java`\n\nwith the content\n\n```\npackage com.rramos.bigdata.utils;\n\nimport static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.BINARY_GROUP;\nimport static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.NUMERIC_GROUP;\nimport static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.STRING_GROUP;\n\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\n\nimport org.apache.commons.codec.binary.Hex;\nimport org.apache.hadoop.hive.ql.exec.Description;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;\nimport org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;\nimport org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\nimport org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;\nimport org.apache.hadoop.io.BytesWritable;\nimport org.apache.hadoop.io.Text;\n\n/**\n * GenericUDFSha2.\n *\n */\n@Description(name = \"sha2\", value = \"_FUNC_(string/binary, len) - Calculates the SHA-2 family of hash functions \"\n    + \"(SHA-224, SHA-256, SHA-384, and SHA-512).\",\n    extended = \"The first argument is the string or binary to be hashed. \"\n    + \"The second argument indicates the desired bit length of the result, \"\n    + \"which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). \"\n    + \"SHA-224 is supported starting from Java 8. \"\n    + \"If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL.\\n\"\n    + \"Example: > SELECT _FUNC_('ABC', 256);\\n 'b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78'\")\npublic class GenericUDFSha2 extends GenericUDF {\n  private transient Converter[] converters = new Converter[2];\n  private transient PrimitiveCategory[] inputTypes = new PrimitiveCategory[2];\n  private final Text output = new Text();\n  private transient boolean isStr;\n  private transient MessageDigest digest;\n\n  @Override\n  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n    checkArgsSize(arguments, 2, 2);\n\n    checkArgPrimitive(arguments, 0);\n    checkArgPrimitive(arguments, 1);\n\n    // the function should support both string and binary input types\n    checkArgGroups(arguments, 0, inputTypes, STRING_GROUP, BINARY_GROUP);\n    checkArgGroups(arguments, 1, inputTypes, NUMERIC_GROUP);\n\n    if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(inputTypes[0]) == STRING_GROUP) {\n      obtainStringConverter(arguments, 0, inputTypes, converters);\n      isStr = true;\n    } else {\n      GenericUDFParamUtils.obtainBinaryConverter(arguments, 0, inputTypes, converters);\n      isStr = false;\n    }\n\n    if (arguments[1] instanceof ConstantObjectInspector) {\n      Integer lenObj = getConstantIntValue(arguments, 1);\n      if (lenObj != null) {\n        int len = lenObj.intValue();\n        if (len == 0) {\n          len = 256;\n        }\n        try {\n          digest = MessageDigest.getInstance(\"SHA-\" + len);\n        } catch (NoSuchAlgorithmException e) {\n          // ignore\n        }\n      }\n    } else {\n      throw new UDFArgumentTypeException(1, getFuncName() + \" only takes constant as \"\n          + getArgOrder(1) + \" argument\");\n    }\n\n    ObjectInspector outputOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector;\n    return outputOI;\n  }\n\n  @Override\n  public Object evaluate(DeferredObject[] arguments) throws HiveException {\n    if (digest == null) {\n      return null;\n    }\n\n    digest.reset();\n    if (isStr) {\n      Text n = GenericUDFParamUtils.getTextValue(arguments, 0, converters);\n      if (n == null) {\n        return null;\n      }\n      digest.update(n.getBytes(), 0, n.getLength());\n    } else {\n      BytesWritable bWr = GenericUDFParamUtils.getBinaryValue(arguments, 0, converters);\n      if (bWr == null) {\n        return null;\n      }\n      digest.update(bWr.getBytes(), 0, bWr.getLength());\n    }\n    byte[] resBin = digest.digest();\n    String resStr = Hex.encodeHexString(resBin);\n\n    output.set(resStr);\n    return output;\n  }\n\n  @Override\n  public String getDisplayString(String[] children) {\n    return getStandardDisplayString(getFuncName(), children);\n  }\n\n  @Override\n  protected String getFuncName() {\n    return \"sha2\";\n  }\n}\n```\n\n\nAnd build the package.\n\n```\nmvn package\n```\n\nAfter compile you find in `target` dir the require jar (`GenericUDFSha2-1.0-SNAPSHOT.jar`) you need to add in Hive.\n\nYou should use your Hadoop Distribution instructions for deploying new jars.\n\nHere are [Cloudera Instructions](https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html).\n\nNext on your Hive session you need to `ADD JAR` and create a `FUNCTION` or `TEMPORARY FUNCTION`\n\n```\n    ADD JAR ./target/GenericUDFSha2-1.0-SNAPSHOT.jar\n    CREATE TEMPORARY FUNCTION sha2 AS 'com.rramos.bigdata.utils.GenericUDFSha2';\n    SELECT sha2(foo) from bar LIMIT1; \n```\n\n[Matthew Rathbone](https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html) Blog has some great tutorial on Hive Funtions. Take a look if you want to go deep with it.\n\nCheers,\nRR\n\n# References\n\n* https://www.cloudera.com/documentation/enterprise/release-notes/topics/rg_cdh_vd.html\n* https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html\n* https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html","source":"_posts/hive-udfs.md","raw":"---\ntitle: Hive UDFS functions\ntags:\n  - Hive\n  - udfs\n  - BigData\n  - Hadoop\n  - Java\n  - Cloudera\ndate: 2017-09-27 21:16:37\n---\n\n\nToday i was going to use a simple sha256 funtion in [Hive](https://hive.apache.org/) in order to mask a colunm and aparently in the latest [Cloudera](https://www.cloudera.com/) distribution the Shipped hive version doesn't have that native function.\n\nThis article will explain how you can build a sha256 or other udfs function and add it in Hive.\n\n# Checking Cloudera Packages Version\n\nCheck the following [URL](https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_package_tarball.html) in order to see the latest shipped package versions in Cloudera.\n\n> CDH 5.12 -> hive-1.1.0+cdh5.12.1+1197\n\n| Return Type| Name(Signature)           | Description          |\n|------------|---------------------------|----------------------|\n| string    | sha2(string/binary, int)   | Calculates the SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512) (as of Hive 1.3.0). The first argument is the string or binary to be hashed. The second argument indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). SHA-224 is supported starting from Java 8. If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL. Example: sha2('ABC', 256) = 'b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78'.|\n\n[Full Version](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF)\n\n# Implement UDFS\n\nthis will server more as an exercise, one could create a more complex udf funtion. For the time being let's create a GenericUDFSha2 based on existing hive 1.3.0 version\n\nYou code clone my repo with some udfs-utils [here](git@github.com:rramos/udfs-utils.git)\n\nThe original code for hive version 1.3.0 is available in the repo\n\nhttps://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java\n\nLet's create the building structure\n\n```\nmkdir -p GenericUDFSha2/org/apache/hadoop/hive/ql/udf/generic\ncd GenericUDFSha2\n```\n\nLet's create a `pom.xml` file\n\n```\n<project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\"\n  xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/maven-v4_0_0.xsd\">\n  <modelVersion>4.0.0</modelVersion>\n  <groupId>com.rramos.bigdata.utils</groupId>\n  <artifactId>GenericUDFSha2</artifactId>\n  <packaging>jar</packaging>\n  <version>1.0-SNAPSHOT</version>\n  <name>GenericUDFSha2</name>\n  <url>http://maven.apache.org</url>\n\n  <!-- properties -->\n  <properties>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n  </properties>\n     \n  <!-- prerequisitesprerequisites -->\n  <prerequisites>\n     <maven>3.0</maven>\n  </prerequisites>\n\n   <!-- Dependencies -->\n   <dependencies>\n\n    <dependency>\n      <groupId>junit</groupId>\n      <artifactId>junit</artifactId>\n      <version>4.12</version>\n      <scope>test</scope>\n    </dependency>\n\n    <dependency>\n       <groupId>org.apache.hive</groupId>\n       <artifactId>hive-exec</artifactId>\n       <version>2.0.0</version>\n    </dependency>\n\n    <dependency>\n    <groupId>joda-time</groupId>\n    <artifactId>joda-time</artifactId>\n    <version>2.9.3</version>\n    </dependency>\n\n  </dependencies>\n\n  <!-- Build options -->\n  <build>\n   <plugins>\n    <plugin>\n     <groupId>org.apache.maven.plugins</groupId>\n      <artifactId>maven-jar-plugin</artifactId>\n      <version>2.6</version>\n       <configuration>\n        <archive>\n          <manifest>\n           <addClasspath>true</addClasspath>\n           <mainClass>com.rramos.bigdata.utils.GenericUDFSha2</mainClass>\n          </manifest>\n        </archive>\n       </configuration>\n      </plugin>\n\n      <plugin>\n          <artifactId>maven-assembly-plugin</artifactId>\n          <configuration>\n                <archive>\n                    <manifest>\n                        <mainClass>\n                            com.rramos.bigdata.utils.GenericUDFSha2\n                        </mainClass>\n                    </manifest>\n                </archive>\n                <descriptorRefs>\n                    <descriptorRef>jar-with-dependencies</descriptorRef>\n                </descriptorRefs>\n           </configuration>\n      </plugin>\n\n    </plugins>\n  </build>\n\n</project>\n```\n\nYou should obviously change for you packaging namespace, i'm just using `com.rramos.bigdata.utils` to be simpler.\n\nNext, let's create the following file\n\n`org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java`\n\nwith the content\n\n```\npackage com.rramos.bigdata.utils;\n\nimport static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.BINARY_GROUP;\nimport static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.NUMERIC_GROUP;\nimport static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.STRING_GROUP;\n\nimport java.security.MessageDigest;\nimport java.security.NoSuchAlgorithmException;\n\nimport org.apache.commons.codec.binary.Hex;\nimport org.apache.hadoop.hive.ql.exec.Description;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentException;\nimport org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;\nimport org.apache.hadoop.hive.ql.metadata.HiveException;\nimport org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;\nimport org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;\nimport org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;\nimport org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;\nimport org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;\nimport org.apache.hadoop.io.BytesWritable;\nimport org.apache.hadoop.io.Text;\n\n/**\n * GenericUDFSha2.\n *\n */\n@Description(name = \"sha2\", value = \"_FUNC_(string/binary, len) - Calculates the SHA-2 family of hash functions \"\n    + \"(SHA-224, SHA-256, SHA-384, and SHA-512).\",\n    extended = \"The first argument is the string or binary to be hashed. \"\n    + \"The second argument indicates the desired bit length of the result, \"\n    + \"which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). \"\n    + \"SHA-224 is supported starting from Java 8. \"\n    + \"If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL.\\n\"\n    + \"Example: > SELECT _FUNC_('ABC', 256);\\n 'b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78'\")\npublic class GenericUDFSha2 extends GenericUDF {\n  private transient Converter[] converters = new Converter[2];\n  private transient PrimitiveCategory[] inputTypes = new PrimitiveCategory[2];\n  private final Text output = new Text();\n  private transient boolean isStr;\n  private transient MessageDigest digest;\n\n  @Override\n  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException {\n    checkArgsSize(arguments, 2, 2);\n\n    checkArgPrimitive(arguments, 0);\n    checkArgPrimitive(arguments, 1);\n\n    // the function should support both string and binary input types\n    checkArgGroups(arguments, 0, inputTypes, STRING_GROUP, BINARY_GROUP);\n    checkArgGroups(arguments, 1, inputTypes, NUMERIC_GROUP);\n\n    if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(inputTypes[0]) == STRING_GROUP) {\n      obtainStringConverter(arguments, 0, inputTypes, converters);\n      isStr = true;\n    } else {\n      GenericUDFParamUtils.obtainBinaryConverter(arguments, 0, inputTypes, converters);\n      isStr = false;\n    }\n\n    if (arguments[1] instanceof ConstantObjectInspector) {\n      Integer lenObj = getConstantIntValue(arguments, 1);\n      if (lenObj != null) {\n        int len = lenObj.intValue();\n        if (len == 0) {\n          len = 256;\n        }\n        try {\n          digest = MessageDigest.getInstance(\"SHA-\" + len);\n        } catch (NoSuchAlgorithmException e) {\n          // ignore\n        }\n      }\n    } else {\n      throw new UDFArgumentTypeException(1, getFuncName() + \" only takes constant as \"\n          + getArgOrder(1) + \" argument\");\n    }\n\n    ObjectInspector outputOI = PrimitiveObjectInspectorFactory.writableStringObjectInspector;\n    return outputOI;\n  }\n\n  @Override\n  public Object evaluate(DeferredObject[] arguments) throws HiveException {\n    if (digest == null) {\n      return null;\n    }\n\n    digest.reset();\n    if (isStr) {\n      Text n = GenericUDFParamUtils.getTextValue(arguments, 0, converters);\n      if (n == null) {\n        return null;\n      }\n      digest.update(n.getBytes(), 0, n.getLength());\n    } else {\n      BytesWritable bWr = GenericUDFParamUtils.getBinaryValue(arguments, 0, converters);\n      if (bWr == null) {\n        return null;\n      }\n      digest.update(bWr.getBytes(), 0, bWr.getLength());\n    }\n    byte[] resBin = digest.digest();\n    String resStr = Hex.encodeHexString(resBin);\n\n    output.set(resStr);\n    return output;\n  }\n\n  @Override\n  public String getDisplayString(String[] children) {\n    return getStandardDisplayString(getFuncName(), children);\n  }\n\n  @Override\n  protected String getFuncName() {\n    return \"sha2\";\n  }\n}\n```\n\n\nAnd build the package.\n\n```\nmvn package\n```\n\nAfter compile you find in `target` dir the require jar (`GenericUDFSha2-1.0-SNAPSHOT.jar`) you need to add in Hive.\n\nYou should use your Hadoop Distribution instructions for deploying new jars.\n\nHere are [Cloudera Instructions](https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html).\n\nNext on your Hive session you need to `ADD JAR` and create a `FUNCTION` or `TEMPORARY FUNCTION`\n\n```\n    ADD JAR ./target/GenericUDFSha2-1.0-SNAPSHOT.jar\n    CREATE TEMPORARY FUNCTION sha2 AS 'com.rramos.bigdata.utils.GenericUDFSha2';\n    SELECT sha2(foo) from bar LIMIT1; \n```\n\n[Matthew Rathbone](https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html) Blog has some great tutorial on Hive Funtions. Take a look if you want to go deep with it.\n\nCheers,\nRR\n\n# References\n\n* https://www.cloudera.com/documentation/enterprise/release-notes/topics/rg_cdh_vd.html\n* https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html\n* https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html","slug":"hive-udfs","published":1,"updated":"2020-09-06T14:38:37.855Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjo000fchpf56kgcodo","content":"<p>Today i was going to use a simple sha256 funtion in <a href=\"https://hive.apache.org/\">Hive</a> in order to mask a colunm and aparently in the latest <a href=\"https://www.cloudera.com/\">Cloudera</a> distribution the Shipped hive version doesn’t have that native function.</p>\n<p>This article will explain how you can build a sha256 or other udfs function and add it in Hive.</p>\n<h1 id=\"Checking-Cloudera-Packages-Version\"><a href=\"#Checking-Cloudera-Packages-Version\" class=\"headerlink\" title=\"Checking Cloudera Packages Version\"></a>Checking Cloudera Packages Version</h1><p>Check the following <a href=\"https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_package_tarball.html\">URL</a> in order to see the latest shipped package versions in Cloudera.</p>\n<blockquote>\n<p>CDH 5.12 -&gt; hive-1.1.0+cdh5.12.1+1197</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Return Type</th>\n<th>Name(Signature)</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>string</td>\n<td>sha2(string/binary, int)</td>\n<td>Calculates the SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512) (as of Hive 1.3.0). The first argument is the string or binary to be hashed. The second argument indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). SHA-224 is supported starting from Java 8. If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL. Example: sha2(‘ABC’, 256) = ‘b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78’.</td>\n</tr>\n</tbody></table>\n<p><a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF\">Full Version</a></p>\n<h1 id=\"Implement-UDFS\"><a href=\"#Implement-UDFS\" class=\"headerlink\" title=\"Implement UDFS\"></a>Implement UDFS</h1><p>this will server more as an exercise, one could create a more complex udf funtion. For the time being let’s create a GenericUDFSha2 based on existing hive 1.3.0 version</p>\n<p>You code clone my repo with some udfs-utils <a href=\"git@github.com:rramos/udfs-utils.git\">here</a></p>\n<p>The original code for hive version 1.3.0 is available in the repo</p>\n<p><a href=\"https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java\">https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java</a></p>\n<p>Let’s create the building structure</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p GenericUDFSha2&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;hive&#x2F;ql&#x2F;udf&#x2F;generic</span><br><span class=\"line\">cd GenericUDFSha2</span><br></pre></td></tr></table></figure>\n<p>Let’s create a <code>pom.xml</code> file</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class=\"line\">  xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;maven-v4_0_0.xsd&quot;&gt;</span><br><span class=\"line\">  &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class=\"line\">  &lt;groupId&gt;com.rramos.bigdata.utils&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;GenericUDFSha2&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">  &lt;packaging&gt;jar&lt;&#x2F;packaging&gt;</span><br><span class=\"line\">  &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class=\"line\">  &lt;name&gt;GenericUDFSha2&lt;&#x2F;name&gt;</span><br><span class=\"line\">  &lt;url&gt;http:&#x2F;&#x2F;maven.apache.org&lt;&#x2F;url&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;!-- properties --&gt;</span><br><span class=\"line\">  &lt;properties&gt;</span><br><span class=\"line\">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt;</span><br><span class=\"line\">  &lt;&#x2F;properties&gt;</span><br><span class=\"line\">     </span><br><span class=\"line\">  &lt;!-- prerequisitesprerequisites --&gt;</span><br><span class=\"line\">  &lt;prerequisites&gt;</span><br><span class=\"line\">     &lt;maven&gt;3.0&lt;&#x2F;maven&gt;</span><br><span class=\"line\">  &lt;&#x2F;prerequisites&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">   &lt;!-- Dependencies --&gt;</span><br><span class=\"line\">   &lt;dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;junit&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;4.12&lt;&#x2F;version&gt;</span><br><span class=\"line\">      &lt;scope&gt;test&lt;&#x2F;scope&gt;</span><br><span class=\"line\">    &lt;&#x2F;dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">       &lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">       &lt;artifactId&gt;hive-exec&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">       &lt;version&gt;2.0.0&lt;&#x2F;version&gt;</span><br><span class=\"line\">    &lt;&#x2F;dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;joda-time&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;joda-time&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;2.9.3&lt;&#x2F;version&gt;</span><br><span class=\"line\">    &lt;&#x2F;dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;&#x2F;dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;!-- Build options --&gt;</span><br><span class=\"line\">  &lt;build&gt;</span><br><span class=\"line\">   &lt;plugins&gt;</span><br><span class=\"line\">    &lt;plugin&gt;</span><br><span class=\"line\">     &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;maven-jar-plugin&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class=\"line\">       &lt;configuration&gt;</span><br><span class=\"line\">        &lt;archive&gt;</span><br><span class=\"line\">          &lt;manifest&gt;</span><br><span class=\"line\">           &lt;addClasspath&gt;true&lt;&#x2F;addClasspath&gt;</span><br><span class=\"line\">           &lt;mainClass&gt;com.rramos.bigdata.utils.GenericUDFSha2&lt;&#x2F;mainClass&gt;</span><br><span class=\"line\">          &lt;&#x2F;manifest&gt;</span><br><span class=\"line\">        &lt;&#x2F;archive&gt;</span><br><span class=\"line\">       &lt;&#x2F;configuration&gt;</span><br><span class=\"line\">      &lt;&#x2F;plugin&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">      &lt;plugin&gt;</span><br><span class=\"line\">          &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">          &lt;configuration&gt;</span><br><span class=\"line\">                &lt;archive&gt;</span><br><span class=\"line\">                    &lt;manifest&gt;</span><br><span class=\"line\">                        &lt;mainClass&gt;</span><br><span class=\"line\">                            com.rramos.bigdata.utils.GenericUDFSha2</span><br><span class=\"line\">                        &lt;&#x2F;mainClass&gt;</span><br><span class=\"line\">                    &lt;&#x2F;manifest&gt;</span><br><span class=\"line\">                &lt;&#x2F;archive&gt;</span><br><span class=\"line\">                &lt;descriptorRefs&gt;</span><br><span class=\"line\">                    &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class=\"line\">                &lt;&#x2F;descriptorRefs&gt;</span><br><span class=\"line\">           &lt;&#x2F;configuration&gt;</span><br><span class=\"line\">      &lt;&#x2F;plugin&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;&#x2F;plugins&gt;</span><br><span class=\"line\">  &lt;&#x2F;build&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure>\n<p>You should obviously change for you packaging namespace, i’m just using <code>com.rramos.bigdata.utils</code> to be simpler.</p>\n<p>Next, let’s create the following file</p>\n<p><code>org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java</code></p>\n<p>with the content</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.rramos.bigdata.utils;</span><br><span class=\"line\"></span><br><span class=\"line\">import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.BINARY_GROUP;</span><br><span class=\"line\">import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.NUMERIC_GROUP;</span><br><span class=\"line\">import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.STRING_GROUP;</span><br><span class=\"line\"></span><br><span class=\"line\">import java.security.MessageDigest;</span><br><span class=\"line\">import java.security.NoSuchAlgorithmException;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.commons.codec.binary.Hex;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;</span><br><span class=\"line\">import org.apache.hadoop.io.BytesWritable;</span><br><span class=\"line\">import org.apache.hadoop.io.Text;</span><br><span class=\"line\"></span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\"> * GenericUDFSha2.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> *&#x2F;</span><br><span class=\"line\">@Description(name &#x3D; &quot;sha2&quot;, value &#x3D; &quot;_FUNC_(string&#x2F;binary, len) - Calculates the SHA-2 family of hash functions &quot;</span><br><span class=\"line\">    + &quot;(SHA-224, SHA-256, SHA-384, and SHA-512).&quot;,</span><br><span class=\"line\">    extended &#x3D; &quot;The first argument is the string or binary to be hashed. &quot;</span><br><span class=\"line\">    + &quot;The second argument indicates the desired bit length of the result, &quot;</span><br><span class=\"line\">    + &quot;which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). &quot;</span><br><span class=\"line\">    + &quot;SHA-224 is supported starting from Java 8. &quot;</span><br><span class=\"line\">    + &quot;If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL.\\n&quot;</span><br><span class=\"line\">    + &quot;Example: &gt; SELECT _FUNC_(&#39;ABC&#39;, 256);\\n &#39;b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78&#39;&quot;)</span><br><span class=\"line\">public class GenericUDFSha2 extends GenericUDF &#123;</span><br><span class=\"line\">  private transient Converter[] converters &#x3D; new Converter[2];</span><br><span class=\"line\">  private transient PrimitiveCategory[] inputTypes &#x3D; new PrimitiveCategory[2];</span><br><span class=\"line\">  private final Text output &#x3D; new Text();</span><br><span class=\"line\">  private transient boolean isStr;</span><br><span class=\"line\">  private transient MessageDigest digest;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123;</span><br><span class=\"line\">    checkArgsSize(arguments, 2, 2);</span><br><span class=\"line\"></span><br><span class=\"line\">    checkArgPrimitive(arguments, 0);</span><br><span class=\"line\">    checkArgPrimitive(arguments, 1);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; the function should support both string and binary input types</span><br><span class=\"line\">    checkArgGroups(arguments, 0, inputTypes, STRING_GROUP, BINARY_GROUP);</span><br><span class=\"line\">    checkArgGroups(arguments, 1, inputTypes, NUMERIC_GROUP);</span><br><span class=\"line\"></span><br><span class=\"line\">    if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(inputTypes[0]) &#x3D;&#x3D; STRING_GROUP) &#123;</span><br><span class=\"line\">      obtainStringConverter(arguments, 0, inputTypes, converters);</span><br><span class=\"line\">      isStr &#x3D; true;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      GenericUDFParamUtils.obtainBinaryConverter(arguments, 0, inputTypes, converters);</span><br><span class=\"line\">      isStr &#x3D; false;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    if (arguments[1] instanceof ConstantObjectInspector) &#123;</span><br><span class=\"line\">      Integer lenObj &#x3D; getConstantIntValue(arguments, 1);</span><br><span class=\"line\">      if (lenObj !&#x3D; null) &#123;</span><br><span class=\"line\">        int len &#x3D; lenObj.intValue();</span><br><span class=\"line\">        if (len &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">          len &#x3D; 256;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">          digest &#x3D; MessageDigest.getInstance(&quot;SHA-&quot; + len);</span><br><span class=\"line\">        &#125; catch (NoSuchAlgorithmException e) &#123;</span><br><span class=\"line\">          &#x2F;&#x2F; ignore</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      throw new UDFArgumentTypeException(1, getFuncName() + &quot; only takes constant as &quot;</span><br><span class=\"line\">          + getArgOrder(1) + &quot; argument&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ObjectInspector outputOI &#x3D; PrimitiveObjectInspectorFactory.writableStringObjectInspector;</span><br><span class=\"line\">    return outputOI;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  public Object evaluate(DeferredObject[] arguments) throws HiveException &#123;</span><br><span class=\"line\">    if (digest &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">      return null;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    digest.reset();</span><br><span class=\"line\">    if (isStr) &#123;</span><br><span class=\"line\">      Text n &#x3D; GenericUDFParamUtils.getTextValue(arguments, 0, converters);</span><br><span class=\"line\">      if (n &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">        return null;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      digest.update(n.getBytes(), 0, n.getLength());</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      BytesWritable bWr &#x3D; GenericUDFParamUtils.getBinaryValue(arguments, 0, converters);</span><br><span class=\"line\">      if (bWr &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">        return null;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      digest.update(bWr.getBytes(), 0, bWr.getLength());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    byte[] resBin &#x3D; digest.digest();</span><br><span class=\"line\">    String resStr &#x3D; Hex.encodeHexString(resBin);</span><br><span class=\"line\"></span><br><span class=\"line\">    output.set(resStr);</span><br><span class=\"line\">    return output;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  public String getDisplayString(String[] children) &#123;</span><br><span class=\"line\">    return getStandardDisplayString(getFuncName(), children);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  protected String getFuncName() &#123;</span><br><span class=\"line\">    return &quot;sha2&quot;;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>And build the package.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn package</span><br></pre></td></tr></table></figure>\n<p>After compile you find in <code>target</code> dir the require jar (<code>GenericUDFSha2-1.0-SNAPSHOT.jar</code>) you need to add in Hive.</p>\n<p>You should use your Hadoop Distribution instructions for deploying new jars.</p>\n<p>Here are <a href=\"https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html\">Cloudera Instructions</a>.</p>\n<p>Next on your Hive session you need to <code>ADD JAR</code> and create a <code>FUNCTION</code> or <code>TEMPORARY FUNCTION</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD JAR .&#x2F;target&#x2F;GenericUDFSha2-1.0-SNAPSHOT.jar</span><br><span class=\"line\">CREATE TEMPORARY FUNCTION sha2 AS &#39;com.rramos.bigdata.utils.GenericUDFSha2&#39;;</span><br><span class=\"line\">SELECT sha2(foo) from bar LIMIT1; </span><br></pre></td></tr></table></figure>\n<p><a href=\"https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html\">Matthew Rathbone</a> Blog has some great tutorial on Hive Funtions. Take a look if you want to go deep with it.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://www.cloudera.com/documentation/enterprise/release-notes/topics/rg_cdh_vd.html\">https://www.cloudera.com/documentation/enterprise/release-notes/topics/rg_cdh_vd.html</a></li>\n<li><a href=\"https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html\">https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html</a></li>\n<li><a href=\"https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html\">https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>Today i was going to use a simple sha256 funtion in <a href=\"https://hive.apache.org/\">Hive</a> in order to mask a colunm and aparently in the latest <a href=\"https://www.cloudera.com/\">Cloudera</a> distribution the Shipped hive version doesn’t have that native function.</p>\n<p>This article will explain how you can build a sha256 or other udfs function and add it in Hive.</p>\n<h1 id=\"Checking-Cloudera-Packages-Version\"><a href=\"#Checking-Cloudera-Packages-Version\" class=\"headerlink\" title=\"Checking Cloudera Packages Version\"></a>Checking Cloudera Packages Version</h1><p>Check the following <a href=\"https://www.cloudera.com/documentation/enterprise/release-notes/topics/cdh_vd_cdh_package_tarball.html\">URL</a> in order to see the latest shipped package versions in Cloudera.</p>\n<blockquote>\n<p>CDH 5.12 -&gt; hive-1.1.0+cdh5.12.1+1197</p>\n</blockquote>\n<table>\n<thead>\n<tr>\n<th>Return Type</th>\n<th>Name(Signature)</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>string</td>\n<td>sha2(string/binary, int)</td>\n<td>Calculates the SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512) (as of Hive 1.3.0). The first argument is the string or binary to be hashed. The second argument indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). SHA-224 is supported starting from Java 8. If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL. Example: sha2(‘ABC’, 256) = ‘b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78’.</td>\n</tr>\n</tbody></table>\n<p><a href=\"https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF\">Full Version</a></p>\n<h1 id=\"Implement-UDFS\"><a href=\"#Implement-UDFS\" class=\"headerlink\" title=\"Implement UDFS\"></a>Implement UDFS</h1><p>this will server more as an exercise, one could create a more complex udf funtion. For the time being let’s create a GenericUDFSha2 based on existing hive 1.3.0 version</p>\n<p>You code clone my repo with some udfs-utils <a href=\"git@github.com:rramos/udfs-utils.git\">here</a></p>\n<p>The original code for hive version 1.3.0 is available in the repo</p>\n<p><a href=\"https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java\">https://github.com/apache/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java</a></p>\n<p>Let’s create the building structure</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p GenericUDFSha2&#x2F;org&#x2F;apache&#x2F;hadoop&#x2F;hive&#x2F;ql&#x2F;udf&#x2F;generic</span><br><span class=\"line\">cd GenericUDFSha2</span><br></pre></td></tr></table></figure>\n<p>Let’s create a <code>pom.xml</code> file</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&lt;project xmlns&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0&quot; xmlns:xsi&#x3D;&quot;http:&#x2F;&#x2F;www.w3.org&#x2F;2001&#x2F;XMLSchema-instance&quot;</span><br><span class=\"line\">  xsi:schemaLocation&#x3D;&quot;http:&#x2F;&#x2F;maven.apache.org&#x2F;POM&#x2F;4.0.0 http:&#x2F;&#x2F;maven.apache.org&#x2F;maven-v4_0_0.xsd&quot;&gt;</span><br><span class=\"line\">  &lt;modelVersion&gt;4.0.0&lt;&#x2F;modelVersion&gt;</span><br><span class=\"line\">  &lt;groupId&gt;com.rramos.bigdata.utils&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">  &lt;artifactId&gt;GenericUDFSha2&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">  &lt;packaging&gt;jar&lt;&#x2F;packaging&gt;</span><br><span class=\"line\">  &lt;version&gt;1.0-SNAPSHOT&lt;&#x2F;version&gt;</span><br><span class=\"line\">  &lt;name&gt;GenericUDFSha2&lt;&#x2F;name&gt;</span><br><span class=\"line\">  &lt;url&gt;http:&#x2F;&#x2F;maven.apache.org&lt;&#x2F;url&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;!-- properties --&gt;</span><br><span class=\"line\">  &lt;properties&gt;</span><br><span class=\"line\">    &lt;project.build.sourceEncoding&gt;UTF-8&lt;&#x2F;project.build.sourceEncoding&gt;</span><br><span class=\"line\">  &lt;&#x2F;properties&gt;</span><br><span class=\"line\">     </span><br><span class=\"line\">  &lt;!-- prerequisitesprerequisites --&gt;</span><br><span class=\"line\">  &lt;prerequisites&gt;</span><br><span class=\"line\">     &lt;maven&gt;3.0&lt;&#x2F;maven&gt;</span><br><span class=\"line\">  &lt;&#x2F;prerequisites&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">   &lt;!-- Dependencies --&gt;</span><br><span class=\"line\">   &lt;dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">      &lt;groupId&gt;junit&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;junit&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;4.12&lt;&#x2F;version&gt;</span><br><span class=\"line\">      &lt;scope&gt;test&lt;&#x2F;scope&gt;</span><br><span class=\"line\">    &lt;&#x2F;dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">       &lt;groupId&gt;org.apache.hive&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">       &lt;artifactId&gt;hive-exec&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">       &lt;version&gt;2.0.0&lt;&#x2F;version&gt;</span><br><span class=\"line\">    &lt;&#x2F;dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;dependency&gt;</span><br><span class=\"line\">    &lt;groupId&gt;joda-time&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">    &lt;artifactId&gt;joda-time&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">    &lt;version&gt;2.9.3&lt;&#x2F;version&gt;</span><br><span class=\"line\">    &lt;&#x2F;dependency&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;&#x2F;dependencies&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">  &lt;!-- Build options --&gt;</span><br><span class=\"line\">  &lt;build&gt;</span><br><span class=\"line\">   &lt;plugins&gt;</span><br><span class=\"line\">    &lt;plugin&gt;</span><br><span class=\"line\">     &lt;groupId&gt;org.apache.maven.plugins&lt;&#x2F;groupId&gt;</span><br><span class=\"line\">      &lt;artifactId&gt;maven-jar-plugin&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">      &lt;version&gt;2.6&lt;&#x2F;version&gt;</span><br><span class=\"line\">       &lt;configuration&gt;</span><br><span class=\"line\">        &lt;archive&gt;</span><br><span class=\"line\">          &lt;manifest&gt;</span><br><span class=\"line\">           &lt;addClasspath&gt;true&lt;&#x2F;addClasspath&gt;</span><br><span class=\"line\">           &lt;mainClass&gt;com.rramos.bigdata.utils.GenericUDFSha2&lt;&#x2F;mainClass&gt;</span><br><span class=\"line\">          &lt;&#x2F;manifest&gt;</span><br><span class=\"line\">        &lt;&#x2F;archive&gt;</span><br><span class=\"line\">       &lt;&#x2F;configuration&gt;</span><br><span class=\"line\">      &lt;&#x2F;plugin&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">      &lt;plugin&gt;</span><br><span class=\"line\">          &lt;artifactId&gt;maven-assembly-plugin&lt;&#x2F;artifactId&gt;</span><br><span class=\"line\">          &lt;configuration&gt;</span><br><span class=\"line\">                &lt;archive&gt;</span><br><span class=\"line\">                    &lt;manifest&gt;</span><br><span class=\"line\">                        &lt;mainClass&gt;</span><br><span class=\"line\">                            com.rramos.bigdata.utils.GenericUDFSha2</span><br><span class=\"line\">                        &lt;&#x2F;mainClass&gt;</span><br><span class=\"line\">                    &lt;&#x2F;manifest&gt;</span><br><span class=\"line\">                &lt;&#x2F;archive&gt;</span><br><span class=\"line\">                &lt;descriptorRefs&gt;</span><br><span class=\"line\">                    &lt;descriptorRef&gt;jar-with-dependencies&lt;&#x2F;descriptorRef&gt;</span><br><span class=\"line\">                &lt;&#x2F;descriptorRefs&gt;</span><br><span class=\"line\">           &lt;&#x2F;configuration&gt;</span><br><span class=\"line\">      &lt;&#x2F;plugin&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">    &lt;&#x2F;plugins&gt;</span><br><span class=\"line\">  &lt;&#x2F;build&gt;</span><br><span class=\"line\"></span><br><span class=\"line\">&lt;&#x2F;project&gt;</span><br></pre></td></tr></table></figure>\n<p>You should obviously change for you packaging namespace, i’m just using <code>com.rramos.bigdata.utils</code> to be simpler.</p>\n<p>Next, let’s create the following file</p>\n<p><code>org/apache/hadoop/hive/ql/udf/generic/GenericUDFSha2.java</code></p>\n<p>with the content</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">package com.rramos.bigdata.utils;</span><br><span class=\"line\"></span><br><span class=\"line\">import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.BINARY_GROUP;</span><br><span class=\"line\">import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.NUMERIC_GROUP;</span><br><span class=\"line\">import static org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils.PrimitiveGrouping.STRING_GROUP;</span><br><span class=\"line\"></span><br><span class=\"line\">import java.security.MessageDigest;</span><br><span class=\"line\">import java.security.NoSuchAlgorithmException;</span><br><span class=\"line\"></span><br><span class=\"line\">import org.apache.commons.codec.binary.Hex;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.Description;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDFArgumentException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.exec.UDFArgumentTypeException;</span><br><span class=\"line\">import org.apache.hadoop.hive.ql.metadata.HiveException;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ConstantObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorConverters.Converter;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.PrimitiveObjectInspector.PrimitiveCategory;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorFactory;</span><br><span class=\"line\">import org.apache.hadoop.hive.serde2.objectinspector.primitive.PrimitiveObjectInspectorUtils;</span><br><span class=\"line\">import org.apache.hadoop.io.BytesWritable;</span><br><span class=\"line\">import org.apache.hadoop.io.Text;</span><br><span class=\"line\"></span><br><span class=\"line\">&#x2F;**</span><br><span class=\"line\"> * GenericUDFSha2.</span><br><span class=\"line\"> *</span><br><span class=\"line\"> *&#x2F;</span><br><span class=\"line\">@Description(name &#x3D; &quot;sha2&quot;, value &#x3D; &quot;_FUNC_(string&#x2F;binary, len) - Calculates the SHA-2 family of hash functions &quot;</span><br><span class=\"line\">    + &quot;(SHA-224, SHA-256, SHA-384, and SHA-512).&quot;,</span><br><span class=\"line\">    extended &#x3D; &quot;The first argument is the string or binary to be hashed. &quot;</span><br><span class=\"line\">    + &quot;The second argument indicates the desired bit length of the result, &quot;</span><br><span class=\"line\">    + &quot;which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). &quot;</span><br><span class=\"line\">    + &quot;SHA-224 is supported starting from Java 8. &quot;</span><br><span class=\"line\">    + &quot;If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL.\\n&quot;</span><br><span class=\"line\">    + &quot;Example: &gt; SELECT _FUNC_(&#39;ABC&#39;, 256);\\n &#39;b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78&#39;&quot;)</span><br><span class=\"line\">public class GenericUDFSha2 extends GenericUDF &#123;</span><br><span class=\"line\">  private transient Converter[] converters &#x3D; new Converter[2];</span><br><span class=\"line\">  private transient PrimitiveCategory[] inputTypes &#x3D; new PrimitiveCategory[2];</span><br><span class=\"line\">  private final Text output &#x3D; new Text();</span><br><span class=\"line\">  private transient boolean isStr;</span><br><span class=\"line\">  private transient MessageDigest digest;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  public ObjectInspector initialize(ObjectInspector[] arguments) throws UDFArgumentException &#123;</span><br><span class=\"line\">    checkArgsSize(arguments, 2, 2);</span><br><span class=\"line\"></span><br><span class=\"line\">    checkArgPrimitive(arguments, 0);</span><br><span class=\"line\">    checkArgPrimitive(arguments, 1);</span><br><span class=\"line\"></span><br><span class=\"line\">    &#x2F;&#x2F; the function should support both string and binary input types</span><br><span class=\"line\">    checkArgGroups(arguments, 0, inputTypes, STRING_GROUP, BINARY_GROUP);</span><br><span class=\"line\">    checkArgGroups(arguments, 1, inputTypes, NUMERIC_GROUP);</span><br><span class=\"line\"></span><br><span class=\"line\">    if (PrimitiveObjectInspectorUtils.getPrimitiveGrouping(inputTypes[0]) &#x3D;&#x3D; STRING_GROUP) &#123;</span><br><span class=\"line\">      obtainStringConverter(arguments, 0, inputTypes, converters);</span><br><span class=\"line\">      isStr &#x3D; true;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      GenericUDFParamUtils.obtainBinaryConverter(arguments, 0, inputTypes, converters);</span><br><span class=\"line\">      isStr &#x3D; false;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    if (arguments[1] instanceof ConstantObjectInspector) &#123;</span><br><span class=\"line\">      Integer lenObj &#x3D; getConstantIntValue(arguments, 1);</span><br><span class=\"line\">      if (lenObj !&#x3D; null) &#123;</span><br><span class=\"line\">        int len &#x3D; lenObj.intValue();</span><br><span class=\"line\">        if (len &#x3D;&#x3D; 0) &#123;</span><br><span class=\"line\">          len &#x3D; 256;</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">        try &#123;</span><br><span class=\"line\">          digest &#x3D; MessageDigest.getInstance(&quot;SHA-&quot; + len);</span><br><span class=\"line\">        &#125; catch (NoSuchAlgorithmException e) &#123;</span><br><span class=\"line\">          &#x2F;&#x2F; ignore</span><br><span class=\"line\">        &#125;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      throw new UDFArgumentTypeException(1, getFuncName() + &quot; only takes constant as &quot;</span><br><span class=\"line\">          + getArgOrder(1) + &quot; argument&quot;);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    ObjectInspector outputOI &#x3D; PrimitiveObjectInspectorFactory.writableStringObjectInspector;</span><br><span class=\"line\">    return outputOI;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  public Object evaluate(DeferredObject[] arguments) throws HiveException &#123;</span><br><span class=\"line\">    if (digest &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">      return null;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    digest.reset();</span><br><span class=\"line\">    if (isStr) &#123;</span><br><span class=\"line\">      Text n &#x3D; GenericUDFParamUtils.getTextValue(arguments, 0, converters);</span><br><span class=\"line\">      if (n &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">        return null;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      digest.update(n.getBytes(), 0, n.getLength());</span><br><span class=\"line\">    &#125; else &#123;</span><br><span class=\"line\">      BytesWritable bWr &#x3D; GenericUDFParamUtils.getBinaryValue(arguments, 0, converters);</span><br><span class=\"line\">      if (bWr &#x3D;&#x3D; null) &#123;</span><br><span class=\"line\">        return null;</span><br><span class=\"line\">      &#125;</span><br><span class=\"line\">      digest.update(bWr.getBytes(), 0, bWr.getLength());</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\">    byte[] resBin &#x3D; digest.digest();</span><br><span class=\"line\">    String resStr &#x3D; Hex.encodeHexString(resBin);</span><br><span class=\"line\"></span><br><span class=\"line\">    output.set(resStr);</span><br><span class=\"line\">    return output;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  public String getDisplayString(String[] children) &#123;</span><br><span class=\"line\">    return getStandardDisplayString(getFuncName(), children);</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">  @Override</span><br><span class=\"line\">  protected String getFuncName() &#123;</span><br><span class=\"line\">    return &quot;sha2&quot;;</span><br><span class=\"line\">  &#125;</span><br><span class=\"line\">&#125;</span><br></pre></td></tr></table></figure>\n\n<p>And build the package.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mvn package</span><br></pre></td></tr></table></figure>\n<p>After compile you find in <code>target</code> dir the require jar (<code>GenericUDFSha2-1.0-SNAPSHOT.jar</code>) you need to add in Hive.</p>\n<p>You should use your Hadoop Distribution instructions for deploying new jars.</p>\n<p>Here are <a href=\"https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html\">Cloudera Instructions</a>.</p>\n<p>Next on your Hive session you need to <code>ADD JAR</code> and create a <code>FUNCTION</code> or <code>TEMPORARY FUNCTION</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ADD JAR .&#x2F;target&#x2F;GenericUDFSha2-1.0-SNAPSHOT.jar</span><br><span class=\"line\">CREATE TEMPORARY FUNCTION sha2 AS &#39;com.rramos.bigdata.utils.GenericUDFSha2&#39;;</span><br><span class=\"line\">SELECT sha2(foo) from bar LIMIT1; </span><br></pre></td></tr></table></figure>\n<p><a href=\"https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html\">Matthew Rathbone</a> Blog has some great tutorial on Hive Funtions. Take a look if you want to go deep with it.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://www.cloudera.com/documentation/enterprise/release-notes/topics/rg_cdh_vd.html\">https://www.cloudera.com/documentation/enterprise/release-notes/topics/rg_cdh_vd.html</a></li>\n<li><a href=\"https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html\">https://www.cloudera.com/documentation/enterprise/5-6-x/topics/cm_mc_hive_udf.html</a></li>\n<li><a href=\"https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html\">https://blog.matthewrathbone.com/2013/08/10/guide-to-writing-hive-udfs.html</a></li>\n</ul>\n"},{"title":"Hive Queries Crash When Inserting GC Exception","date":"2016-10-18T23:05:00.000Z","_content":"\n\nWhen running some queries with hive sometimes we get a very nice java exception of overhead limit\n\n```\nException in thread \"main\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n```\n\nI didn't know but it turns out you can supply directly on the jdbc connection of beeline the parameters to increase this values.\n\n\n```\njdbc:hive2://localhost:10000/default?mapreduce.map.memory.mb=3809;mapreduce.map.java.opts=-Xmx3428m;mapreduce.reduce.memory.mb=2560;mapreduce.reduce.java.opts=-Xmx2304m;\n```\n\nIt is important to understand the size of the containers in your cluster and this is usefull for some adhoc procedures.\n\nThe best way is to configure the containers memory size in hadoop, but this kind of quick solutions are usefull, expecially for testing parameters of new workflows.\n\nThere is some good info regarding this configuration in altiscale documentation:\n\nhttps://documentation.altiscale.com/heapsize-for-mappers-and-reducers\n\nCheers,\nRR\n","source":"_posts/hiveGC-md.md","raw":"---\ntitle: hive Queries crash when inserting GC Exception\ntags:\n  - hadoop\n  - hive\n  - GC\n  - exception\n  - beeline\ndate: 2016-10-19 00:05:00\n---\n\n\nWhen running some queries with hive sometimes we get a very nice java exception of overhead limit\n\n```\nException in thread \"main\" java.lang.OutOfMemoryError: GC overhead limit exceeded\n```\n\nI didn't know but it turns out you can supply directly on the jdbc connection of beeline the parameters to increase this values.\n\n\n```\njdbc:hive2://localhost:10000/default?mapreduce.map.memory.mb=3809;mapreduce.map.java.opts=-Xmx3428m;mapreduce.reduce.memory.mb=2560;mapreduce.reduce.java.opts=-Xmx2304m;\n```\n\nIt is important to understand the size of the containers in your cluster and this is usefull for some adhoc procedures.\n\nThe best way is to configure the containers memory size in hadoop, but this kind of quick solutions are usefull, expecially for testing parameters of new workflows.\n\nThere is some good info regarding this configuration in altiscale documentation:\n\nhttps://documentation.altiscale.com/heapsize-for-mappers-and-reducers\n\nCheers,\nRR\n","slug":"hiveGC-md","published":1,"updated":"2020-09-06T14:38:37.855Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjp000gchpf8makcx19","content":"<p>When running some queries with hive sometimes we get a very nice java exception of overhead limit</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded</span><br></pre></td></tr></table></figure>\n<p>I didn’t know but it turns out you can supply directly on the jdbc connection of beeline the parameters to increase this values.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jdbc:hive2:&#x2F;&#x2F;localhost:10000&#x2F;default?mapreduce.map.memory.mb&#x3D;3809;mapreduce.map.java.opts&#x3D;-Xmx3428m;mapreduce.reduce.memory.mb&#x3D;2560;mapreduce.reduce.java.opts&#x3D;-Xmx2304m;</span><br></pre></td></tr></table></figure>\n<p>It is important to understand the size of the containers in your cluster and this is usefull for some adhoc procedures.</p>\n<p>The best way is to configure the containers memory size in hadoop, but this kind of quick solutions are usefull, expecially for testing parameters of new workflows.</p>\n<p>There is some good info regarding this configuration in altiscale documentation:</p>\n<p><a href=\"https://documentation.altiscale.com/heapsize-for-mappers-and-reducers\">https://documentation.altiscale.com/heapsize-for-mappers-and-reducers</a></p>\n<p>Cheers,<br>RR</p>\n","site":{"data":{}},"excerpt":"","more":"<p>When running some queries with hive sometimes we get a very nice java exception of overhead limit</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Exception in thread &quot;main&quot; java.lang.OutOfMemoryError: GC overhead limit exceeded</span><br></pre></td></tr></table></figure>\n<p>I didn’t know but it turns out you can supply directly on the jdbc connection of beeline the parameters to increase this values.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">jdbc:hive2:&#x2F;&#x2F;localhost:10000&#x2F;default?mapreduce.map.memory.mb&#x3D;3809;mapreduce.map.java.opts&#x3D;-Xmx3428m;mapreduce.reduce.memory.mb&#x3D;2560;mapreduce.reduce.java.opts&#x3D;-Xmx2304m;</span><br></pre></td></tr></table></figure>\n<p>It is important to understand the size of the containers in your cluster and this is usefull for some adhoc procedures.</p>\n<p>The best way is to configure the containers memory size in hadoop, but this kind of quick solutions are usefull, expecially for testing parameters of new workflows.</p>\n<p>There is some good info regarding this configuration in altiscale documentation:</p>\n<p><a href=\"https://documentation.altiscale.com/heapsize-for-mappers-and-reducers\">https://documentation.altiscale.com/heapsize-for-mappers-and-reducers</a></p>\n<p>Cheers,<br>RR</p>\n"},{"title":"keybase.io","date":"2016-09-18T00:17:20.000Z","_content":"\n\n# Keybase\n\nSome time back i used to have my PGP key registered on MIT pgp server http://pgp.mit.edu/. It's one of the largest public directory for PGP keys, but it's kind of old fashnioned and attracts mostly the academic world.\n\nI'm not entirely sure how i come to find http://keybase.io but it seems a new interesting way to get PGP to the general audience.\n\nIt allows, as the slogan say:\n\n> Keybase maps your identity to your public keys, and vice versa.\n\nIn order to be a trully trusted PKI the registration is done by invitation only which seems interesting in order to maintain some sort of quality assurance, but will make the evolution a lot slower.\n\nSo basically this allows you to associate your domain, twitter account, github account,etc with you keys.\n\nIt also has it's own distributed filesystem KBFS.\n\nhttps://keybase.io/docs/kbfs/understanding_kbfs\n\nNow this, seems to me, like the most important factor on this solution. Because you can quickly access other people keys, trust them, and follow you know friends.\n\nI still have some invites left, in case you are interested drop me an email :)\n\nCheers, \nRR@keybase\nhttps://keybase.io/ruimsramos","source":"_posts/keybase-io.md","raw":"---\ntitle: keybase.io\ntags:\n  - keybase\n  - pgp\n  - encryption\n  - pki\n  - keys\n  - kbfs\ndate: 2016-09-18 01:17:20\n---\n\n\n# Keybase\n\nSome time back i used to have my PGP key registered on MIT pgp server http://pgp.mit.edu/. It's one of the largest public directory for PGP keys, but it's kind of old fashnioned and attracts mostly the academic world.\n\nI'm not entirely sure how i come to find http://keybase.io but it seems a new interesting way to get PGP to the general audience.\n\nIt allows, as the slogan say:\n\n> Keybase maps your identity to your public keys, and vice versa.\n\nIn order to be a trully trusted PKI the registration is done by invitation only which seems interesting in order to maintain some sort of quality assurance, but will make the evolution a lot slower.\n\nSo basically this allows you to associate your domain, twitter account, github account,etc with you keys.\n\nIt also has it's own distributed filesystem KBFS.\n\nhttps://keybase.io/docs/kbfs/understanding_kbfs\n\nNow this, seems to me, like the most important factor on this solution. Because you can quickly access other people keys, trust them, and follow you know friends.\n\nI still have some invites left, in case you are interested drop me an email :)\n\nCheers, \nRR@keybase\nhttps://keybase.io/ruimsramos","slug":"keybase-io","published":1,"updated":"2020-09-06T14:38:37.855Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjp000hchpf8udlc14f","content":"<h1 id=\"Keybase\"><a href=\"#Keybase\" class=\"headerlink\" title=\"Keybase\"></a>Keybase</h1><p>Some time back i used to have my PGP key registered on MIT pgp server <a href=\"http://pgp.mit.edu/\">http://pgp.mit.edu/</a>. It’s one of the largest public directory for PGP keys, but it’s kind of old fashnioned and attracts mostly the academic world.</p>\n<p>I’m not entirely sure how i come to find <a href=\"http://keybase.io/\">http://keybase.io</a> but it seems a new interesting way to get PGP to the general audience.</p>\n<p>It allows, as the slogan say:</p>\n<blockquote>\n<p>Keybase maps your identity to your public keys, and vice versa.</p>\n</blockquote>\n<p>In order to be a trully trusted PKI the registration is done by invitation only which seems interesting in order to maintain some sort of quality assurance, but will make the evolution a lot slower.</p>\n<p>So basically this allows you to associate your domain, twitter account, github account,etc with you keys.</p>\n<p>It also has it’s own distributed filesystem KBFS.</p>\n<p><a href=\"https://keybase.io/docs/kbfs/understanding_kbfs\">https://keybase.io/docs/kbfs/understanding_kbfs</a></p>\n<p>Now this, seems to me, like the most important factor on this solution. Because you can quickly access other people keys, trust them, and follow you know friends.</p>\n<p>I still have some invites left, in case you are interested drop me an email :)</p>\n<p>Cheers,<br>RR@keybase<br><a href=\"https://keybase.io/ruimsramos\">https://keybase.io/ruimsramos</a></p>\n","site":{"data":{}},"excerpt":"","more":"<h1 id=\"Keybase\"><a href=\"#Keybase\" class=\"headerlink\" title=\"Keybase\"></a>Keybase</h1><p>Some time back i used to have my PGP key registered on MIT pgp server <a href=\"http://pgp.mit.edu/\">http://pgp.mit.edu/</a>. It’s one of the largest public directory for PGP keys, but it’s kind of old fashnioned and attracts mostly the academic world.</p>\n<p>I’m not entirely sure how i come to find <a href=\"http://keybase.io/\">http://keybase.io</a> but it seems a new interesting way to get PGP to the general audience.</p>\n<p>It allows, as the slogan say:</p>\n<blockquote>\n<p>Keybase maps your identity to your public keys, and vice versa.</p>\n</blockquote>\n<p>In order to be a trully trusted PKI the registration is done by invitation only which seems interesting in order to maintain some sort of quality assurance, but will make the evolution a lot slower.</p>\n<p>So basically this allows you to associate your domain, twitter account, github account,etc with you keys.</p>\n<p>It also has it’s own distributed filesystem KBFS.</p>\n<p><a href=\"https://keybase.io/docs/kbfs/understanding_kbfs\">https://keybase.io/docs/kbfs/understanding_kbfs</a></p>\n<p>Now this, seems to me, like the most important factor on this solution. Because you can quickly access other people keys, trust them, and follow you know friends.</p>\n<p>I still have some invites left, in case you are interested drop me an email :)</p>\n<p>Cheers,<br>RR@keybase<br><a href=\"https://keybase.io/ruimsramos\">https://keybase.io/ruimsramos</a></p>\n"},{"title":"Streaming SQL for Kafka (KSQL)","date":"2017-09-24T14:54:00.000Z","_content":"\n\nIn this article i'll explore Apache Kafka KSQL\n\n## Requirements\n\n * Docker\n * docker-compose\n\n## Description\n\n{% blockquote Offical Website Definition%}\n\nKSQL is an open source streaming SQL engine that implements continuous, interactive queries against Apache Kafka™. It allows you to query, read, write, and process data in Apache Kafka in real-time, at scale using SQL commands. KSQL interacts directly with the Kafka Streams API, removing the requirement of building a Java app.\n\n{% endblockquote %}\n\n## PoC Setup\n\nCreate the following file `docker-compose.yml`\n\n```\n---\nversion: '2'\nservices:\n  zookeeper:\n    image: \"confluentinc/cp-zookeeper:latest\"\n    hostname: zookeeper\n    ports:\n      - '32181:32181'\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 32181\n      ZOOKEEPER_TICK_TIME: 2000\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n  kafka:\n    image: \"confluentinc/cp-enterprise-kafka:latest\"\n    hostname: kafka\n    ports:\n      - '9092:9092'\n      - '29092:29092'\n    depends_on:\n      - zookeeper\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\n      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092\n      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:32181\n      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1\n      CONFLUENT_METRICS_ENABLE: 'true'\n      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n  schema-registry:\n    image: \"confluentinc/cp-schema-registry:latest\"\n    hostname: schema-registry\n    depends_on:\n      - zookeeper\n      - kafka\n    ports:\n      - '8081:8081'\n    environment:\n      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:32181\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n  # Runs the Kafka KSQL data generator for topic called \"pageviews\"\n  ksql-datagen-pageviews:\n    image: \"confluentinc/ksql-examples:latest\"\n    hostname: ksql-datagen-pageviews\n    depends_on:\n      - kafka\n      - schema-registry\n    # Note: The container's `run` script will perform the same readiness checks\n    # for Kafka and Confluent Schema Registry, but that's ok because they complete fast.\n    # The reason we check for readiness here is that we can insert a sleep time\n    # for topic creation before we start the application.\n    command: \"bash -c 'echo Waiting for Kafka to be ready... && \\\n                       cub kafka-ready -b kafka:29092 1 20 && \\\n                       echo Waiting for Confluent Schema Registry to be ready... && \\\n                       cub sr-ready schema-registry 8081 20 && \\\n                       echo Waiting a few seconds for topic creation to finish... && \\\n                       sleep 2 && \\\n                       java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar\n                       quickstart=pageviews format=delimited topic=pageviews bootstrap-server=kafka:29092 maxInterval=100 iterations=1000 && \\\n                       java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar\n                       quickstart=pageviews format=delimited topic=pageviews bootstrap-server=kafka:29092 maxInterval=1000'\"\n    environment:\n      KSQL_CONFIG_DIR: \"/etc/ksql\"\n      KSQL_LOG4J_OPTS: \"-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties\"\n      STREAMS_BOOTSTRAP_SERVERS: kafka:29092\n      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry\n      STREAMS_SCHEMA_REGISTRY_PORT: 8081\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n  # Runs the Kafka KSQL data generator for topic called \"users\"\n  ksql-datagen-users:\n    image: \"confluentinc/ksql-examples:latest\"\n    hostname: ksql-datagen-users\n    depends_on:\n      - kafka\n      - schema-registry\n    # Note: The container's `run` script will perform the same readiness checks\n    # for Kafka and Confluent Schema Registry, but that's ok because they complete fast.\n    # The reason we check for readiness here is that we can insert a sleep time\n    # for topic creation before we start the application.\n    command: \"bash -c 'echo Waiting for Kafka to be ready... && \\\n                       cub kafka-ready -b kafka:29092 1 20 && \\\n                       echo Waiting for Confluent Schema Registry to be ready... && \\\n                       cub sr-ready schema-registry 8081 20 && \\\n                       echo Waiting a few seconds for topic creation to finish... && \\\n                       sleep 2 && \\\n                       java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar\n                       quickstart=users format=json topic=users bootstrap-server=kafka:29092 maxInterval=100 iterations=1000 && \\\n                       java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar\n                       quickstart=users format=json topic=users bootstrap-server=kafka:29092 maxInterval=1000'\"\n    environment:\n      KSQL_CONFIG_DIR: \"/etc/ksql\"\n      KSQL_LOG4J_OPTS: \"-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties\"\n      STREAMS_BOOTSTRAP_SERVERS: kafka:29092\n      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry\n      STREAMS_SCHEMA_REGISTRY_PORT: 8081\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n  # Runs the Kafka KSQL application\n  ksql-cli:\n    image: \"confluentinc/ksql-cli:latest\"\n    hostname: ksql-cli\n    depends_on:\n      - kafka\n      - schema-registry\n      - ksql-datagen-pageviews\n      - ksql-datagen-users\n    command: \"perl -e 'while(1){ sleep 99999 }'\"\n    environment:\n      KSQL_CONFIG_DIR: \"/etc/ksql\"\n      KSQL_LOG4J_OPTS: \"-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties\"\n      STREAMS_BOOTSTRAP_SERVERS: kafka:29092\n      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry\n      STREAMS_SCHEMA_REGISTRY_PORT: 8081\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n```\n\nPrepare the enviroment with the following command\n\n```\nsudo docker-compose up -d\n```\n\nThis will create the minimal services to test ksql\n* zookeeper\n* schema-registry\n* kafka\n* ksql-cli\n\nIt also will create 2 dockers which will generate data in order to test:\n* ksql-datagen-pageviews\n* ksql-datagen-users\n\nThoose dockers are kafka producers, which we will use to test KSQL.\n\n\n## KSQL\n\nExecute the CLI in order to test\n\n```\nsudo docker-compose exec ksql-cli ksql-cli local --bootstrap-server kafka:29092\n```\n\nYou will see something like this\n\n```\n                       ======================================\n                       =      _  __ _____  ____  _          =\n                       =     | |/ // ____|/ __ \\| |         =\n                       =     | ' /| (___ | |  | | |         =\n                       =     |  <  \\___ \\| |  | | |         =\n                       =     | . \\ ____) | |__| | |____     =\n                       =     |_|\\_\\_____/ \\___\\_\\______|    =\n                       =                                    =\n                       =   Streaming SQL Engine for Kafka   =\nCopyright 2017 Confluent Inc.                         \n\nCLI v0.1, Server v0.1 located at http://localhost:9098\n\nHaving trouble? Type 'help' (case-insensitive) for a rundown of how things work!\n\nksql>\n```\n\n\nNow let's create a new **STREAM**\n\n```\nCREATE STREAM pageviews_original (viewtime bigint, userid varchar, pageid varchar) WITH (kafka_topic='pageviews', value_format='DELIMITED');\n```\n\nLet's describe the created object\n\n```\nksql> DESCRIBE pageviews_original;\n\n Field    | Type            \n----------------------------\n ROWTIME  | BIGINT          \n ROWKEY   | VARCHAR(STRING) \n VIEWTIME | BIGINT          \n USERID   | VARCHAR(STRING) \n PAGEID   | VARCHAR(STRING) \n```\n\nNow lets create a users **TABLE**\n\n```\nCREATE TABLE users_original (registertime bigint, gender varchar, regionid varchar, userid varchar) WITH (kafka_topic='users', value_format='JSON');\n```\n\nAnd confirm the DDL\n\n```\nksql> DESCRIBE users_original;\n\n Field        | Type            \n--------------------------------\n ROWTIME      | BIGINT          \n ROWKEY       | VARCHAR(STRING) \n REGISTERTIME | BIGINT          \n GENDER       | VARCHAR(STRING) \n REGIONID     | VARCHAR(STRING) \n USERID       | VARCHAR(STRING) \n```\n\nSo there are two main objects **TABLES** and **STREAMS**. To see the existing ones, execute:\n\n```\nksql> SHOW STREAMS;\n\n Stream Name        | Kafka Topic | Format    \n----------------------------------------------\n PAGEVIEWS_ORIGINAL | pageviews   | DELIMITED \n```\n\nand \n\n```\nksql> SHOW TABLES;\n\n Table Name     | Kafka Topic | Format | Windowed \n--------------------------------------------------\n USERS_ORIGINAL | users       | JSON   | false    \n```\n\n### Differences between STREAMS and TABLES\n\n**Stream**\n\nA stream is an unbounded sequence of structured data (\"facts\"). For example, we could have a stream of financial transactions such as \"Alice sent $100 to Bob, then Charlie sent $50 to Bob\". Facts in a stream are immutable, which means new facts can be inserted to a stream, but existing facts can never be updated or deleted. Streams can be created from a Kafka topic or derived from existing streams and tables.\n\n**Table**\n\nA table is a view of a stream, or another table, and represents a collection of evolving facts. For example, we could have a table that contains the latest financial information such as \"Bob’s current account balance is $150\". It is the equivalent of a traditional database table but enriched by streaming semantics such as windowing. Facts in a table are mutable, which means new facts can be inserted to the table, and existing facts can be updated or deleted. Tables can be created from a Kafka topic or derived from existing streams and tables.\n\n### Extending the Model\n\nLets create a new pageviews stream with key `pageid` and joi it with table `users`.\n\nFirst create the new stream\n\n```\nCREATE STREAM pageviews \\\n   (viewtime BIGINT, \\\n    userid VARCHAR, \\\n    pageid VARCHAR) \\\n   WITH (kafka_topic='pageviews', \\\n         value_format='DELIMITED', \\\n         key='pageid', \\\n         timestamp='viewtime');\n```\n\nAnd users table\n\n```\nCREATE TABLE users \\\n   (registertime BIGINT, \\\n    gender VARCHAR, \\\n    regionid VARCHAR, \\\n    userid VARCHAR, \\\n    interests array<VARCHAR>, \\\n    contact_info map<VARCHAR, VARCHAR>) \\\n   WITH (kafka_topic='users', \\\n         value_format='JSON');\n\n```\n\n#### Aggregations\n\nLet's test some aggregations\n\n```\nSELECT count(*),userid FROM pageviews GROUP BY userid;\n```\n\nAs data arrives in stream this is constantly updating , but we can verify that the count values begins from the start of the query, this could be usefull to dump to another topic with transformed data.\n\nLet's do that.\n\n```\nCREATE TABLE user_counts AS select count(*),userid from pageviews group by userid;\n```\n\nWe can use the statement `SHOW TOPICS` which is usefull and confirmed that the USER_COUNTS TOPICS is created.\n\nThe following aggregations are available\n\n|Function    | Example    | Description |\n|------------|------------|--------------|\n|COUNT       | COUNT(col1)| Count the number of rows|\n|MAX         | MAX(col1)  | Return the maximum value for a given column and window|\n|MIN         |MIN(col1)  | Return the minimum value for a given column and window|\n|SUM         | SUM(col1) |  Sums the column values|\n\n## Window\n\nThe WINDOW clause lets you control how to group input records that have the same key into so-called windows for operations such as aggregations or joins. Windows are tracked per record key. KSQL supports the following WINDOW types:\n\n* TUMBLING\n* HOPPING\n* SESSION\n\n### Window Tumbling\n\nTUMBLING: Tumbling windows group input records into fixed-sized, non-overlapping windows based on the records' timestamps. You must specify the window size for tumbling windows. Note: Tumbling windows are a special case of hopping windows where the window size is equal to the advance interval.\n\nExample:\n```\nSELECT item_id, SUM(quantity)\n  FROM orders\n  WINDOW TUMBLING (SIZE 20 SECONDS)\n  GROUP BY item_id;\n```\n\n\n### Window HOPPING\n\nHOPPING: Hopping windows group input records into fixed-sized, (possibly) overlapping windows based on the records' timestamps. You must specify the window size and the advance interval for hopping windows.\n\nExample:\n\n```\nSELECT item_id, SUM(quantity)\n  FROM orders\n  WINDOW HOPPING (SIZE 20 SECONDS, ADVANCE BY 5 SECONDS)\n  GROUP BY item_id;\n```\n\n### Window SESSION\n\nSESSION: Session windows group input records into so-called sessions. You must specify the session inactivity gap parameter for session windows. For example, imagine you set the inactivity gap to 5 minutes. If, for a given record key such as \"alice\", no new input data arrives for more than 5 minutes, then the current session for \"alice\" is closed, and any newly arriving data for \"alice\" in the future will mark the beginning of a new session.\n\nExample:\n\n```\nSELECT item_id, SUM(quantity)\n  FROM orders\n  WINDOW SESSION (20 SECONDS)\n  GROUP BY item_id;\n```\n\n## Transformations\n\nLet's create a new stream with a column transformation\n\n```\nCREATE STREAM pageviews_transformed \\\n  WITH (timestamp='viewtime', \\\n        partitions=5, \\\n        value_format='JSON') AS \\\n  SELECT viewtime, \\\n         userid, \\\n         pageid, \\\n         TIMESTAMPTOSTRING(viewtime, 'yyyy-MM-dd HH:mm:ss.SSS') AS timestring \\\n  FROM pageviews \\\n  PARTITION BY userid;\n```\n\n## Joining\n\nAnd joining the STREAM with enriched data from TABLE\n\n```\nCREATE STREAM pageviews_enriched AS \\\n   SELECT pv.viewtime, \\\n          pv.userid AS userid, \\\n          pv.pageid, \\\n          pv.timestring, \\\n          u.gender, \\\n          u.regionid, \\\n          u.interests, \\\n          u.contact_info \\\n   FROM pageviews_transformed pv \\\n   LEFT JOIN users u ON pv.userid = users.userid;\n```\n\n# Use Cases\n\nCommon KSQL use cases are:\n\n* Fraud detection - identify and act on out of the ordinary data to provide real-time awareness.\n* Personalization - create real-time experiences and insight for end users driven by data.\n* Notifications - build custom alerts and messages based on real-time data.\n* Real-time Analytics - power real-time dashboards to understand what’s happening as it does.\n* Sensor data and IoT - understand and deliver sensor data how and where it needs to be.\n* Customer 360 - provide a clear, real-time understanding of your customers across every interaction.\n\n### Streaming ETL\n\nKSQL makes it simple to transform data within the pipeline, readying messages to cleanly land in another system\n\n### Anomaly Detection\n\nKSQL is a good fit for identifying patterns or anomalies on real-time data. By processing the stream as data arrives you can identify and properly surface out of the ordinary events with millisecond latency.\n\n### Monitoring\n\nKafka's ability to provide scalable ordered messages with stream processing make it a common solution for log data monitoring and alerting. KSQL lends a familiar syntax for tracking, understanding, and managing alerts.\n\n# Conclusion\n\nOn a first glance KSQL seems pretty easy to start using it. It's still pretty new, i would give it a time to mature before start using it on production.\n\nFor small transformations, data-quality control and analytical queries that don't take into account a large windows this seems a good solution. For more complex queries i still prefer to keep Kafka to it's core business and let the computation work to Spark.  \n\n\n## More Information\n\n* KSQL is available as a developer preview on [Github](http://go2.confluent.io/Q0KX0lH0Qq2kx0t3r00U8X0)\n* Read the [documentation](http://go2.confluent.io/m000tV2080xr3kXlHKr0QX0) for KSQL\n* Join the conversation in the #ksql channel in the [Confluent Community Slack](http://go2.confluent.io/m000tW2080xr3kXlHKs0QX0). \n\n## References\n\n* https://github.com/confluentinc/ksql\n* https://github.com/confluentinc/ksql/tree/0.1.x/docs/quickstart#quick-start\n* https://github.com/confluentinc/ksql/tree/0.1.x/ksql-clickstream-demo#clickstream-analysis\n\n\n\n\n\n","source":"_posts/ksql.md","raw":"---\ntitle: Streaming SQL for Kafka (KSQL)\ntags:\n  - KSQL\n  - Kafka\n  - SQL\n  - Streaming\n  - Docker\ndate: 2017-09-24 15:54:00\n---\n\n\nIn this article i'll explore Apache Kafka KSQL\n\n## Requirements\n\n * Docker\n * docker-compose\n\n## Description\n\n{% blockquote Offical Website Definition%}\n\nKSQL is an open source streaming SQL engine that implements continuous, interactive queries against Apache Kafka™. It allows you to query, read, write, and process data in Apache Kafka in real-time, at scale using SQL commands. KSQL interacts directly with the Kafka Streams API, removing the requirement of building a Java app.\n\n{% endblockquote %}\n\n## PoC Setup\n\nCreate the following file `docker-compose.yml`\n\n```\n---\nversion: '2'\nservices:\n  zookeeper:\n    image: \"confluentinc/cp-zookeeper:latest\"\n    hostname: zookeeper\n    ports:\n      - '32181:32181'\n    environment:\n      ZOOKEEPER_CLIENT_PORT: 32181\n      ZOOKEEPER_TICK_TIME: 2000\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n  kafka:\n    image: \"confluentinc/cp-enterprise-kafka:latest\"\n    hostname: kafka\n    ports:\n      - '9092:9092'\n      - '29092:29092'\n    depends_on:\n      - zookeeper\n    environment:\n      KAFKA_BROKER_ID: 1\n      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181\n      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT\n      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT\n      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092\n      KAFKA_AUTO_CREATE_TOPICS_ENABLE: \"true\"\n      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter\n      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092\n      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:32181\n      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1\n      CONFLUENT_METRICS_ENABLE: 'true'\n      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n  schema-registry:\n    image: \"confluentinc/cp-schema-registry:latest\"\n    hostname: schema-registry\n    depends_on:\n      - zookeeper\n      - kafka\n    ports:\n      - '8081:8081'\n    environment:\n      SCHEMA_REGISTRY_HOST_NAME: schema-registry\n      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:32181\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n  # Runs the Kafka KSQL data generator for topic called \"pageviews\"\n  ksql-datagen-pageviews:\n    image: \"confluentinc/ksql-examples:latest\"\n    hostname: ksql-datagen-pageviews\n    depends_on:\n      - kafka\n      - schema-registry\n    # Note: The container's `run` script will perform the same readiness checks\n    # for Kafka and Confluent Schema Registry, but that's ok because they complete fast.\n    # The reason we check for readiness here is that we can insert a sleep time\n    # for topic creation before we start the application.\n    command: \"bash -c 'echo Waiting for Kafka to be ready... && \\\n                       cub kafka-ready -b kafka:29092 1 20 && \\\n                       echo Waiting for Confluent Schema Registry to be ready... && \\\n                       cub sr-ready schema-registry 8081 20 && \\\n                       echo Waiting a few seconds for topic creation to finish... && \\\n                       sleep 2 && \\\n                       java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar\n                       quickstart=pageviews format=delimited topic=pageviews bootstrap-server=kafka:29092 maxInterval=100 iterations=1000 && \\\n                       java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar\n                       quickstart=pageviews format=delimited topic=pageviews bootstrap-server=kafka:29092 maxInterval=1000'\"\n    environment:\n      KSQL_CONFIG_DIR: \"/etc/ksql\"\n      KSQL_LOG4J_OPTS: \"-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties\"\n      STREAMS_BOOTSTRAP_SERVERS: kafka:29092\n      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry\n      STREAMS_SCHEMA_REGISTRY_PORT: 8081\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n  # Runs the Kafka KSQL data generator for topic called \"users\"\n  ksql-datagen-users:\n    image: \"confluentinc/ksql-examples:latest\"\n    hostname: ksql-datagen-users\n    depends_on:\n      - kafka\n      - schema-registry\n    # Note: The container's `run` script will perform the same readiness checks\n    # for Kafka and Confluent Schema Registry, but that's ok because they complete fast.\n    # The reason we check for readiness here is that we can insert a sleep time\n    # for topic creation before we start the application.\n    command: \"bash -c 'echo Waiting for Kafka to be ready... && \\\n                       cub kafka-ready -b kafka:29092 1 20 && \\\n                       echo Waiting for Confluent Schema Registry to be ready... && \\\n                       cub sr-ready schema-registry 8081 20 && \\\n                       echo Waiting a few seconds for topic creation to finish... && \\\n                       sleep 2 && \\\n                       java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar\n                       quickstart=users format=json topic=users bootstrap-server=kafka:29092 maxInterval=100 iterations=1000 && \\\n                       java -jar /usr/share/java/ksql-examples/ksql-examples-0.1-SNAPSHOT-standalone.jar\n                       quickstart=users format=json topic=users bootstrap-server=kafka:29092 maxInterval=1000'\"\n    environment:\n      KSQL_CONFIG_DIR: \"/etc/ksql\"\n      KSQL_LOG4J_OPTS: \"-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties\"\n      STREAMS_BOOTSTRAP_SERVERS: kafka:29092\n      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry\n      STREAMS_SCHEMA_REGISTRY_PORT: 8081\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n  # Runs the Kafka KSQL application\n  ksql-cli:\n    image: \"confluentinc/ksql-cli:latest\"\n    hostname: ksql-cli\n    depends_on:\n      - kafka\n      - schema-registry\n      - ksql-datagen-pageviews\n      - ksql-datagen-users\n    command: \"perl -e 'while(1){ sleep 99999 }'\"\n    environment:\n      KSQL_CONFIG_DIR: \"/etc/ksql\"\n      KSQL_LOG4J_OPTS: \"-Dlog4j.configuration=file:/etc/ksql/log4j-rolling.properties\"\n      STREAMS_BOOTSTRAP_SERVERS: kafka:29092\n      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry\n      STREAMS_SCHEMA_REGISTRY_PORT: 8081\n    extra_hosts:\n      - \"moby:127.0.0.1\"\n\n```\n\nPrepare the enviroment with the following command\n\n```\nsudo docker-compose up -d\n```\n\nThis will create the minimal services to test ksql\n* zookeeper\n* schema-registry\n* kafka\n* ksql-cli\n\nIt also will create 2 dockers which will generate data in order to test:\n* ksql-datagen-pageviews\n* ksql-datagen-users\n\nThoose dockers are kafka producers, which we will use to test KSQL.\n\n\n## KSQL\n\nExecute the CLI in order to test\n\n```\nsudo docker-compose exec ksql-cli ksql-cli local --bootstrap-server kafka:29092\n```\n\nYou will see something like this\n\n```\n                       ======================================\n                       =      _  __ _____  ____  _          =\n                       =     | |/ // ____|/ __ \\| |         =\n                       =     | ' /| (___ | |  | | |         =\n                       =     |  <  \\___ \\| |  | | |         =\n                       =     | . \\ ____) | |__| | |____     =\n                       =     |_|\\_\\_____/ \\___\\_\\______|    =\n                       =                                    =\n                       =   Streaming SQL Engine for Kafka   =\nCopyright 2017 Confluent Inc.                         \n\nCLI v0.1, Server v0.1 located at http://localhost:9098\n\nHaving trouble? Type 'help' (case-insensitive) for a rundown of how things work!\n\nksql>\n```\n\n\nNow let's create a new **STREAM**\n\n```\nCREATE STREAM pageviews_original (viewtime bigint, userid varchar, pageid varchar) WITH (kafka_topic='pageviews', value_format='DELIMITED');\n```\n\nLet's describe the created object\n\n```\nksql> DESCRIBE pageviews_original;\n\n Field    | Type            \n----------------------------\n ROWTIME  | BIGINT          \n ROWKEY   | VARCHAR(STRING) \n VIEWTIME | BIGINT          \n USERID   | VARCHAR(STRING) \n PAGEID   | VARCHAR(STRING) \n```\n\nNow lets create a users **TABLE**\n\n```\nCREATE TABLE users_original (registertime bigint, gender varchar, regionid varchar, userid varchar) WITH (kafka_topic='users', value_format='JSON');\n```\n\nAnd confirm the DDL\n\n```\nksql> DESCRIBE users_original;\n\n Field        | Type            \n--------------------------------\n ROWTIME      | BIGINT          \n ROWKEY       | VARCHAR(STRING) \n REGISTERTIME | BIGINT          \n GENDER       | VARCHAR(STRING) \n REGIONID     | VARCHAR(STRING) \n USERID       | VARCHAR(STRING) \n```\n\nSo there are two main objects **TABLES** and **STREAMS**. To see the existing ones, execute:\n\n```\nksql> SHOW STREAMS;\n\n Stream Name        | Kafka Topic | Format    \n----------------------------------------------\n PAGEVIEWS_ORIGINAL | pageviews   | DELIMITED \n```\n\nand \n\n```\nksql> SHOW TABLES;\n\n Table Name     | Kafka Topic | Format | Windowed \n--------------------------------------------------\n USERS_ORIGINAL | users       | JSON   | false    \n```\n\n### Differences between STREAMS and TABLES\n\n**Stream**\n\nA stream is an unbounded sequence of structured data (\"facts\"). For example, we could have a stream of financial transactions such as \"Alice sent $100 to Bob, then Charlie sent $50 to Bob\". Facts in a stream are immutable, which means new facts can be inserted to a stream, but existing facts can never be updated or deleted. Streams can be created from a Kafka topic or derived from existing streams and tables.\n\n**Table**\n\nA table is a view of a stream, or another table, and represents a collection of evolving facts. For example, we could have a table that contains the latest financial information such as \"Bob’s current account balance is $150\". It is the equivalent of a traditional database table but enriched by streaming semantics such as windowing. Facts in a table are mutable, which means new facts can be inserted to the table, and existing facts can be updated or deleted. Tables can be created from a Kafka topic or derived from existing streams and tables.\n\n### Extending the Model\n\nLets create a new pageviews stream with key `pageid` and joi it with table `users`.\n\nFirst create the new stream\n\n```\nCREATE STREAM pageviews \\\n   (viewtime BIGINT, \\\n    userid VARCHAR, \\\n    pageid VARCHAR) \\\n   WITH (kafka_topic='pageviews', \\\n         value_format='DELIMITED', \\\n         key='pageid', \\\n         timestamp='viewtime');\n```\n\nAnd users table\n\n```\nCREATE TABLE users \\\n   (registertime BIGINT, \\\n    gender VARCHAR, \\\n    regionid VARCHAR, \\\n    userid VARCHAR, \\\n    interests array<VARCHAR>, \\\n    contact_info map<VARCHAR, VARCHAR>) \\\n   WITH (kafka_topic='users', \\\n         value_format='JSON');\n\n```\n\n#### Aggregations\n\nLet's test some aggregations\n\n```\nSELECT count(*),userid FROM pageviews GROUP BY userid;\n```\n\nAs data arrives in stream this is constantly updating , but we can verify that the count values begins from the start of the query, this could be usefull to dump to another topic with transformed data.\n\nLet's do that.\n\n```\nCREATE TABLE user_counts AS select count(*),userid from pageviews group by userid;\n```\n\nWe can use the statement `SHOW TOPICS` which is usefull and confirmed that the USER_COUNTS TOPICS is created.\n\nThe following aggregations are available\n\n|Function    | Example    | Description |\n|------------|------------|--------------|\n|COUNT       | COUNT(col1)| Count the number of rows|\n|MAX         | MAX(col1)  | Return the maximum value for a given column and window|\n|MIN         |MIN(col1)  | Return the minimum value for a given column and window|\n|SUM         | SUM(col1) |  Sums the column values|\n\n## Window\n\nThe WINDOW clause lets you control how to group input records that have the same key into so-called windows for operations such as aggregations or joins. Windows are tracked per record key. KSQL supports the following WINDOW types:\n\n* TUMBLING\n* HOPPING\n* SESSION\n\n### Window Tumbling\n\nTUMBLING: Tumbling windows group input records into fixed-sized, non-overlapping windows based on the records' timestamps. You must specify the window size for tumbling windows. Note: Tumbling windows are a special case of hopping windows where the window size is equal to the advance interval.\n\nExample:\n```\nSELECT item_id, SUM(quantity)\n  FROM orders\n  WINDOW TUMBLING (SIZE 20 SECONDS)\n  GROUP BY item_id;\n```\n\n\n### Window HOPPING\n\nHOPPING: Hopping windows group input records into fixed-sized, (possibly) overlapping windows based on the records' timestamps. You must specify the window size and the advance interval for hopping windows.\n\nExample:\n\n```\nSELECT item_id, SUM(quantity)\n  FROM orders\n  WINDOW HOPPING (SIZE 20 SECONDS, ADVANCE BY 5 SECONDS)\n  GROUP BY item_id;\n```\n\n### Window SESSION\n\nSESSION: Session windows group input records into so-called sessions. You must specify the session inactivity gap parameter for session windows. For example, imagine you set the inactivity gap to 5 minutes. If, for a given record key such as \"alice\", no new input data arrives for more than 5 minutes, then the current session for \"alice\" is closed, and any newly arriving data for \"alice\" in the future will mark the beginning of a new session.\n\nExample:\n\n```\nSELECT item_id, SUM(quantity)\n  FROM orders\n  WINDOW SESSION (20 SECONDS)\n  GROUP BY item_id;\n```\n\n## Transformations\n\nLet's create a new stream with a column transformation\n\n```\nCREATE STREAM pageviews_transformed \\\n  WITH (timestamp='viewtime', \\\n        partitions=5, \\\n        value_format='JSON') AS \\\n  SELECT viewtime, \\\n         userid, \\\n         pageid, \\\n         TIMESTAMPTOSTRING(viewtime, 'yyyy-MM-dd HH:mm:ss.SSS') AS timestring \\\n  FROM pageviews \\\n  PARTITION BY userid;\n```\n\n## Joining\n\nAnd joining the STREAM with enriched data from TABLE\n\n```\nCREATE STREAM pageviews_enriched AS \\\n   SELECT pv.viewtime, \\\n          pv.userid AS userid, \\\n          pv.pageid, \\\n          pv.timestring, \\\n          u.gender, \\\n          u.regionid, \\\n          u.interests, \\\n          u.contact_info \\\n   FROM pageviews_transformed pv \\\n   LEFT JOIN users u ON pv.userid = users.userid;\n```\n\n# Use Cases\n\nCommon KSQL use cases are:\n\n* Fraud detection - identify and act on out of the ordinary data to provide real-time awareness.\n* Personalization - create real-time experiences and insight for end users driven by data.\n* Notifications - build custom alerts and messages based on real-time data.\n* Real-time Analytics - power real-time dashboards to understand what’s happening as it does.\n* Sensor data and IoT - understand and deliver sensor data how and where it needs to be.\n* Customer 360 - provide a clear, real-time understanding of your customers across every interaction.\n\n### Streaming ETL\n\nKSQL makes it simple to transform data within the pipeline, readying messages to cleanly land in another system\n\n### Anomaly Detection\n\nKSQL is a good fit for identifying patterns or anomalies on real-time data. By processing the stream as data arrives you can identify and properly surface out of the ordinary events with millisecond latency.\n\n### Monitoring\n\nKafka's ability to provide scalable ordered messages with stream processing make it a common solution for log data monitoring and alerting. KSQL lends a familiar syntax for tracking, understanding, and managing alerts.\n\n# Conclusion\n\nOn a first glance KSQL seems pretty easy to start using it. It's still pretty new, i would give it a time to mature before start using it on production.\n\nFor small transformations, data-quality control and analytical queries that don't take into account a large windows this seems a good solution. For more complex queries i still prefer to keep Kafka to it's core business and let the computation work to Spark.  \n\n\n## More Information\n\n* KSQL is available as a developer preview on [Github](http://go2.confluent.io/Q0KX0lH0Qq2kx0t3r00U8X0)\n* Read the [documentation](http://go2.confluent.io/m000tV2080xr3kXlHKr0QX0) for KSQL\n* Join the conversation in the #ksql channel in the [Confluent Community Slack](http://go2.confluent.io/m000tW2080xr3kXlHKs0QX0). \n\n## References\n\n* https://github.com/confluentinc/ksql\n* https://github.com/confluentinc/ksql/tree/0.1.x/docs/quickstart#quick-start\n* https://github.com/confluentinc/ksql/tree/0.1.x/ksql-clickstream-demo#clickstream-analysis\n\n\n\n\n\n","slug":"ksql","published":1,"updated":"2020-09-06T14:38:37.855Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjq000ichpfcnli7gqb","content":"<p>In this article i’ll explore Apache Kafka KSQL</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><ul>\n<li>Docker</li>\n<li>docker-compose</li>\n</ul>\n<h2 id=\"Description\"><a href=\"#Description\" class=\"headerlink\" title=\"Description\"></a>Description</h2><blockquote><p>KSQL is an open source streaming SQL engine that implements continuous, interactive queries against Apache Kafka™. It allows you to query, read, write, and process data in Apache Kafka in real-time, at scale using SQL commands. KSQL interacts directly with the Kafka Streams API, removing the requirement of building a Java app.</p>\n<footer><strong>Offical Website Definition</strong></footer></blockquote>\n\n<h2 id=\"PoC-Setup\"><a href=\"#PoC-Setup\" class=\"headerlink\" title=\"PoC Setup\"></a>PoC Setup</h2><p>Create the following file <code>docker-compose.yml</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">version: &#39;2&#39;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  zookeeper:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;cp-zookeeper:latest&quot;</span><br><span class=\"line\">    hostname: zookeeper</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &#39;32181:32181&#39;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      ZOOKEEPER_CLIENT_PORT: 32181</span><br><span class=\"line\">      ZOOKEEPER_TICK_TIME: 2000</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  kafka:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;cp-enterprise-kafka:latest&quot;</span><br><span class=\"line\">    hostname: kafka</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &#39;9092:9092&#39;</span><br><span class=\"line\">      - &#39;29092:29092&#39;</span><br><span class=\"line\">    depends_on:</span><br><span class=\"line\">      - zookeeper</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      KAFKA_BROKER_ID: 1</span><br><span class=\"line\">      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181</span><br><span class=\"line\">      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT</span><br><span class=\"line\">      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT</span><br><span class=\"line\">      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT:&#x2F;&#x2F;kafka:29092,PLAINTEXT_HOST:&#x2F;&#x2F;localhost:9092</span><br><span class=\"line\">      KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;true&quot;</span><br><span class=\"line\">      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter</span><br><span class=\"line\">      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1</span><br><span class=\"line\">      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092</span><br><span class=\"line\">      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:32181</span><br><span class=\"line\">      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1</span><br><span class=\"line\">      CONFLUENT_METRICS_ENABLE: &#39;true&#39;</span><br><span class=\"line\">      CONFLUENT_SUPPORT_CUSTOMER_ID: &#39;anonymous&#39;</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  schema-registry:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;cp-schema-registry:latest&quot;</span><br><span class=\"line\">    hostname: schema-registry</span><br><span class=\"line\">    depends_on:</span><br><span class=\"line\">      - zookeeper</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &#39;8081:8081&#39;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      SCHEMA_REGISTRY_HOST_NAME: schema-registry</span><br><span class=\"line\">      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:32181</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Runs the Kafka KSQL data generator for topic called &quot;pageviews&quot;</span><br><span class=\"line\">  ksql-datagen-pageviews:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;ksql-examples:latest&quot;</span><br><span class=\"line\">    hostname: ksql-datagen-pageviews</span><br><span class=\"line\">    depends_on:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">      - schema-registry</span><br><span class=\"line\">    # Note: The container&#39;s &#96;run&#96; script will perform the same readiness checks</span><br><span class=\"line\">    # for Kafka and Confluent Schema Registry, but that&#39;s ok because they complete fast.</span><br><span class=\"line\">    # The reason we check for readiness here is that we can insert a sleep time</span><br><span class=\"line\">    # for topic creation before we start the application.</span><br><span class=\"line\">    command: &quot;bash -c &#39;echo Waiting for Kafka to be ready... &amp;&amp; \\</span><br><span class=\"line\">                       cub kafka-ready -b kafka:29092 1 20 &amp;&amp; \\</span><br><span class=\"line\">                       echo Waiting for Confluent Schema Registry to be ready... &amp;&amp; \\</span><br><span class=\"line\">                       cub sr-ready schema-registry 8081 20 &amp;&amp; \\</span><br><span class=\"line\">                       echo Waiting a few seconds for topic creation to finish... &amp;&amp; \\</span><br><span class=\"line\">                       sleep 2 &amp;&amp; \\</span><br><span class=\"line\">                       java -jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;ksql-examples&#x2F;ksql-examples-0.1-SNAPSHOT-standalone.jar</span><br><span class=\"line\">                       quickstart&#x3D;pageviews format&#x3D;delimited topic&#x3D;pageviews bootstrap-server&#x3D;kafka:29092 maxInterval&#x3D;100 iterations&#x3D;1000 &amp;&amp; \\</span><br><span class=\"line\">                       java -jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;ksql-examples&#x2F;ksql-examples-0.1-SNAPSHOT-standalone.jar</span><br><span class=\"line\">                       quickstart&#x3D;pageviews format&#x3D;delimited topic&#x3D;pageviews bootstrap-server&#x3D;kafka:29092 maxInterval&#x3D;1000&#39;&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      KSQL_CONFIG_DIR: &quot;&#x2F;etc&#x2F;ksql&quot;</span><br><span class=\"line\">      KSQL_LOG4J_OPTS: &quot;-Dlog4j.configuration&#x3D;file:&#x2F;etc&#x2F;ksql&#x2F;log4j-rolling.properties&quot;</span><br><span class=\"line\">      STREAMS_BOOTSTRAP_SERVERS: kafka:29092</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_PORT: 8081</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Runs the Kafka KSQL data generator for topic called &quot;users&quot;</span><br><span class=\"line\">  ksql-datagen-users:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;ksql-examples:latest&quot;</span><br><span class=\"line\">    hostname: ksql-datagen-users</span><br><span class=\"line\">    depends_on:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">      - schema-registry</span><br><span class=\"line\">    # Note: The container&#39;s &#96;run&#96; script will perform the same readiness checks</span><br><span class=\"line\">    # for Kafka and Confluent Schema Registry, but that&#39;s ok because they complete fast.</span><br><span class=\"line\">    # The reason we check for readiness here is that we can insert a sleep time</span><br><span class=\"line\">    # for topic creation before we start the application.</span><br><span class=\"line\">    command: &quot;bash -c &#39;echo Waiting for Kafka to be ready... &amp;&amp; \\</span><br><span class=\"line\">                       cub kafka-ready -b kafka:29092 1 20 &amp;&amp; \\</span><br><span class=\"line\">                       echo Waiting for Confluent Schema Registry to be ready... &amp;&amp; \\</span><br><span class=\"line\">                       cub sr-ready schema-registry 8081 20 &amp;&amp; \\</span><br><span class=\"line\">                       echo Waiting a few seconds for topic creation to finish... &amp;&amp; \\</span><br><span class=\"line\">                       sleep 2 &amp;&amp; \\</span><br><span class=\"line\">                       java -jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;ksql-examples&#x2F;ksql-examples-0.1-SNAPSHOT-standalone.jar</span><br><span class=\"line\">                       quickstart&#x3D;users format&#x3D;json topic&#x3D;users bootstrap-server&#x3D;kafka:29092 maxInterval&#x3D;100 iterations&#x3D;1000 &amp;&amp; \\</span><br><span class=\"line\">                       java -jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;ksql-examples&#x2F;ksql-examples-0.1-SNAPSHOT-standalone.jar</span><br><span class=\"line\">                       quickstart&#x3D;users format&#x3D;json topic&#x3D;users bootstrap-server&#x3D;kafka:29092 maxInterval&#x3D;1000&#39;&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      KSQL_CONFIG_DIR: &quot;&#x2F;etc&#x2F;ksql&quot;</span><br><span class=\"line\">      KSQL_LOG4J_OPTS: &quot;-Dlog4j.configuration&#x3D;file:&#x2F;etc&#x2F;ksql&#x2F;log4j-rolling.properties&quot;</span><br><span class=\"line\">      STREAMS_BOOTSTRAP_SERVERS: kafka:29092</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_PORT: 8081</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Runs the Kafka KSQL application</span><br><span class=\"line\">  ksql-cli:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;ksql-cli:latest&quot;</span><br><span class=\"line\">    hostname: ksql-cli</span><br><span class=\"line\">    depends_on:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">      - schema-registry</span><br><span class=\"line\">      - ksql-datagen-pageviews</span><br><span class=\"line\">      - ksql-datagen-users</span><br><span class=\"line\">    command: &quot;perl -e &#39;while(1)&#123; sleep 99999 &#125;&#39;&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      KSQL_CONFIG_DIR: &quot;&#x2F;etc&#x2F;ksql&quot;</span><br><span class=\"line\">      KSQL_LOG4J_OPTS: &quot;-Dlog4j.configuration&#x3D;file:&#x2F;etc&#x2F;ksql&#x2F;log4j-rolling.properties&quot;</span><br><span class=\"line\">      STREAMS_BOOTSTRAP_SERVERS: kafka:29092</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_PORT: 8081</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Prepare the enviroment with the following command</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker-compose up -d</span><br></pre></td></tr></table></figure>\n<p>This will create the minimal services to test ksql</p>\n<ul>\n<li>zookeeper</li>\n<li>schema-registry</li>\n<li>kafka</li>\n<li>ksql-cli</li>\n</ul>\n<p>It also will create 2 dockers which will generate data in order to test:</p>\n<ul>\n<li>ksql-datagen-pageviews</li>\n<li>ksql-datagen-users</li>\n</ul>\n<p>Thoose dockers are kafka producers, which we will use to test KSQL.</p>\n<h2 id=\"KSQL\"><a href=\"#KSQL\" class=\"headerlink\" title=\"KSQL\"></a>KSQL</h2><p>Execute the CLI in order to test</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker-compose exec ksql-cli ksql-cli local --bootstrap-server kafka:29092</span><br></pre></td></tr></table></figure>\n<p>You will see something like this</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">                       &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">                       &#x3D;      _  __ _____  ____  _          &#x3D;</span><br><span class=\"line\">                       &#x3D;     | |&#x2F; &#x2F;&#x2F; ____|&#x2F; __ \\| |         &#x3D;</span><br><span class=\"line\">                       &#x3D;     | &#39; &#x2F;| (___ | |  | | |         &#x3D;</span><br><span class=\"line\">                       &#x3D;     |  &lt;  \\___ \\| |  | | |         &#x3D;</span><br><span class=\"line\">                       &#x3D;     | . \\ ____) | |__| | |____     &#x3D;</span><br><span class=\"line\">                       &#x3D;     |_|\\_\\_____&#x2F; \\___\\_\\______|    &#x3D;</span><br><span class=\"line\">                       &#x3D;                                    &#x3D;</span><br><span class=\"line\">                       &#x3D;   Streaming SQL Engine for Kafka   &#x3D;</span><br><span class=\"line\">Copyright 2017 Confluent Inc.                         </span><br><span class=\"line\"></span><br><span class=\"line\">CLI v0.1, Server v0.1 located at http:&#x2F;&#x2F;localhost:9098</span><br><span class=\"line\"></span><br><span class=\"line\">Having trouble? Type &#39;help&#39; (case-insensitive) for a rundown of how things work!</span><br><span class=\"line\"></span><br><span class=\"line\">ksql&gt;</span><br></pre></td></tr></table></figure>\n\n<p>Now let’s create a new <strong>STREAM</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM pageviews_original (viewtime bigint, userid varchar, pageid varchar) WITH (kafka_topic&#x3D;&#39;pageviews&#39;, value_format&#x3D;&#39;DELIMITED&#39;);</span><br></pre></td></tr></table></figure>\n<p>Let’s describe the created object</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ksql&gt; DESCRIBE pageviews_original;</span><br><span class=\"line\"></span><br><span class=\"line\"> Field    | Type            </span><br><span class=\"line\">----------------------------</span><br><span class=\"line\"> ROWTIME  | BIGINT          </span><br><span class=\"line\"> ROWKEY   | VARCHAR(STRING) </span><br><span class=\"line\"> VIEWTIME | BIGINT          </span><br><span class=\"line\"> USERID   | VARCHAR(STRING) </span><br><span class=\"line\"> PAGEID   | VARCHAR(STRING) </span><br></pre></td></tr></table></figure>\n<p>Now lets create a users <strong>TABLE</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE users_original (registertime bigint, gender varchar, regionid varchar, userid varchar) WITH (kafka_topic&#x3D;&#39;users&#39;, value_format&#x3D;&#39;JSON&#39;);</span><br></pre></td></tr></table></figure>\n<p>And confirm the DDL</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ksql&gt; DESCRIBE users_original;</span><br><span class=\"line\"></span><br><span class=\"line\"> Field        | Type            </span><br><span class=\"line\">--------------------------------</span><br><span class=\"line\"> ROWTIME      | BIGINT          </span><br><span class=\"line\"> ROWKEY       | VARCHAR(STRING) </span><br><span class=\"line\"> REGISTERTIME | BIGINT          </span><br><span class=\"line\"> GENDER       | VARCHAR(STRING) </span><br><span class=\"line\"> REGIONID     | VARCHAR(STRING) </span><br><span class=\"line\"> USERID       | VARCHAR(STRING) </span><br></pre></td></tr></table></figure>\n<p>So there are two main objects <strong>TABLES</strong> and <strong>STREAMS</strong>. To see the existing ones, execute:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ksql&gt; SHOW STREAMS;</span><br><span class=\"line\"></span><br><span class=\"line\"> Stream Name        | Kafka Topic | Format    </span><br><span class=\"line\">----------------------------------------------</span><br><span class=\"line\"> PAGEVIEWS_ORIGINAL | pageviews   | DELIMITED </span><br></pre></td></tr></table></figure>\n<p>and </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ksql&gt; SHOW TABLES;</span><br><span class=\"line\"></span><br><span class=\"line\"> Table Name     | Kafka Topic | Format | Windowed </span><br><span class=\"line\">--------------------------------------------------</span><br><span class=\"line\"> USERS_ORIGINAL | users       | JSON   | false    </span><br></pre></td></tr></table></figure>\n<h3 id=\"Differences-between-STREAMS-and-TABLES\"><a href=\"#Differences-between-STREAMS-and-TABLES\" class=\"headerlink\" title=\"Differences between STREAMS and TABLES\"></a>Differences between STREAMS and TABLES</h3><p><strong>Stream</strong></p>\n<p>A stream is an unbounded sequence of structured data (“facts”). For example, we could have a stream of financial transactions such as “Alice sent $100 to Bob, then Charlie sent $50 to Bob”. Facts in a stream are immutable, which means new facts can be inserted to a stream, but existing facts can never be updated or deleted. Streams can be created from a Kafka topic or derived from existing streams and tables.</p>\n<p><strong>Table</strong></p>\n<p>A table is a view of a stream, or another table, and represents a collection of evolving facts. For example, we could have a table that contains the latest financial information such as “Bob’s current account balance is $150”. It is the equivalent of a traditional database table but enriched by streaming semantics such as windowing. Facts in a table are mutable, which means new facts can be inserted to the table, and existing facts can be updated or deleted. Tables can be created from a Kafka topic or derived from existing streams and tables.</p>\n<h3 id=\"Extending-the-Model\"><a href=\"#Extending-the-Model\" class=\"headerlink\" title=\"Extending the Model\"></a>Extending the Model</h3><p>Lets create a new pageviews stream with key <code>pageid</code> and joi it with table <code>users</code>.</p>\n<p>First create the new stream</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM pageviews \\</span><br><span class=\"line\">   (viewtime BIGINT, \\</span><br><span class=\"line\">    userid VARCHAR, \\</span><br><span class=\"line\">    pageid VARCHAR) \\</span><br><span class=\"line\">   WITH (kafka_topic&#x3D;&#39;pageviews&#39;, \\</span><br><span class=\"line\">         value_format&#x3D;&#39;DELIMITED&#39;, \\</span><br><span class=\"line\">         key&#x3D;&#39;pageid&#39;, \\</span><br><span class=\"line\">         timestamp&#x3D;&#39;viewtime&#39;);</span><br></pre></td></tr></table></figure>\n<p>And users table</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE users \\</span><br><span class=\"line\">   (registertime BIGINT, \\</span><br><span class=\"line\">    gender VARCHAR, \\</span><br><span class=\"line\">    regionid VARCHAR, \\</span><br><span class=\"line\">    userid VARCHAR, \\</span><br><span class=\"line\">    interests array&lt;VARCHAR&gt;, \\</span><br><span class=\"line\">    contact_info map&lt;VARCHAR, VARCHAR&gt;) \\</span><br><span class=\"line\">   WITH (kafka_topic&#x3D;&#39;users&#39;, \\</span><br><span class=\"line\">         value_format&#x3D;&#39;JSON&#39;);</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"Aggregations\"><a href=\"#Aggregations\" class=\"headerlink\" title=\"Aggregations\"></a>Aggregations</h4><p>Let’s test some aggregations</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT count(*),userid FROM pageviews GROUP BY userid;</span><br></pre></td></tr></table></figure>\n<p>As data arrives in stream this is constantly updating , but we can verify that the count values begins from the start of the query, this could be usefull to dump to another topic with transformed data.</p>\n<p>Let’s do that.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE user_counts AS select count(*),userid from pageviews group by userid;</span><br></pre></td></tr></table></figure>\n<p>We can use the statement <code>SHOW TOPICS</code> which is usefull and confirmed that the USER_COUNTS TOPICS is created.</p>\n<p>The following aggregations are available</p>\n<table>\n<thead>\n<tr>\n<th>Function</th>\n<th>Example</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>COUNT</td>\n<td>COUNT(col1)</td>\n<td>Count the number of rows</td>\n</tr>\n<tr>\n<td>MAX</td>\n<td>MAX(col1)</td>\n<td>Return the maximum value for a given column and window</td>\n</tr>\n<tr>\n<td>MIN</td>\n<td>MIN(col1)</td>\n<td>Return the minimum value for a given column and window</td>\n</tr>\n<tr>\n<td>SUM</td>\n<td>SUM(col1)</td>\n<td>Sums the column values</td>\n</tr>\n</tbody></table>\n<h2 id=\"Window\"><a href=\"#Window\" class=\"headerlink\" title=\"Window\"></a>Window</h2><p>The WINDOW clause lets you control how to group input records that have the same key into so-called windows for operations such as aggregations or joins. Windows are tracked per record key. KSQL supports the following WINDOW types:</p>\n<ul>\n<li>TUMBLING</li>\n<li>HOPPING</li>\n<li>SESSION</li>\n</ul>\n<h3 id=\"Window-Tumbling\"><a href=\"#Window-Tumbling\" class=\"headerlink\" title=\"Window Tumbling\"></a>Window Tumbling</h3><p>TUMBLING: Tumbling windows group input records into fixed-sized, non-overlapping windows based on the records’ timestamps. You must specify the window size for tumbling windows. Note: Tumbling windows are a special case of hopping windows where the window size is equal to the advance interval.</p>\n<p>Example:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT item_id, SUM(quantity)</span><br><span class=\"line\">  FROM orders</span><br><span class=\"line\">  WINDOW TUMBLING (SIZE 20 SECONDS)</span><br><span class=\"line\">  GROUP BY item_id;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Window-HOPPING\"><a href=\"#Window-HOPPING\" class=\"headerlink\" title=\"Window HOPPING\"></a>Window HOPPING</h3><p>HOPPING: Hopping windows group input records into fixed-sized, (possibly) overlapping windows based on the records’ timestamps. You must specify the window size and the advance interval for hopping windows.</p>\n<p>Example:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT item_id, SUM(quantity)</span><br><span class=\"line\">  FROM orders</span><br><span class=\"line\">  WINDOW HOPPING (SIZE 20 SECONDS, ADVANCE BY 5 SECONDS)</span><br><span class=\"line\">  GROUP BY item_id;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Window-SESSION\"><a href=\"#Window-SESSION\" class=\"headerlink\" title=\"Window SESSION\"></a>Window SESSION</h3><p>SESSION: Session windows group input records into so-called sessions. You must specify the session inactivity gap parameter for session windows. For example, imagine you set the inactivity gap to 5 minutes. If, for a given record key such as “alice”, no new input data arrives for more than 5 minutes, then the current session for “alice” is closed, and any newly arriving data for “alice” in the future will mark the beginning of a new session.</p>\n<p>Example:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT item_id, SUM(quantity)</span><br><span class=\"line\">  FROM orders</span><br><span class=\"line\">  WINDOW SESSION (20 SECONDS)</span><br><span class=\"line\">  GROUP BY item_id;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Transformations\"><a href=\"#Transformations\" class=\"headerlink\" title=\"Transformations\"></a>Transformations</h2><p>Let’s create a new stream with a column transformation</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM pageviews_transformed \\</span><br><span class=\"line\">  WITH (timestamp&#x3D;&#39;viewtime&#39;, \\</span><br><span class=\"line\">        partitions&#x3D;5, \\</span><br><span class=\"line\">        value_format&#x3D;&#39;JSON&#39;) AS \\</span><br><span class=\"line\">  SELECT viewtime, \\</span><br><span class=\"line\">         userid, \\</span><br><span class=\"line\">         pageid, \\</span><br><span class=\"line\">         TIMESTAMPTOSTRING(viewtime, &#39;yyyy-MM-dd HH:mm:ss.SSS&#39;) AS timestring \\</span><br><span class=\"line\">  FROM pageviews \\</span><br><span class=\"line\">  PARTITION BY userid;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Joining\"><a href=\"#Joining\" class=\"headerlink\" title=\"Joining\"></a>Joining</h2><p>And joining the STREAM with enriched data from TABLE</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM pageviews_enriched AS \\</span><br><span class=\"line\">   SELECT pv.viewtime, \\</span><br><span class=\"line\">          pv.userid AS userid, \\</span><br><span class=\"line\">          pv.pageid, \\</span><br><span class=\"line\">          pv.timestring, \\</span><br><span class=\"line\">          u.gender, \\</span><br><span class=\"line\">          u.regionid, \\</span><br><span class=\"line\">          u.interests, \\</span><br><span class=\"line\">          u.contact_info \\</span><br><span class=\"line\">   FROM pageviews_transformed pv \\</span><br><span class=\"line\">   LEFT JOIN users u ON pv.userid &#x3D; users.userid;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Use-Cases\"><a href=\"#Use-Cases\" class=\"headerlink\" title=\"Use Cases\"></a>Use Cases</h1><p>Common KSQL use cases are:</p>\n<ul>\n<li>Fraud detection - identify and act on out of the ordinary data to provide real-time awareness.</li>\n<li>Personalization - create real-time experiences and insight for end users driven by data.</li>\n<li>Notifications - build custom alerts and messages based on real-time data.</li>\n<li>Real-time Analytics - power real-time dashboards to understand what’s happening as it does.</li>\n<li>Sensor data and IoT - understand and deliver sensor data how and where it needs to be.</li>\n<li>Customer 360 - provide a clear, real-time understanding of your customers across every interaction.</li>\n</ul>\n<h3 id=\"Streaming-ETL\"><a href=\"#Streaming-ETL\" class=\"headerlink\" title=\"Streaming ETL\"></a>Streaming ETL</h3><p>KSQL makes it simple to transform data within the pipeline, readying messages to cleanly land in another system</p>\n<h3 id=\"Anomaly-Detection\"><a href=\"#Anomaly-Detection\" class=\"headerlink\" title=\"Anomaly Detection\"></a>Anomaly Detection</h3><p>KSQL is a good fit for identifying patterns or anomalies on real-time data. By processing the stream as data arrives you can identify and properly surface out of the ordinary events with millisecond latency.</p>\n<h3 id=\"Monitoring\"><a href=\"#Monitoring\" class=\"headerlink\" title=\"Monitoring\"></a>Monitoring</h3><p>Kafka’s ability to provide scalable ordered messages with stream processing make it a common solution for log data monitoring and alerting. KSQL lends a familiar syntax for tracking, understanding, and managing alerts.</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>On a first glance KSQL seems pretty easy to start using it. It’s still pretty new, i would give it a time to mature before start using it on production.</p>\n<p>For small transformations, data-quality control and analytical queries that don’t take into account a large windows this seems a good solution. For more complex queries i still prefer to keep Kafka to it’s core business and let the computation work to Spark.  </p>\n<h2 id=\"More-Information\"><a href=\"#More-Information\" class=\"headerlink\" title=\"More Information\"></a>More Information</h2><ul>\n<li>KSQL is available as a developer preview on <a href=\"http://go2.confluent.io/Q0KX0lH0Qq2kx0t3r00U8X0\">Github</a></li>\n<li>Read the <a href=\"http://go2.confluent.io/m000tV2080xr3kXlHKr0QX0\">documentation</a> for KSQL</li>\n<li>Join the conversation in the #ksql channel in the <a href=\"http://go2.confluent.io/m000tW2080xr3kXlHKs0QX0\">Confluent Community Slack</a>. </li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://github.com/confluentinc/ksql\">https://github.com/confluentinc/ksql</a></li>\n<li><a href=\"https://github.com/confluentinc/ksql/tree/0.1.x/docs/quickstart#quick-start\">https://github.com/confluentinc/ksql/tree/0.1.x/docs/quickstart#quick-start</a></li>\n<li><a href=\"https://github.com/confluentinc/ksql/tree/0.1.x/ksql-clickstream-demo#clickstream-analysis\">https://github.com/confluentinc/ksql/tree/0.1.x/ksql-clickstream-demo#clickstream-analysis</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>In this article i’ll explore Apache Kafka KSQL</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><ul>\n<li>Docker</li>\n<li>docker-compose</li>\n</ul>\n<h2 id=\"Description\"><a href=\"#Description\" class=\"headerlink\" title=\"Description\"></a>Description</h2><blockquote><p>KSQL is an open source streaming SQL engine that implements continuous, interactive queries against Apache Kafka™. It allows you to query, read, write, and process data in Apache Kafka in real-time, at scale using SQL commands. KSQL interacts directly with the Kafka Streams API, removing the requirement of building a Java app.</p>\n<footer><strong>Offical Website Definition</strong></footer></blockquote>\n\n<h2 id=\"PoC-Setup\"><a href=\"#PoC-Setup\" class=\"headerlink\" title=\"PoC Setup\"></a>PoC Setup</h2><p>Create the following file <code>docker-compose.yml</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br><span class=\"line\">44</span><br><span class=\"line\">45</span><br><span class=\"line\">46</span><br><span class=\"line\">47</span><br><span class=\"line\">48</span><br><span class=\"line\">49</span><br><span class=\"line\">50</span><br><span class=\"line\">51</span><br><span class=\"line\">52</span><br><span class=\"line\">53</span><br><span class=\"line\">54</span><br><span class=\"line\">55</span><br><span class=\"line\">56</span><br><span class=\"line\">57</span><br><span class=\"line\">58</span><br><span class=\"line\">59</span><br><span class=\"line\">60</span><br><span class=\"line\">61</span><br><span class=\"line\">62</span><br><span class=\"line\">63</span><br><span class=\"line\">64</span><br><span class=\"line\">65</span><br><span class=\"line\">66</span><br><span class=\"line\">67</span><br><span class=\"line\">68</span><br><span class=\"line\">69</span><br><span class=\"line\">70</span><br><span class=\"line\">71</span><br><span class=\"line\">72</span><br><span class=\"line\">73</span><br><span class=\"line\">74</span><br><span class=\"line\">75</span><br><span class=\"line\">76</span><br><span class=\"line\">77</span><br><span class=\"line\">78</span><br><span class=\"line\">79</span><br><span class=\"line\">80</span><br><span class=\"line\">81</span><br><span class=\"line\">82</span><br><span class=\"line\">83</span><br><span class=\"line\">84</span><br><span class=\"line\">85</span><br><span class=\"line\">86</span><br><span class=\"line\">87</span><br><span class=\"line\">88</span><br><span class=\"line\">89</span><br><span class=\"line\">90</span><br><span class=\"line\">91</span><br><span class=\"line\">92</span><br><span class=\"line\">93</span><br><span class=\"line\">94</span><br><span class=\"line\">95</span><br><span class=\"line\">96</span><br><span class=\"line\">97</span><br><span class=\"line\">98</span><br><span class=\"line\">99</span><br><span class=\"line\">100</span><br><span class=\"line\">101</span><br><span class=\"line\">102</span><br><span class=\"line\">103</span><br><span class=\"line\">104</span><br><span class=\"line\">105</span><br><span class=\"line\">106</span><br><span class=\"line\">107</span><br><span class=\"line\">108</span><br><span class=\"line\">109</span><br><span class=\"line\">110</span><br><span class=\"line\">111</span><br><span class=\"line\">112</span><br><span class=\"line\">113</span><br><span class=\"line\">114</span><br><span class=\"line\">115</span><br><span class=\"line\">116</span><br><span class=\"line\">117</span><br><span class=\"line\">118</span><br><span class=\"line\">119</span><br><span class=\"line\">120</span><br><span class=\"line\">121</span><br><span class=\"line\">122</span><br><span class=\"line\">123</span><br><span class=\"line\">124</span><br><span class=\"line\">125</span><br><span class=\"line\">126</span><br><span class=\"line\">127</span><br><span class=\"line\">128</span><br><span class=\"line\">129</span><br><span class=\"line\">130</span><br><span class=\"line\">131</span><br><span class=\"line\">132</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">version: &#39;2&#39;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  zookeeper:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;cp-zookeeper:latest&quot;</span><br><span class=\"line\">    hostname: zookeeper</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &#39;32181:32181&#39;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      ZOOKEEPER_CLIENT_PORT: 32181</span><br><span class=\"line\">      ZOOKEEPER_TICK_TIME: 2000</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  kafka:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;cp-enterprise-kafka:latest&quot;</span><br><span class=\"line\">    hostname: kafka</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &#39;9092:9092&#39;</span><br><span class=\"line\">      - &#39;29092:29092&#39;</span><br><span class=\"line\">    depends_on:</span><br><span class=\"line\">      - zookeeper</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      KAFKA_BROKER_ID: 1</span><br><span class=\"line\">      KAFKA_ZOOKEEPER_CONNECT: zookeeper:32181</span><br><span class=\"line\">      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT</span><br><span class=\"line\">      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT</span><br><span class=\"line\">      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT:&#x2F;&#x2F;kafka:29092,PLAINTEXT_HOST:&#x2F;&#x2F;localhost:9092</span><br><span class=\"line\">      KAFKA_AUTO_CREATE_TOPICS_ENABLE: &quot;true&quot;</span><br><span class=\"line\">      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter</span><br><span class=\"line\">      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1</span><br><span class=\"line\">      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka:29092</span><br><span class=\"line\">      CONFLUENT_METRICS_REPORTER_ZOOKEEPER_CONNECT: zookeeper:32181</span><br><span class=\"line\">      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1</span><br><span class=\"line\">      CONFLUENT_METRICS_ENABLE: &#39;true&#39;</span><br><span class=\"line\">      CONFLUENT_SUPPORT_CUSTOMER_ID: &#39;anonymous&#39;</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  schema-registry:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;cp-schema-registry:latest&quot;</span><br><span class=\"line\">    hostname: schema-registry</span><br><span class=\"line\">    depends_on:</span><br><span class=\"line\">      - zookeeper</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">      - &#39;8081:8081&#39;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      SCHEMA_REGISTRY_HOST_NAME: schema-registry</span><br><span class=\"line\">      SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: zookeeper:32181</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Runs the Kafka KSQL data generator for topic called &quot;pageviews&quot;</span><br><span class=\"line\">  ksql-datagen-pageviews:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;ksql-examples:latest&quot;</span><br><span class=\"line\">    hostname: ksql-datagen-pageviews</span><br><span class=\"line\">    depends_on:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">      - schema-registry</span><br><span class=\"line\">    # Note: The container&#39;s &#96;run&#96; script will perform the same readiness checks</span><br><span class=\"line\">    # for Kafka and Confluent Schema Registry, but that&#39;s ok because they complete fast.</span><br><span class=\"line\">    # The reason we check for readiness here is that we can insert a sleep time</span><br><span class=\"line\">    # for topic creation before we start the application.</span><br><span class=\"line\">    command: &quot;bash -c &#39;echo Waiting for Kafka to be ready... &amp;&amp; \\</span><br><span class=\"line\">                       cub kafka-ready -b kafka:29092 1 20 &amp;&amp; \\</span><br><span class=\"line\">                       echo Waiting for Confluent Schema Registry to be ready... &amp;&amp; \\</span><br><span class=\"line\">                       cub sr-ready schema-registry 8081 20 &amp;&amp; \\</span><br><span class=\"line\">                       echo Waiting a few seconds for topic creation to finish... &amp;&amp; \\</span><br><span class=\"line\">                       sleep 2 &amp;&amp; \\</span><br><span class=\"line\">                       java -jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;ksql-examples&#x2F;ksql-examples-0.1-SNAPSHOT-standalone.jar</span><br><span class=\"line\">                       quickstart&#x3D;pageviews format&#x3D;delimited topic&#x3D;pageviews bootstrap-server&#x3D;kafka:29092 maxInterval&#x3D;100 iterations&#x3D;1000 &amp;&amp; \\</span><br><span class=\"line\">                       java -jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;ksql-examples&#x2F;ksql-examples-0.1-SNAPSHOT-standalone.jar</span><br><span class=\"line\">                       quickstart&#x3D;pageviews format&#x3D;delimited topic&#x3D;pageviews bootstrap-server&#x3D;kafka:29092 maxInterval&#x3D;1000&#39;&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      KSQL_CONFIG_DIR: &quot;&#x2F;etc&#x2F;ksql&quot;</span><br><span class=\"line\">      KSQL_LOG4J_OPTS: &quot;-Dlog4j.configuration&#x3D;file:&#x2F;etc&#x2F;ksql&#x2F;log4j-rolling.properties&quot;</span><br><span class=\"line\">      STREAMS_BOOTSTRAP_SERVERS: kafka:29092</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_PORT: 8081</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Runs the Kafka KSQL data generator for topic called &quot;users&quot;</span><br><span class=\"line\">  ksql-datagen-users:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;ksql-examples:latest&quot;</span><br><span class=\"line\">    hostname: ksql-datagen-users</span><br><span class=\"line\">    depends_on:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">      - schema-registry</span><br><span class=\"line\">    # Note: The container&#39;s &#96;run&#96; script will perform the same readiness checks</span><br><span class=\"line\">    # for Kafka and Confluent Schema Registry, but that&#39;s ok because they complete fast.</span><br><span class=\"line\">    # The reason we check for readiness here is that we can insert a sleep time</span><br><span class=\"line\">    # for topic creation before we start the application.</span><br><span class=\"line\">    command: &quot;bash -c &#39;echo Waiting for Kafka to be ready... &amp;&amp; \\</span><br><span class=\"line\">                       cub kafka-ready -b kafka:29092 1 20 &amp;&amp; \\</span><br><span class=\"line\">                       echo Waiting for Confluent Schema Registry to be ready... &amp;&amp; \\</span><br><span class=\"line\">                       cub sr-ready schema-registry 8081 20 &amp;&amp; \\</span><br><span class=\"line\">                       echo Waiting a few seconds for topic creation to finish... &amp;&amp; \\</span><br><span class=\"line\">                       sleep 2 &amp;&amp; \\</span><br><span class=\"line\">                       java -jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;ksql-examples&#x2F;ksql-examples-0.1-SNAPSHOT-standalone.jar</span><br><span class=\"line\">                       quickstart&#x3D;users format&#x3D;json topic&#x3D;users bootstrap-server&#x3D;kafka:29092 maxInterval&#x3D;100 iterations&#x3D;1000 &amp;&amp; \\</span><br><span class=\"line\">                       java -jar &#x2F;usr&#x2F;share&#x2F;java&#x2F;ksql-examples&#x2F;ksql-examples-0.1-SNAPSHOT-standalone.jar</span><br><span class=\"line\">                       quickstart&#x3D;users format&#x3D;json topic&#x3D;users bootstrap-server&#x3D;kafka:29092 maxInterval&#x3D;1000&#39;&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      KSQL_CONFIG_DIR: &quot;&#x2F;etc&#x2F;ksql&quot;</span><br><span class=\"line\">      KSQL_LOG4J_OPTS: &quot;-Dlog4j.configuration&#x3D;file:&#x2F;etc&#x2F;ksql&#x2F;log4j-rolling.properties&quot;</span><br><span class=\"line\">      STREAMS_BOOTSTRAP_SERVERS: kafka:29092</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_PORT: 8081</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  # Runs the Kafka KSQL application</span><br><span class=\"line\">  ksql-cli:</span><br><span class=\"line\">    image: &quot;confluentinc&#x2F;ksql-cli:latest&quot;</span><br><span class=\"line\">    hostname: ksql-cli</span><br><span class=\"line\">    depends_on:</span><br><span class=\"line\">      - kafka</span><br><span class=\"line\">      - schema-registry</span><br><span class=\"line\">      - ksql-datagen-pageviews</span><br><span class=\"line\">      - ksql-datagen-users</span><br><span class=\"line\">    command: &quot;perl -e &#39;while(1)&#123; sleep 99999 &#125;&#39;&quot;</span><br><span class=\"line\">    environment:</span><br><span class=\"line\">      KSQL_CONFIG_DIR: &quot;&#x2F;etc&#x2F;ksql&quot;</span><br><span class=\"line\">      KSQL_LOG4J_OPTS: &quot;-Dlog4j.configuration&#x3D;file:&#x2F;etc&#x2F;ksql&#x2F;log4j-rolling.properties&quot;</span><br><span class=\"line\">      STREAMS_BOOTSTRAP_SERVERS: kafka:29092</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_HOST: schema-registry</span><br><span class=\"line\">      STREAMS_SCHEMA_REGISTRY_PORT: 8081</span><br><span class=\"line\">    extra_hosts:</span><br><span class=\"line\">      - &quot;moby:127.0.0.1&quot;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>Prepare the enviroment with the following command</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker-compose up -d</span><br></pre></td></tr></table></figure>\n<p>This will create the minimal services to test ksql</p>\n<ul>\n<li>zookeeper</li>\n<li>schema-registry</li>\n<li>kafka</li>\n<li>ksql-cli</li>\n</ul>\n<p>It also will create 2 dockers which will generate data in order to test:</p>\n<ul>\n<li>ksql-datagen-pageviews</li>\n<li>ksql-datagen-users</li>\n</ul>\n<p>Thoose dockers are kafka producers, which we will use to test KSQL.</p>\n<h2 id=\"KSQL\"><a href=\"#KSQL\" class=\"headerlink\" title=\"KSQL\"></a>KSQL</h2><p>Execute the CLI in order to test</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker-compose exec ksql-cli ksql-cli local --bootstrap-server kafka:29092</span><br></pre></td></tr></table></figure>\n<p>You will see something like this</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">                       &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class=\"line\">                       &#x3D;      _  __ _____  ____  _          &#x3D;</span><br><span class=\"line\">                       &#x3D;     | |&#x2F; &#x2F;&#x2F; ____|&#x2F; __ \\| |         &#x3D;</span><br><span class=\"line\">                       &#x3D;     | &#39; &#x2F;| (___ | |  | | |         &#x3D;</span><br><span class=\"line\">                       &#x3D;     |  &lt;  \\___ \\| |  | | |         &#x3D;</span><br><span class=\"line\">                       &#x3D;     | . \\ ____) | |__| | |____     &#x3D;</span><br><span class=\"line\">                       &#x3D;     |_|\\_\\_____&#x2F; \\___\\_\\______|    &#x3D;</span><br><span class=\"line\">                       &#x3D;                                    &#x3D;</span><br><span class=\"line\">                       &#x3D;   Streaming SQL Engine for Kafka   &#x3D;</span><br><span class=\"line\">Copyright 2017 Confluent Inc.                         </span><br><span class=\"line\"></span><br><span class=\"line\">CLI v0.1, Server v0.1 located at http:&#x2F;&#x2F;localhost:9098</span><br><span class=\"line\"></span><br><span class=\"line\">Having trouble? Type &#39;help&#39; (case-insensitive) for a rundown of how things work!</span><br><span class=\"line\"></span><br><span class=\"line\">ksql&gt;</span><br></pre></td></tr></table></figure>\n\n<p>Now let’s create a new <strong>STREAM</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM pageviews_original (viewtime bigint, userid varchar, pageid varchar) WITH (kafka_topic&#x3D;&#39;pageviews&#39;, value_format&#x3D;&#39;DELIMITED&#39;);</span><br></pre></td></tr></table></figure>\n<p>Let’s describe the created object</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ksql&gt; DESCRIBE pageviews_original;</span><br><span class=\"line\"></span><br><span class=\"line\"> Field    | Type            </span><br><span class=\"line\">----------------------------</span><br><span class=\"line\"> ROWTIME  | BIGINT          </span><br><span class=\"line\"> ROWKEY   | VARCHAR(STRING) </span><br><span class=\"line\"> VIEWTIME | BIGINT          </span><br><span class=\"line\"> USERID   | VARCHAR(STRING) </span><br><span class=\"line\"> PAGEID   | VARCHAR(STRING) </span><br></pre></td></tr></table></figure>\n<p>Now lets create a users <strong>TABLE</strong></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE users_original (registertime bigint, gender varchar, regionid varchar, userid varchar) WITH (kafka_topic&#x3D;&#39;users&#39;, value_format&#x3D;&#39;JSON&#39;);</span><br></pre></td></tr></table></figure>\n<p>And confirm the DDL</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ksql&gt; DESCRIBE users_original;</span><br><span class=\"line\"></span><br><span class=\"line\"> Field        | Type            </span><br><span class=\"line\">--------------------------------</span><br><span class=\"line\"> ROWTIME      | BIGINT          </span><br><span class=\"line\"> ROWKEY       | VARCHAR(STRING) </span><br><span class=\"line\"> REGISTERTIME | BIGINT          </span><br><span class=\"line\"> GENDER       | VARCHAR(STRING) </span><br><span class=\"line\"> REGIONID     | VARCHAR(STRING) </span><br><span class=\"line\"> USERID       | VARCHAR(STRING) </span><br></pre></td></tr></table></figure>\n<p>So there are two main objects <strong>TABLES</strong> and <strong>STREAMS</strong>. To see the existing ones, execute:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ksql&gt; SHOW STREAMS;</span><br><span class=\"line\"></span><br><span class=\"line\"> Stream Name        | Kafka Topic | Format    </span><br><span class=\"line\">----------------------------------------------</span><br><span class=\"line\"> PAGEVIEWS_ORIGINAL | pageviews   | DELIMITED </span><br></pre></td></tr></table></figure>\n<p>and </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">ksql&gt; SHOW TABLES;</span><br><span class=\"line\"></span><br><span class=\"line\"> Table Name     | Kafka Topic | Format | Windowed </span><br><span class=\"line\">--------------------------------------------------</span><br><span class=\"line\"> USERS_ORIGINAL | users       | JSON   | false    </span><br></pre></td></tr></table></figure>\n<h3 id=\"Differences-between-STREAMS-and-TABLES\"><a href=\"#Differences-between-STREAMS-and-TABLES\" class=\"headerlink\" title=\"Differences between STREAMS and TABLES\"></a>Differences between STREAMS and TABLES</h3><p><strong>Stream</strong></p>\n<p>A stream is an unbounded sequence of structured data (“facts”). For example, we could have a stream of financial transactions such as “Alice sent $100 to Bob, then Charlie sent $50 to Bob”. Facts in a stream are immutable, which means new facts can be inserted to a stream, but existing facts can never be updated or deleted. Streams can be created from a Kafka topic or derived from existing streams and tables.</p>\n<p><strong>Table</strong></p>\n<p>A table is a view of a stream, or another table, and represents a collection of evolving facts. For example, we could have a table that contains the latest financial information such as “Bob’s current account balance is $150”. It is the equivalent of a traditional database table but enriched by streaming semantics such as windowing. Facts in a table are mutable, which means new facts can be inserted to the table, and existing facts can be updated or deleted. Tables can be created from a Kafka topic or derived from existing streams and tables.</p>\n<h3 id=\"Extending-the-Model\"><a href=\"#Extending-the-Model\" class=\"headerlink\" title=\"Extending the Model\"></a>Extending the Model</h3><p>Lets create a new pageviews stream with key <code>pageid</code> and joi it with table <code>users</code>.</p>\n<p>First create the new stream</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM pageviews \\</span><br><span class=\"line\">   (viewtime BIGINT, \\</span><br><span class=\"line\">    userid VARCHAR, \\</span><br><span class=\"line\">    pageid VARCHAR) \\</span><br><span class=\"line\">   WITH (kafka_topic&#x3D;&#39;pageviews&#39;, \\</span><br><span class=\"line\">         value_format&#x3D;&#39;DELIMITED&#39;, \\</span><br><span class=\"line\">         key&#x3D;&#39;pageid&#39;, \\</span><br><span class=\"line\">         timestamp&#x3D;&#39;viewtime&#39;);</span><br></pre></td></tr></table></figure>\n<p>And users table</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE users \\</span><br><span class=\"line\">   (registertime BIGINT, \\</span><br><span class=\"line\">    gender VARCHAR, \\</span><br><span class=\"line\">    regionid VARCHAR, \\</span><br><span class=\"line\">    userid VARCHAR, \\</span><br><span class=\"line\">    interests array&lt;VARCHAR&gt;, \\</span><br><span class=\"line\">    contact_info map&lt;VARCHAR, VARCHAR&gt;) \\</span><br><span class=\"line\">   WITH (kafka_topic&#x3D;&#39;users&#39;, \\</span><br><span class=\"line\">         value_format&#x3D;&#39;JSON&#39;);</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<h4 id=\"Aggregations\"><a href=\"#Aggregations\" class=\"headerlink\" title=\"Aggregations\"></a>Aggregations</h4><p>Let’s test some aggregations</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT count(*),userid FROM pageviews GROUP BY userid;</span><br></pre></td></tr></table></figure>\n<p>As data arrives in stream this is constantly updating , but we can verify that the count values begins from the start of the query, this could be usefull to dump to another topic with transformed data.</p>\n<p>Let’s do that.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE TABLE user_counts AS select count(*),userid from pageviews group by userid;</span><br></pre></td></tr></table></figure>\n<p>We can use the statement <code>SHOW TOPICS</code> which is usefull and confirmed that the USER_COUNTS TOPICS is created.</p>\n<p>The following aggregations are available</p>\n<table>\n<thead>\n<tr>\n<th>Function</th>\n<th>Example</th>\n<th>Description</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>COUNT</td>\n<td>COUNT(col1)</td>\n<td>Count the number of rows</td>\n</tr>\n<tr>\n<td>MAX</td>\n<td>MAX(col1)</td>\n<td>Return the maximum value for a given column and window</td>\n</tr>\n<tr>\n<td>MIN</td>\n<td>MIN(col1)</td>\n<td>Return the minimum value for a given column and window</td>\n</tr>\n<tr>\n<td>SUM</td>\n<td>SUM(col1)</td>\n<td>Sums the column values</td>\n</tr>\n</tbody></table>\n<h2 id=\"Window\"><a href=\"#Window\" class=\"headerlink\" title=\"Window\"></a>Window</h2><p>The WINDOW clause lets you control how to group input records that have the same key into so-called windows for operations such as aggregations or joins. Windows are tracked per record key. KSQL supports the following WINDOW types:</p>\n<ul>\n<li>TUMBLING</li>\n<li>HOPPING</li>\n<li>SESSION</li>\n</ul>\n<h3 id=\"Window-Tumbling\"><a href=\"#Window-Tumbling\" class=\"headerlink\" title=\"Window Tumbling\"></a>Window Tumbling</h3><p>TUMBLING: Tumbling windows group input records into fixed-sized, non-overlapping windows based on the records’ timestamps. You must specify the window size for tumbling windows. Note: Tumbling windows are a special case of hopping windows where the window size is equal to the advance interval.</p>\n<p>Example:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT item_id, SUM(quantity)</span><br><span class=\"line\">  FROM orders</span><br><span class=\"line\">  WINDOW TUMBLING (SIZE 20 SECONDS)</span><br><span class=\"line\">  GROUP BY item_id;</span><br></pre></td></tr></table></figure>\n\n<h3 id=\"Window-HOPPING\"><a href=\"#Window-HOPPING\" class=\"headerlink\" title=\"Window HOPPING\"></a>Window HOPPING</h3><p>HOPPING: Hopping windows group input records into fixed-sized, (possibly) overlapping windows based on the records’ timestamps. You must specify the window size and the advance interval for hopping windows.</p>\n<p>Example:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT item_id, SUM(quantity)</span><br><span class=\"line\">  FROM orders</span><br><span class=\"line\">  WINDOW HOPPING (SIZE 20 SECONDS, ADVANCE BY 5 SECONDS)</span><br><span class=\"line\">  GROUP BY item_id;</span><br></pre></td></tr></table></figure>\n<h3 id=\"Window-SESSION\"><a href=\"#Window-SESSION\" class=\"headerlink\" title=\"Window SESSION\"></a>Window SESSION</h3><p>SESSION: Session windows group input records into so-called sessions. You must specify the session inactivity gap parameter for session windows. For example, imagine you set the inactivity gap to 5 minutes. If, for a given record key such as “alice”, no new input data arrives for more than 5 minutes, then the current session for “alice” is closed, and any newly arriving data for “alice” in the future will mark the beginning of a new session.</p>\n<p>Example:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">SELECT item_id, SUM(quantity)</span><br><span class=\"line\">  FROM orders</span><br><span class=\"line\">  WINDOW SESSION (20 SECONDS)</span><br><span class=\"line\">  GROUP BY item_id;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Transformations\"><a href=\"#Transformations\" class=\"headerlink\" title=\"Transformations\"></a>Transformations</h2><p>Let’s create a new stream with a column transformation</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM pageviews_transformed \\</span><br><span class=\"line\">  WITH (timestamp&#x3D;&#39;viewtime&#39;, \\</span><br><span class=\"line\">        partitions&#x3D;5, \\</span><br><span class=\"line\">        value_format&#x3D;&#39;JSON&#39;) AS \\</span><br><span class=\"line\">  SELECT viewtime, \\</span><br><span class=\"line\">         userid, \\</span><br><span class=\"line\">         pageid, \\</span><br><span class=\"line\">         TIMESTAMPTOSTRING(viewtime, &#39;yyyy-MM-dd HH:mm:ss.SSS&#39;) AS timestring \\</span><br><span class=\"line\">  FROM pageviews \\</span><br><span class=\"line\">  PARTITION BY userid;</span><br></pre></td></tr></table></figure>\n<h2 id=\"Joining\"><a href=\"#Joining\" class=\"headerlink\" title=\"Joining\"></a>Joining</h2><p>And joining the STREAM with enriched data from TABLE</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">CREATE STREAM pageviews_enriched AS \\</span><br><span class=\"line\">   SELECT pv.viewtime, \\</span><br><span class=\"line\">          pv.userid AS userid, \\</span><br><span class=\"line\">          pv.pageid, \\</span><br><span class=\"line\">          pv.timestring, \\</span><br><span class=\"line\">          u.gender, \\</span><br><span class=\"line\">          u.regionid, \\</span><br><span class=\"line\">          u.interests, \\</span><br><span class=\"line\">          u.contact_info \\</span><br><span class=\"line\">   FROM pageviews_transformed pv \\</span><br><span class=\"line\">   LEFT JOIN users u ON pv.userid &#x3D; users.userid;</span><br></pre></td></tr></table></figure>\n<h1 id=\"Use-Cases\"><a href=\"#Use-Cases\" class=\"headerlink\" title=\"Use Cases\"></a>Use Cases</h1><p>Common KSQL use cases are:</p>\n<ul>\n<li>Fraud detection - identify and act on out of the ordinary data to provide real-time awareness.</li>\n<li>Personalization - create real-time experiences and insight for end users driven by data.</li>\n<li>Notifications - build custom alerts and messages based on real-time data.</li>\n<li>Real-time Analytics - power real-time dashboards to understand what’s happening as it does.</li>\n<li>Sensor data and IoT - understand and deliver sensor data how and where it needs to be.</li>\n<li>Customer 360 - provide a clear, real-time understanding of your customers across every interaction.</li>\n</ul>\n<h3 id=\"Streaming-ETL\"><a href=\"#Streaming-ETL\" class=\"headerlink\" title=\"Streaming ETL\"></a>Streaming ETL</h3><p>KSQL makes it simple to transform data within the pipeline, readying messages to cleanly land in another system</p>\n<h3 id=\"Anomaly-Detection\"><a href=\"#Anomaly-Detection\" class=\"headerlink\" title=\"Anomaly Detection\"></a>Anomaly Detection</h3><p>KSQL is a good fit for identifying patterns or anomalies on real-time data. By processing the stream as data arrives you can identify and properly surface out of the ordinary events with millisecond latency.</p>\n<h3 id=\"Monitoring\"><a href=\"#Monitoring\" class=\"headerlink\" title=\"Monitoring\"></a>Monitoring</h3><p>Kafka’s ability to provide scalable ordered messages with stream processing make it a common solution for log data monitoring and alerting. KSQL lends a familiar syntax for tracking, understanding, and managing alerts.</p>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>On a first glance KSQL seems pretty easy to start using it. It’s still pretty new, i would give it a time to mature before start using it on production.</p>\n<p>For small transformations, data-quality control and analytical queries that don’t take into account a large windows this seems a good solution. For more complex queries i still prefer to keep Kafka to it’s core business and let the computation work to Spark.  </p>\n<h2 id=\"More-Information\"><a href=\"#More-Information\" class=\"headerlink\" title=\"More Information\"></a>More Information</h2><ul>\n<li>KSQL is available as a developer preview on <a href=\"http://go2.confluent.io/Q0KX0lH0Qq2kx0t3r00U8X0\">Github</a></li>\n<li>Read the <a href=\"http://go2.confluent.io/m000tV2080xr3kXlHKr0QX0\">documentation</a> for KSQL</li>\n<li>Join the conversation in the #ksql channel in the <a href=\"http://go2.confluent.io/m000tW2080xr3kXlHKs0QX0\">Confluent Community Slack</a>. </li>\n</ul>\n<h2 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h2><ul>\n<li><a href=\"https://github.com/confluentinc/ksql\">https://github.com/confluentinc/ksql</a></li>\n<li><a href=\"https://github.com/confluentinc/ksql/tree/0.1.x/docs/quickstart#quick-start\">https://github.com/confluentinc/ksql/tree/0.1.x/docs/quickstart#quick-start</a></li>\n<li><a href=\"https://github.com/confluentinc/ksql/tree/0.1.x/ksql-clickstream-demo#clickstream-analysis\">https://github.com/confluentinc/ksql/tree/0.1.x/ksql-clickstream-demo#clickstream-analysis</a></li>\n</ul>\n"},{"title":"Terminal Markdown Viewer and MarkDown SpellChecker","date":"2017-09-07T01:00:16.000Z","_content":"\nIt's been a while that i've updated this posts. So let me start for something i've found out the other day.\n\n## Terminal Markdown Viewer\n\nI though it would be cool to have some tool that could parse me Markdown files and present them nicely formatted, like `cat` command.\n\nAnd it seems i'm not the only one :)\n\nhttps://github.com/axiros/terminal_markdown_viewer\n\nIn order to install just follow the instruction on the site. Pip worked well for me:\n\n```\npip install mdv\n```\n\nI've updated my PATH environment var on `.bashrc` with:\n\n```\nexport PYTHONPATH=\"${PYTHONPATH}:/${HOME}/.local/lib/python2.7/site-packages\"\nexport PATH=\"$PATH:~/.local/bin\"\n```\n\nI can now execute `mdv` command try it out it some .md files :D\n\n## A Markdown spell checking tool.\n\nThe other thing is regarding spell-check, as you can probably notice i really do a lot of errors, but i don't wont to use a high resource editor just because of this how about a MD spell checking tool ?\n\nWell i'm trying this one ad i'm using [hexo][http://hexo.io] for the page generation and it uses already npm modules just added a new one \n\n### install\n\n```\nnpm i markdown-spellcheck -g\n```\n\nOne can run interactive like\n\n```\nmdspell \"**/*.md\"\n```\n\nIt's a bit slow but the database would grow in terms, i'm giving a try to it.\n\nI also add to add `.spelling` no to keep the dictionary database on git. I probably would add it later.\n\nCheers,\nRR","source":"_posts/mkv.md","raw":"---\ntitle: Terminal Markdown Viewer and MarkDown SpellChecker\ndate: 2017-09-07 02:00:16\ntags:\n  - Markdown\n  - Tools\n  - Utils\n  - Linux\n---\n\nIt's been a while that i've updated this posts. So let me start for something i've found out the other day.\n\n## Terminal Markdown Viewer\n\nI though it would be cool to have some tool that could parse me Markdown files and present them nicely formatted, like `cat` command.\n\nAnd it seems i'm not the only one :)\n\nhttps://github.com/axiros/terminal_markdown_viewer\n\nIn order to install just follow the instruction on the site. Pip worked well for me:\n\n```\npip install mdv\n```\n\nI've updated my PATH environment var on `.bashrc` with:\n\n```\nexport PYTHONPATH=\"${PYTHONPATH}:/${HOME}/.local/lib/python2.7/site-packages\"\nexport PATH=\"$PATH:~/.local/bin\"\n```\n\nI can now execute `mdv` command try it out it some .md files :D\n\n## A Markdown spell checking tool.\n\nThe other thing is regarding spell-check, as you can probably notice i really do a lot of errors, but i don't wont to use a high resource editor just because of this how about a MD spell checking tool ?\n\nWell i'm trying this one ad i'm using [hexo][http://hexo.io] for the page generation and it uses already npm modules just added a new one \n\n### install\n\n```\nnpm i markdown-spellcheck -g\n```\n\nOne can run interactive like\n\n```\nmdspell \"**/*.md\"\n```\n\nIt's a bit slow but the database would grow in terms, i'm giving a try to it.\n\nI also add to add `.spelling` no to keep the dictionary database on git. I probably would add it later.\n\nCheers,\nRR","slug":"mkv","published":1,"updated":"2020-09-06T14:38:37.855Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjq000jchpfbzixbjxq","content":"<p>It’s been a while that i’ve updated this posts. So let me start for something i’ve found out the other day.</p>\n<h2 id=\"Terminal-Markdown-Viewer\"><a href=\"#Terminal-Markdown-Viewer\" class=\"headerlink\" title=\"Terminal Markdown Viewer\"></a>Terminal Markdown Viewer</h2><p>I though it would be cool to have some tool that could parse me Markdown files and present them nicely formatted, like <code>cat</code> command.</p>\n<p>And it seems i’m not the only one :)</p>\n<p><a href=\"https://github.com/axiros/terminal_markdown_viewer\">https://github.com/axiros/terminal_markdown_viewer</a></p>\n<p>In order to install just follow the instruction on the site. Pip worked well for me:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install mdv</span><br></pre></td></tr></table></figure>\n<p>I’ve updated my PATH environment var on <code>.bashrc</code> with:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export PYTHONPATH&#x3D;&quot;$&#123;PYTHONPATH&#125;:&#x2F;$&#123;HOME&#125;&#x2F;.local&#x2F;lib&#x2F;python2.7&#x2F;site-packages&quot;</span><br><span class=\"line\">export PATH&#x3D;&quot;$PATH:~&#x2F;.local&#x2F;bin&quot;</span><br></pre></td></tr></table></figure>\n<p>I can now execute <code>mdv</code> command try it out it some .md files :D</p>\n<h2 id=\"A-Markdown-spell-checking-tool\"><a href=\"#A-Markdown-spell-checking-tool\" class=\"headerlink\" title=\"A Markdown spell checking tool.\"></a>A Markdown spell checking tool.</h2><p>The other thing is regarding spell-check, as you can probably notice i really do a lot of errors, but i don’t wont to use a high resource editor just because of this how about a MD spell checking tool ?</p>\n<p>Well i’m trying this one ad i’m using [hexo][<a href=\"http://hexo.io]\">http://hexo.io]</a> for the page generation and it uses already npm modules just added a new one </p>\n<h3 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm i markdown-spellcheck -g</span><br></pre></td></tr></table></figure>\n<p>One can run interactive like</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mdspell &quot;**&#x2F;*.md&quot;</span><br></pre></td></tr></table></figure>\n<p>It’s a bit slow but the database would grow in terms, i’m giving a try to it.</p>\n<p>I also add to add <code>.spelling</code> no to keep the dictionary database on git. I probably would add it later.</p>\n<p>Cheers,<br>RR</p>\n","site":{"data":{}},"excerpt":"","more":"<p>It’s been a while that i’ve updated this posts. So let me start for something i’ve found out the other day.</p>\n<h2 id=\"Terminal-Markdown-Viewer\"><a href=\"#Terminal-Markdown-Viewer\" class=\"headerlink\" title=\"Terminal Markdown Viewer\"></a>Terminal Markdown Viewer</h2><p>I though it would be cool to have some tool that could parse me Markdown files and present them nicely formatted, like <code>cat</code> command.</p>\n<p>And it seems i’m not the only one :)</p>\n<p><a href=\"https://github.com/axiros/terminal_markdown_viewer\">https://github.com/axiros/terminal_markdown_viewer</a></p>\n<p>In order to install just follow the instruction on the site. Pip worked well for me:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">pip install mdv</span><br></pre></td></tr></table></figure>\n<p>I’ve updated my PATH environment var on <code>.bashrc</code> with:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">export PYTHONPATH&#x3D;&quot;$&#123;PYTHONPATH&#125;:&#x2F;$&#123;HOME&#125;&#x2F;.local&#x2F;lib&#x2F;python2.7&#x2F;site-packages&quot;</span><br><span class=\"line\">export PATH&#x3D;&quot;$PATH:~&#x2F;.local&#x2F;bin&quot;</span><br></pre></td></tr></table></figure>\n<p>I can now execute <code>mdv</code> command try it out it some .md files :D</p>\n<h2 id=\"A-Markdown-spell-checking-tool\"><a href=\"#A-Markdown-spell-checking-tool\" class=\"headerlink\" title=\"A Markdown spell checking tool.\"></a>A Markdown spell checking tool.</h2><p>The other thing is regarding spell-check, as you can probably notice i really do a lot of errors, but i don’t wont to use a high resource editor just because of this how about a MD spell checking tool ?</p>\n<p>Well i’m trying this one ad i’m using [hexo][<a href=\"http://hexo.io]\">http://hexo.io]</a> for the page generation and it uses already npm modules just added a new one </p>\n<h3 id=\"install\"><a href=\"#install\" class=\"headerlink\" title=\"install\"></a>install</h3><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">npm i markdown-spellcheck -g</span><br></pre></td></tr></table></figure>\n<p>One can run interactive like</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mdspell &quot;**&#x2F;*.md&quot;</span><br></pre></td></tr></table></figure>\n<p>It’s a bit slow but the database would grow in terms, i’m giving a try to it.</p>\n<p>I also add to add <code>.spelling</code> no to keep the dictionary database on git. I probably would add it later.</p>\n<p>Cheers,<br>RR</p>\n"},{"title":"Nifi Parcel for CDH","date":"2017-09-29T01:01:48.000Z","_content":"\n\nI've been using [Apache Nifi](https://nifi.apache.org/) for some time now. But has a Cloudera user it would be nice to have it managed centralized in Cloudera Manager.\n\nI found this [repo]() which had only one commit for doing exactly this, so i decided to give it a try, to see if it worked.\n\n# Requirements\n\n* Virtual machine with Cloudera quickstart pre-installed\n* I had to patch the available Nifi version with [NIFI-3466](https://github.com/apache/nifi/pull/1604)\n* And parcel repo also with [Issue1](https://github.com/prateek/nifi-parcel/issues/1)\n\nSo i'll be using my own forks that have this changes allready, but will be following the author (prateek) instructions.\n\n# Instructions \n\n## Install Requirements\n\n* cloudera/cm_ext\n\n```sh\ncd /tmp\ngit clone https://github.com/cloudera/cm_ext\ncd cm_ext/validator\nmvn install\n```\n\n* Create parcel & CSD:\n\n```sh\n$ cd /tmp\n$ git clone git@github.com:rramos/nifi.git\n$ cd nifi\n$ mvn clean install\n$ cd /tmp\n$ git clone git@github.com:rramos/nifi-parcel.git\n$ cd nifi-parcel\n$ POINT_VERSION=5 VALIDATOR_DIR=/tmp/cm_ext ./build-parcel.sh /tmp/nifi/nifi-assembly/target/nifi-*-SNAPSHOT-bin.tar.gz\n$ VALIDATOR_DIR=/tmp/cm_ext ./build-csd.sh\n```\n\n# Test the Parcel\n\nCloudera quickstart images comes with java version 1.7. But i need version 1.8 for Nifi otherwise i'll get minor major version missmatch issue. So i had to make some changes.\n\n* First start your cloudera quickstart VM and copy the folders:\n    * `build-csd` \n    * `build-parcel` \n    \nthere in my case i had IP `192.168.122.132` (change for your case).\n\n```sh\nscp -r build-parcel cloudera@192.168.122.132:/tmp\nscp -r build-csd cloudera@192.168.122.132:/tmp\n```\n\n* SSH in the machine and install java8\n\n```sh\nyum install java-1.8.0-openjdk\n```\n\n* Run the following command in `build-parcel` folder\n\n```sh\ncd build-parcel\npython -m  SimpleHTTPServer 14641\n```\n\nNow open your browser at `http://192.168.122.132:7180` and access Cloudera Manager (cloudera:cloudera).\n\nNavigate to -> Parcels -> Edit Settings. \n\nAdd `http://192.168.122.132:14641` \n\n{% asset_img parcel01.png [NIFI Parcel] %}\n\nDownload and install the Nifi Parcel.\n\n* Copy NIFI home\n\n```sh\ncp -r /tmp/build-parcel/NIFI-0.0.5.nifi.p0.5 /opt/cloudera/parcels/NIFI/\n```\n\n* Correct java path in file `/opt/cloudera/parcels/NIFI/bin/nifi-env.sh`\n\n```sh\n# The java implementation to use.\nexport JAVA_HOME=/usr/lib/jvm/jre-1.8.0\n```\n\n* Copy the csd jars \n\n```sh\ncp /tmp/build-csd/NIFI-1.2.0.jar /opt/cloudera/csd/\nmkdir /opt/cloudera/csd/NIFI-1.2.0\ncp /tmp/build-csd/NIFI-1.2.0.jar /opt/cloudera/csd/NIFI-1.2.0/\ncd /opt/cloudera/csd/NIFI-1.2.0/\njar xvf NIFI-1.2.0.jar\nrm -f NIFI-1.2.0.jar\n```\n\n* Correct ownership\n\n```\nchown -R cloudera-scm:cloudera-scm /opt/cloudera\n```\n\n* Apply Configuration changes to Cloudera Manager and restart the service\n\n* Move CSD to Cloudera Manager's CSD Repo\n \n```sh\nsudo service cloudera-scm-server restart\n```\n\n* After login again in CM wait for zookeeper to be running normal, and add a new service (Nifi)\n\n{% asset_img parcel02.png [Add NIFI Service] %}\n\nAfter terminating the wizard you should have access to Nifi Interface\n\n\n{% asset_img parcel03.png [Nifi Web UI] %}\n\n \n# Conclusion\n\nI must say that prateek repo with only that commit worked pretty well and the instructions where also clear. There where some minor adjustments because of the java version, but we can start/stop the service via Cloudera Manager.\n\nThere are some pending items refered in his github page:\n* Currently `NiFi` runs under the `root` user\n* Expose config options under Cloudera Manager\n* Conf folder from parcels is used, this needs to be migrated to ConfigWriter\n* Expose metrics from NiFi\n\nI haven't tested the configuration in cluster mode as i has using the quickstart VM.\n\nThe configuration options in CM would be very good improvement. Auto configuring zookeeper and the other nifi options. I'll try to contribute to prateek excelent work if i manage to get some time.\n\nCheers,\nRR\n\n# References\n\n* https://nifi.apache.org\n* https://github.com/prateek/nifi-parcel\n* https://github.com/cloudera/cm_ext\n* https://www.cloudera.com/downloads/quickstart_vms/5-12.html\n","source":"_posts/nifiparcel.md","raw":"---\ntitle: Nifi Parcel for CDH\ntags:\n  - Cloudera\n  - Nifi\n  - CDH Parcel\n  - Hadoop\ndate: 2017-09-29 02:01:48\n---\n\n\nI've been using [Apache Nifi](https://nifi.apache.org/) for some time now. But has a Cloudera user it would be nice to have it managed centralized in Cloudera Manager.\n\nI found this [repo]() which had only one commit for doing exactly this, so i decided to give it a try, to see if it worked.\n\n# Requirements\n\n* Virtual machine with Cloudera quickstart pre-installed\n* I had to patch the available Nifi version with [NIFI-3466](https://github.com/apache/nifi/pull/1604)\n* And parcel repo also with [Issue1](https://github.com/prateek/nifi-parcel/issues/1)\n\nSo i'll be using my own forks that have this changes allready, but will be following the author (prateek) instructions.\n\n# Instructions \n\n## Install Requirements\n\n* cloudera/cm_ext\n\n```sh\ncd /tmp\ngit clone https://github.com/cloudera/cm_ext\ncd cm_ext/validator\nmvn install\n```\n\n* Create parcel & CSD:\n\n```sh\n$ cd /tmp\n$ git clone git@github.com:rramos/nifi.git\n$ cd nifi\n$ mvn clean install\n$ cd /tmp\n$ git clone git@github.com:rramos/nifi-parcel.git\n$ cd nifi-parcel\n$ POINT_VERSION=5 VALIDATOR_DIR=/tmp/cm_ext ./build-parcel.sh /tmp/nifi/nifi-assembly/target/nifi-*-SNAPSHOT-bin.tar.gz\n$ VALIDATOR_DIR=/tmp/cm_ext ./build-csd.sh\n```\n\n# Test the Parcel\n\nCloudera quickstart images comes with java version 1.7. But i need version 1.8 for Nifi otherwise i'll get minor major version missmatch issue. So i had to make some changes.\n\n* First start your cloudera quickstart VM and copy the folders:\n    * `build-csd` \n    * `build-parcel` \n    \nthere in my case i had IP `192.168.122.132` (change for your case).\n\n```sh\nscp -r build-parcel cloudera@192.168.122.132:/tmp\nscp -r build-csd cloudera@192.168.122.132:/tmp\n```\n\n* SSH in the machine and install java8\n\n```sh\nyum install java-1.8.0-openjdk\n```\n\n* Run the following command in `build-parcel` folder\n\n```sh\ncd build-parcel\npython -m  SimpleHTTPServer 14641\n```\n\nNow open your browser at `http://192.168.122.132:7180` and access Cloudera Manager (cloudera:cloudera).\n\nNavigate to -> Parcels -> Edit Settings. \n\nAdd `http://192.168.122.132:14641` \n\n{% asset_img parcel01.png [NIFI Parcel] %}\n\nDownload and install the Nifi Parcel.\n\n* Copy NIFI home\n\n```sh\ncp -r /tmp/build-parcel/NIFI-0.0.5.nifi.p0.5 /opt/cloudera/parcels/NIFI/\n```\n\n* Correct java path in file `/opt/cloudera/parcels/NIFI/bin/nifi-env.sh`\n\n```sh\n# The java implementation to use.\nexport JAVA_HOME=/usr/lib/jvm/jre-1.8.0\n```\n\n* Copy the csd jars \n\n```sh\ncp /tmp/build-csd/NIFI-1.2.0.jar /opt/cloudera/csd/\nmkdir /opt/cloudera/csd/NIFI-1.2.0\ncp /tmp/build-csd/NIFI-1.2.0.jar /opt/cloudera/csd/NIFI-1.2.0/\ncd /opt/cloudera/csd/NIFI-1.2.0/\njar xvf NIFI-1.2.0.jar\nrm -f NIFI-1.2.0.jar\n```\n\n* Correct ownership\n\n```\nchown -R cloudera-scm:cloudera-scm /opt/cloudera\n```\n\n* Apply Configuration changes to Cloudera Manager and restart the service\n\n* Move CSD to Cloudera Manager's CSD Repo\n \n```sh\nsudo service cloudera-scm-server restart\n```\n\n* After login again in CM wait for zookeeper to be running normal, and add a new service (Nifi)\n\n{% asset_img parcel02.png [Add NIFI Service] %}\n\nAfter terminating the wizard you should have access to Nifi Interface\n\n\n{% asset_img parcel03.png [Nifi Web UI] %}\n\n \n# Conclusion\n\nI must say that prateek repo with only that commit worked pretty well and the instructions where also clear. There where some minor adjustments because of the java version, but we can start/stop the service via Cloudera Manager.\n\nThere are some pending items refered in his github page:\n* Currently `NiFi` runs under the `root` user\n* Expose config options under Cloudera Manager\n* Conf folder from parcels is used, this needs to be migrated to ConfigWriter\n* Expose metrics from NiFi\n\nI haven't tested the configuration in cluster mode as i has using the quickstart VM.\n\nThe configuration options in CM would be very good improvement. Auto configuring zookeeper and the other nifi options. I'll try to contribute to prateek excelent work if i manage to get some time.\n\nCheers,\nRR\n\n# References\n\n* https://nifi.apache.org\n* https://github.com/prateek/nifi-parcel\n* https://github.com/cloudera/cm_ext\n* https://www.cloudera.com/downloads/quickstart_vms/5-12.html\n","slug":"nifiparcel","published":1,"updated":"2020-09-06T14:38:37.855Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjr000kchpfdku0bfju","content":"<p>I’ve been using <a href=\"https://nifi.apache.org/\">Apache Nifi</a> for some time now. But has a Cloudera user it would be nice to have it managed centralized in Cloudera Manager.</p>\n<p>I found this <a href=\"\">repo</a> which had only one commit for doing exactly this, so i decided to give it a try, to see if it worked.</p>\n<h1 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h1><ul>\n<li>Virtual machine with Cloudera quickstart pre-installed</li>\n<li>I had to patch the available Nifi version with <a href=\"https://github.com/apache/nifi/pull/1604\">NIFI-3466</a></li>\n<li>And parcel repo also with <a href=\"https://github.com/prateek/nifi-parcel/issues/1\">Issue1</a></li>\n</ul>\n<p>So i’ll be using my own forks that have this changes allready, but will be following the author (prateek) instructions.</p>\n<h1 id=\"Instructions\"><a href=\"#Instructions\" class=\"headerlink\" title=\"Instructions\"></a>Instructions</h1><h2 id=\"Install-Requirements\"><a href=\"#Install-Requirements\" class=\"headerlink\" title=\"Install Requirements\"></a>Install Requirements</h2><ul>\n<li>cloudera/cm_ext</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> /tmp</span><br><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/cloudera/cm_ext</span><br><span class=\"line\"><span class=\"built_in\">cd</span> cm_ext/validator</span><br><span class=\"line\">mvn install</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Create parcel &amp; CSD:</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">cd</span> /tmp</span><br><span class=\"line\">$ git <span class=\"built_in\">clone</span> git@github.com:rramos/nifi.git</span><br><span class=\"line\">$ <span class=\"built_in\">cd</span> nifi</span><br><span class=\"line\">$ mvn clean install</span><br><span class=\"line\">$ <span class=\"built_in\">cd</span> /tmp</span><br><span class=\"line\">$ git <span class=\"built_in\">clone</span> git@github.com:rramos/nifi-parcel.git</span><br><span class=\"line\">$ <span class=\"built_in\">cd</span> nifi-parcel</span><br><span class=\"line\">$ POINT_VERSION=5 VALIDATOR_DIR=/tmp/cm_ext ./build-parcel.sh /tmp/nifi/nifi-assembly/target/nifi-*-SNAPSHOT-bin.tar.gz</span><br><span class=\"line\">$ VALIDATOR_DIR=/tmp/cm_ext ./build-csd.sh</span><br></pre></td></tr></table></figure>\n<h1 id=\"Test-the-Parcel\"><a href=\"#Test-the-Parcel\" class=\"headerlink\" title=\"Test the Parcel\"></a>Test the Parcel</h1><p>Cloudera quickstart images comes with java version 1.7. But i need version 1.8 for Nifi otherwise i’ll get minor major version missmatch issue. So i had to make some changes.</p>\n<ul>\n<li>First start your cloudera quickstart VM and copy the folders:<ul>\n<li><p><code>build-csd</code> </p>\n</li>\n<li><p><code>build-parcel</code> </p>\n<p>there in my case i had IP <code>192.168.122.132</code> (change for your case).</p>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -r build-parcel cloudera@192.168.122.132:/tmp</span><br><span class=\"line\">scp -r build-csd cloudera@192.168.122.132:/tmp</span><br></pre></td></tr></table></figure>\n<ul>\n<li>SSH in the machine and install java8</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install java-1.8.0-openjdk</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Run the following command in <code>build-parcel</code> folder</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> build-parcel</span><br><span class=\"line\">python -m  SimpleHTTPServer 14641</span><br></pre></td></tr></table></figure>\n<p>Now open your browser at <code>http://192.168.122.132:7180</code> and access Cloudera Manager (cloudera:cloudera).</p>\n<p>Navigate to -&gt; Parcels -&gt; Edit Settings. </p>\n<p>Add <code>http://192.168.122.132:14641</code> </p>\n<img src=\"/2017/09/29/nifiparcel/parcel01.png\" class=\"\" title=\"[NIFI Parcel]\">\n\n<p>Download and install the Nifi Parcel.</p>\n<ul>\n<li>Copy NIFI home</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp -r /tmp/build-parcel/NIFI-0.0.5.nifi.p0.5 /opt/cloudera/parcels/NIFI/</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Correct java path in file <code>/opt/cloudera/parcels/NIFI/bin/nifi-env.sh</code></li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># The java implementation to use.</span></span><br><span class=\"line\"><span class=\"built_in\">export</span> JAVA_HOME=/usr/lib/jvm/jre-1.8.0</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Copy the csd jars </li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp /tmp/build-csd/NIFI-1.2.0.jar /opt/cloudera/csd/</span><br><span class=\"line\">mkdir /opt/cloudera/csd/NIFI-1.2.0</span><br><span class=\"line\">cp /tmp/build-csd/NIFI-1.2.0.jar /opt/cloudera/csd/NIFI-1.2.0/</span><br><span class=\"line\"><span class=\"built_in\">cd</span> /opt/cloudera/csd/NIFI-1.2.0/</span><br><span class=\"line\">jar xvf NIFI-1.2.0.jar</span><br><span class=\"line\">rm -f NIFI-1.2.0.jar</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Correct ownership</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chown -R cloudera-scm:cloudera-scm &#x2F;opt&#x2F;cloudera</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>Apply Configuration changes to Cloudera Manager and restart the service</p>\n</li>\n<li><p>Move CSD to Cloudera Manager’s CSD Repo</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo service cloudera-scm-server restart</span><br></pre></td></tr></table></figure></li>\n<li><p>After login again in CM wait for zookeeper to be running normal, and add a new service (Nifi)</p>\n</li>\n</ul>\n<img src=\"/2017/09/29/nifiparcel/parcel02.png\" class=\"\" title=\"[Add NIFI Service]\">\n\n<p>After terminating the wizard you should have access to Nifi Interface</p>\n<img src=\"/2017/09/29/nifiparcel/parcel03.png\" class=\"\" title=\"[Nifi Web UI]\">\n\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>I must say that prateek repo with only that commit worked pretty well and the instructions where also clear. There where some minor adjustments because of the java version, but we can start/stop the service via Cloudera Manager.</p>\n<p>There are some pending items refered in his github page:</p>\n<ul>\n<li>Currently <code>NiFi</code> runs under the <code>root</code> user</li>\n<li>Expose config options under Cloudera Manager</li>\n<li>Conf folder from parcels is used, this needs to be migrated to ConfigWriter</li>\n<li>Expose metrics from NiFi</li>\n</ul>\n<p>I haven’t tested the configuration in cluster mode as i has using the quickstart VM.</p>\n<p>The configuration options in CM would be very good improvement. Auto configuring zookeeper and the other nifi options. I’ll try to contribute to prateek excelent work if i manage to get some time.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://nifi.apache.org/\">https://nifi.apache.org</a></li>\n<li><a href=\"https://github.com/prateek/nifi-parcel\">https://github.com/prateek/nifi-parcel</a></li>\n<li><a href=\"https://github.com/cloudera/cm_ext\">https://github.com/cloudera/cm_ext</a></li>\n<li><a href=\"https://www.cloudera.com/downloads/quickstart_vms/5-12.html\">https://www.cloudera.com/downloads/quickstart_vms/5-12.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>I’ve been using <a href=\"https://nifi.apache.org/\">Apache Nifi</a> for some time now. But has a Cloudera user it would be nice to have it managed centralized in Cloudera Manager.</p>\n<p>I found this <a href=\"\">repo</a> which had only one commit for doing exactly this, so i decided to give it a try, to see if it worked.</p>\n<h1 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h1><ul>\n<li>Virtual machine with Cloudera quickstart pre-installed</li>\n<li>I had to patch the available Nifi version with <a href=\"https://github.com/apache/nifi/pull/1604\">NIFI-3466</a></li>\n<li>And parcel repo also with <a href=\"https://github.com/prateek/nifi-parcel/issues/1\">Issue1</a></li>\n</ul>\n<p>So i’ll be using my own forks that have this changes allready, but will be following the author (prateek) instructions.</p>\n<h1 id=\"Instructions\"><a href=\"#Instructions\" class=\"headerlink\" title=\"Instructions\"></a>Instructions</h1><h2 id=\"Install-Requirements\"><a href=\"#Install-Requirements\" class=\"headerlink\" title=\"Install Requirements\"></a>Install Requirements</h2><ul>\n<li>cloudera/cm_ext</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> /tmp</span><br><span class=\"line\">git <span class=\"built_in\">clone</span> https://github.com/cloudera/cm_ext</span><br><span class=\"line\"><span class=\"built_in\">cd</span> cm_ext/validator</span><br><span class=\"line\">mvn install</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Create parcel &amp; CSD:</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ <span class=\"built_in\">cd</span> /tmp</span><br><span class=\"line\">$ git <span class=\"built_in\">clone</span> git@github.com:rramos/nifi.git</span><br><span class=\"line\">$ <span class=\"built_in\">cd</span> nifi</span><br><span class=\"line\">$ mvn clean install</span><br><span class=\"line\">$ <span class=\"built_in\">cd</span> /tmp</span><br><span class=\"line\">$ git <span class=\"built_in\">clone</span> git@github.com:rramos/nifi-parcel.git</span><br><span class=\"line\">$ <span class=\"built_in\">cd</span> nifi-parcel</span><br><span class=\"line\">$ POINT_VERSION=5 VALIDATOR_DIR=/tmp/cm_ext ./build-parcel.sh /tmp/nifi/nifi-assembly/target/nifi-*-SNAPSHOT-bin.tar.gz</span><br><span class=\"line\">$ VALIDATOR_DIR=/tmp/cm_ext ./build-csd.sh</span><br></pre></td></tr></table></figure>\n<h1 id=\"Test-the-Parcel\"><a href=\"#Test-the-Parcel\" class=\"headerlink\" title=\"Test the Parcel\"></a>Test the Parcel</h1><p>Cloudera quickstart images comes with java version 1.7. But i need version 1.8 for Nifi otherwise i’ll get minor major version missmatch issue. So i had to make some changes.</p>\n<ul>\n<li>First start your cloudera quickstart VM and copy the folders:<ul>\n<li><p><code>build-csd</code> </p>\n</li>\n<li><p><code>build-parcel</code> </p>\n<p>there in my case i had IP <code>192.168.122.132</code> (change for your case).</p>\n</li>\n</ul>\n</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">scp -r build-parcel cloudera@192.168.122.132:/tmp</span><br><span class=\"line\">scp -r build-csd cloudera@192.168.122.132:/tmp</span><br></pre></td></tr></table></figure>\n<ul>\n<li>SSH in the machine and install java8</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">yum install java-1.8.0-openjdk</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Run the following command in <code>build-parcel</code> folder</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">cd</span> build-parcel</span><br><span class=\"line\">python -m  SimpleHTTPServer 14641</span><br></pre></td></tr></table></figure>\n<p>Now open your browser at <code>http://192.168.122.132:7180</code> and access Cloudera Manager (cloudera:cloudera).</p>\n<p>Navigate to -&gt; Parcels -&gt; Edit Settings. </p>\n<p>Add <code>http://192.168.122.132:14641</code> </p>\n<img src=\"/2017/09/29/nifiparcel/parcel01.png\" class=\"\" title=\"[NIFI Parcel]\">\n\n<p>Download and install the Nifi Parcel.</p>\n<ul>\n<li>Copy NIFI home</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp -r /tmp/build-parcel/NIFI-0.0.5.nifi.p0.5 /opt/cloudera/parcels/NIFI/</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Correct java path in file <code>/opt/cloudera/parcels/NIFI/bin/nifi-env.sh</code></li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># The java implementation to use.</span></span><br><span class=\"line\"><span class=\"built_in\">export</span> JAVA_HOME=/usr/lib/jvm/jre-1.8.0</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Copy the csd jars </li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">cp /tmp/build-csd/NIFI-1.2.0.jar /opt/cloudera/csd/</span><br><span class=\"line\">mkdir /opt/cloudera/csd/NIFI-1.2.0</span><br><span class=\"line\">cp /tmp/build-csd/NIFI-1.2.0.jar /opt/cloudera/csd/NIFI-1.2.0/</span><br><span class=\"line\"><span class=\"built_in\">cd</span> /opt/cloudera/csd/NIFI-1.2.0/</span><br><span class=\"line\">jar xvf NIFI-1.2.0.jar</span><br><span class=\"line\">rm -f NIFI-1.2.0.jar</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Correct ownership</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">chown -R cloudera-scm:cloudera-scm &#x2F;opt&#x2F;cloudera</span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>Apply Configuration changes to Cloudera Manager and restart the service</p>\n</li>\n<li><p>Move CSD to Cloudera Manager’s CSD Repo</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo service cloudera-scm-server restart</span><br></pre></td></tr></table></figure></li>\n<li><p>After login again in CM wait for zookeeper to be running normal, and add a new service (Nifi)</p>\n</li>\n</ul>\n<img src=\"/2017/09/29/nifiparcel/parcel02.png\" class=\"\" title=\"[Add NIFI Service]\">\n\n<p>After terminating the wizard you should have access to Nifi Interface</p>\n<img src=\"/2017/09/29/nifiparcel/parcel03.png\" class=\"\" title=\"[Nifi Web UI]\">\n\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>I must say that prateek repo with only that commit worked pretty well and the instructions where also clear. There where some minor adjustments because of the java version, but we can start/stop the service via Cloudera Manager.</p>\n<p>There are some pending items refered in his github page:</p>\n<ul>\n<li>Currently <code>NiFi</code> runs under the <code>root</code> user</li>\n<li>Expose config options under Cloudera Manager</li>\n<li>Conf folder from parcels is used, this needs to be migrated to ConfigWriter</li>\n<li>Expose metrics from NiFi</li>\n</ul>\n<p>I haven’t tested the configuration in cluster mode as i has using the quickstart VM.</p>\n<p>The configuration options in CM would be very good improvement. Auto configuring zookeeper and the other nifi options. I’ll try to contribute to prateek excelent work if i manage to get some time.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://nifi.apache.org/\">https://nifi.apache.org</a></li>\n<li><a href=\"https://github.com/prateek/nifi-parcel\">https://github.com/prateek/nifi-parcel</a></li>\n<li><a href=\"https://github.com/cloudera/cm_ext\">https://github.com/cloudera/cm_ext</a></li>\n<li><a href=\"https://www.cloudera.com/downloads/quickstart_vms/5-12.html\">https://www.cloudera.com/downloads/quickstart_vms/5-12.html</a></li>\n</ul>\n"},{"title":"Portainer a UI for Docker Management","date":"2017-09-23T16:22:37.000Z","_content":"\n\n[Portainer](https://portainer.io/) is:\n\n{% blockquote Offical Website Definition%}\n\nPORTAINER IS AN OPEN-SOURCE LIGHTWEIGHT MANAGEMENT UI WHICH ALLOWS YOU TO EASILY MANAGE YOUR DOCKER HOSTS OR SWARM CLUSTERS\n{% endblockquote %}\n\nIn order to manage the dockers where it is running one should pass the following option\n\n```\nsudo docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer\n```\n\n\n## Snapshot\n\n{% asset_img portainer.io.png [Portainer.io] %}\n\n# Setup\n\nCreate the following `docker-compose.yml` file \n\n```\nversion: '2'\nservices:\n  portainer:\n    restart: always\n    ports:\n     - \"9000:9000\"\n    volumes:\n     - ./data:/opt/data\n     - /var/run/docker.sock:/var/run/docker.sock\n    image: \"portainer/portainer\"\n\n```\n\nand execute `docker-compose up -d`\n\nYou can then access the interface at: http://localhost:9000\n\n\nCheers,\nRR","source":"_posts/portainer.md","raw":"---\ntitle: Portainer a UI for docker management\ntags:\n  - Docker\n  - Portainer\ndate: 2017-09-23 17:22:37\n---\n\n\n[Portainer](https://portainer.io/) is:\n\n{% blockquote Offical Website Definition%}\n\nPORTAINER IS AN OPEN-SOURCE LIGHTWEIGHT MANAGEMENT UI WHICH ALLOWS YOU TO EASILY MANAGE YOUR DOCKER HOSTS OR SWARM CLUSTERS\n{% endblockquote %}\n\nIn order to manage the dockers where it is running one should pass the following option\n\n```\nsudo docker run -d -p 9000:9000 -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer\n```\n\n\n## Snapshot\n\n{% asset_img portainer.io.png [Portainer.io] %}\n\n# Setup\n\nCreate the following `docker-compose.yml` file \n\n```\nversion: '2'\nservices:\n  portainer:\n    restart: always\n    ports:\n     - \"9000:9000\"\n    volumes:\n     - ./data:/opt/data\n     - /var/run/docker.sock:/var/run/docker.sock\n    image: \"portainer/portainer\"\n\n```\n\nand execute `docker-compose up -d`\n\nYou can then access the interface at: http://localhost:9000\n\n\nCheers,\nRR","slug":"portainer","published":1,"updated":"2020-09-06T14:38:37.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjs000lchpf70bkfhu6","content":"<p><a href=\"https://portainer.io/\">Portainer</a> is:</p>\n<blockquote><p>PORTAINER IS AN OPEN-SOURCE LIGHTWEIGHT MANAGEMENT UI WHICH ALLOWS YOU TO EASILY MANAGE YOUR DOCKER HOSTS OR SWARM CLUSTERS</p>\n<footer><strong>Offical Website Definition</strong></footer></blockquote>\n\n<p>In order to manage the dockers where it is running one should pass the following option</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker run -d -p 9000:9000 -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock portainer&#x2F;portainer</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Snapshot\"><a href=\"#Snapshot\" class=\"headerlink\" title=\"Snapshot\"></a>Snapshot</h2><img src=\"/2017/09/23/portainer/portainer.io.png\" class=\"\" title=\"[Portainer.io]\">\n\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><p>Create the following <code>docker-compose.yml</code> file </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &#39;2&#39;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  portainer:</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;9000:9000&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - .&#x2F;data:&#x2F;opt&#x2F;data</span><br><span class=\"line\">     - &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock</span><br><span class=\"line\">    image: &quot;portainer&#x2F;portainer&quot;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>and execute <code>docker-compose up -d</code></p>\n<p>You can then access the interface at: <a href=\"http://localhost:9000/\">http://localhost:9000</a></p>\n<p>Cheers,<br>RR</p>\n","site":{"data":{}},"excerpt":"","more":"<p><a href=\"https://portainer.io/\">Portainer</a> is:</p>\n<blockquote><p>PORTAINER IS AN OPEN-SOURCE LIGHTWEIGHT MANAGEMENT UI WHICH ALLOWS YOU TO EASILY MANAGE YOUR DOCKER HOSTS OR SWARM CLUSTERS</p>\n<footer><strong>Offical Website Definition</strong></footer></blockquote>\n\n<p>In order to manage the dockers where it is running one should pass the following option</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker run -d -p 9000:9000 -v &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock portainer&#x2F;portainer</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"Snapshot\"><a href=\"#Snapshot\" class=\"headerlink\" title=\"Snapshot\"></a>Snapshot</h2><img src=\"/2017/09/23/portainer/portainer.io.png\" class=\"\" title=\"[Portainer.io]\">\n\n<h1 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h1><p>Create the following <code>docker-compose.yml</code> file </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &#39;2&#39;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  portainer:</span><br><span class=\"line\">    restart: always</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;9000:9000&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - .&#x2F;data:&#x2F;opt&#x2F;data</span><br><span class=\"line\">     - &#x2F;var&#x2F;run&#x2F;docker.sock:&#x2F;var&#x2F;run&#x2F;docker.sock</span><br><span class=\"line\">    image: &quot;portainer&#x2F;portainer&quot;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>and execute <code>docker-compose up -d</code></p>\n<p>You can then access the interface at: <a href=\"http://localhost:9000/\">http://localhost:9000</a></p>\n<p>Cheers,<br>RR</p>\n"},{"title":"Porto Tech HUB 2016","date":"2016-09-23T21:40:10.000Z","_content":"\n\nI've attend today to the second edition of Porto Tech HUB 2016. It was good to see old friends and the raising tech potencial in the City.\n\nThere where some good presentations, good organization and the location was appealing also.\n\nI especially liked Christoffer Noring presentation about [NativeScript](https://www.nativescript.org/) and [Angular](https://angularjs.org) and Miguel Veiga about [Storm](http://storm.apache.org) and [Kafka](http://kafka.apache.org).\n\nThis last one have more to do with my actual work.\n\n* [http://www.portotechhub.com](http://www.portotechhub.com)\n* [https://twitter.com/PortoTechHub](https://twitter.com/PortoTechHub)","source":"_posts/portotechhub-com.md","raw":"---\ntitle: Porto Tech HUB 2016\ntags:\n  - PortoTechHub\n  - Conferences\n  - Porto\n  - Tech\ndate: 2016-09-23 22:40:10\n---\n\n\nI've attend today to the second edition of Porto Tech HUB 2016. It was good to see old friends and the raising tech potencial in the City.\n\nThere where some good presentations, good organization and the location was appealing also.\n\nI especially liked Christoffer Noring presentation about [NativeScript](https://www.nativescript.org/) and [Angular](https://angularjs.org) and Miguel Veiga about [Storm](http://storm.apache.org) and [Kafka](http://kafka.apache.org).\n\nThis last one have more to do with my actual work.\n\n* [http://www.portotechhub.com](http://www.portotechhub.com)\n* [https://twitter.com/PortoTechHub](https://twitter.com/PortoTechHub)","slug":"portotechhub-com","published":1,"updated":"2020-09-06T14:38:37.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjt000mchpf4wg91kgz","content":"<p>I’ve attend today to the second edition of Porto Tech HUB 2016. It was good to see old friends and the raising tech potencial in the City.</p>\n<p>There where some good presentations, good organization and the location was appealing also.</p>\n<p>I especially liked Christoffer Noring presentation about <a href=\"https://www.nativescript.org/\">NativeScript</a> and <a href=\"https://angularjs.org/\">Angular</a> and Miguel Veiga about <a href=\"http://storm.apache.org/\">Storm</a> and <a href=\"http://kafka.apache.org/\">Kafka</a>.</p>\n<p>This last one have more to do with my actual work.</p>\n<ul>\n<li><a href=\"http://www.portotechhub.com/\">http://www.portotechhub.com</a></li>\n<li><a href=\"https://twitter.com/PortoTechHub\">https://twitter.com/PortoTechHub</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>I’ve attend today to the second edition of Porto Tech HUB 2016. It was good to see old friends and the raising tech potencial in the City.</p>\n<p>There where some good presentations, good organization and the location was appealing also.</p>\n<p>I especially liked Christoffer Noring presentation about <a href=\"https://www.nativescript.org/\">NativeScript</a> and <a href=\"https://angularjs.org/\">Angular</a> and Miguel Veiga about <a href=\"http://storm.apache.org/\">Storm</a> and <a href=\"http://kafka.apache.org/\">Kafka</a>.</p>\n<p>This last one have more to do with my actual work.</p>\n<ul>\n<li><a href=\"http://www.portotechhub.com/\">http://www.portotechhub.com</a></li>\n<li><a href=\"https://twitter.com/PortoTechHub\">https://twitter.com/PortoTechHub</a></li>\n</ul>\n"},{"title":"PowerShell on Linux","date":"2017-10-06T22:11:39.000Z","_content":"\n\nThis article will present the required steps to install [powershell](https://en.wikipedia.org/wiki/PowerShell) on Linux.\n\n# Objective\n\nThe objective is to operate from Linux [Windows Azure Pack](https://www.microsoft.com/en-us/cloud-platform/windows-azure-pack).\n\n# Procedure\n\n* Import the public repository GPG keys\n\n```sh\ncurl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\n```\n\n* Register the Microsoft Ubuntu repository\n\n```sh\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | sudo tee /etc/apt/sources.list.d/microsoft.list\n```\n\n* Update the list of products\n\n```sh\nsudo apt-get update\n```\n\n* Install PowerShell\n\n```sh\nsudo apt-get install -y powershell\n```\n\n* Start PowerShell\n\n```sh\npowershell\n```\n\n# Add Azure module\n\nInstall Azure module in powershell running with `sudo`\n\n```\nInstall-Module AzureRM\nInstall-Module Azure\n```\n\nCheck module location\n\n```sh\n(gmo -l Azure*).path\n```\n\n\n# Uninstall\n\n```sh\nsudo apt-get remove powershell\n```\n\n# Conclusion\n\nAfter some hours i've given up making this thing work on Linux. The time it takes doesn't payoff, and even if i could make it work, i would still need to make Vagrant work with it for some VM's provisioning in a private cloud. It would take me less time to do it by hand :|\n\nFrom a standard testing perspective it's nice to have this thing on Linux, because you could centralize the management, but at the moment of this article it doesn't seem to payoff the time one would need to setup this. \n\nIf you have lot's of Windows machines or Azure infrastructure to manage just get a Windows PC, or wait for Microsoft to fix this, as there are lot's of reports regarding this issue. But seriously i don't see an effort from Microft to make this work properly, what's the gain, rigth !?.\n\nCheers,\nRR\n\n\n# References\n\n* https://github.com/PowerShell/PowerShell\n* https://github.com/bgelens/WAPTenantPublicAPI\n* http://www.tech-coffee.net/windows-azure-pack-powershell-tenant-api/\n* https://blogs.msdn.microsoft.com/powershell/2016/08/18/powershell-on-linux-and-open-source-2/\n* https://blogs.technet.microsoft.com/privatecloud/2013/12/10/windows-azure-pack-reconfigure-portal-names-ports-and-use-trusted-certificates/\n","source":"_posts/powershellonlinux.md","raw":"---\ntitle: PowerShell on Linux\ntags:\n  - PowerShell\n  - Linux\n  - Windows\ndate: 2017-10-06 23:11:39\n---\n\n\nThis article will present the required steps to install [powershell](https://en.wikipedia.org/wiki/PowerShell) on Linux.\n\n# Objective\n\nThe objective is to operate from Linux [Windows Azure Pack](https://www.microsoft.com/en-us/cloud-platform/windows-azure-pack).\n\n# Procedure\n\n* Import the public repository GPG keys\n\n```sh\ncurl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -\n```\n\n* Register the Microsoft Ubuntu repository\n\n```sh\ncurl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | sudo tee /etc/apt/sources.list.d/microsoft.list\n```\n\n* Update the list of products\n\n```sh\nsudo apt-get update\n```\n\n* Install PowerShell\n\n```sh\nsudo apt-get install -y powershell\n```\n\n* Start PowerShell\n\n```sh\npowershell\n```\n\n# Add Azure module\n\nInstall Azure module in powershell running with `sudo`\n\n```\nInstall-Module AzureRM\nInstall-Module Azure\n```\n\nCheck module location\n\n```sh\n(gmo -l Azure*).path\n```\n\n\n# Uninstall\n\n```sh\nsudo apt-get remove powershell\n```\n\n# Conclusion\n\nAfter some hours i've given up making this thing work on Linux. The time it takes doesn't payoff, and even if i could make it work, i would still need to make Vagrant work with it for some VM's provisioning in a private cloud. It would take me less time to do it by hand :|\n\nFrom a standard testing perspective it's nice to have this thing on Linux, because you could centralize the management, but at the moment of this article it doesn't seem to payoff the time one would need to setup this. \n\nIf you have lot's of Windows machines or Azure infrastructure to manage just get a Windows PC, or wait for Microsoft to fix this, as there are lot's of reports regarding this issue. But seriously i don't see an effort from Microft to make this work properly, what's the gain, rigth !?.\n\nCheers,\nRR\n\n\n# References\n\n* https://github.com/PowerShell/PowerShell\n* https://github.com/bgelens/WAPTenantPublicAPI\n* http://www.tech-coffee.net/windows-azure-pack-powershell-tenant-api/\n* https://blogs.msdn.microsoft.com/powershell/2016/08/18/powershell-on-linux-and-open-source-2/\n* https://blogs.technet.microsoft.com/privatecloud/2013/12/10/windows-azure-pack-reconfigure-portal-names-ports-and-use-trusted-certificates/\n","slug":"powershellonlinux","published":1,"updated":"2020-09-06T14:38:37.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjt000nchpf3noxc07s","content":"<p>This article will present the required steps to install <a href=\"https://en.wikipedia.org/wiki/PowerShell\">powershell</a> on Linux.</p>\n<h1 id=\"Objective\"><a href=\"#Objective\" class=\"headerlink\" title=\"Objective\"></a>Objective</h1><p>The objective is to operate from Linux <a href=\"https://www.microsoft.com/en-us/cloud-platform/windows-azure-pack\">Windows Azure Pack</a>.</p>\n<h1 id=\"Procedure\"><a href=\"#Procedure\" class=\"headerlink\" title=\"Procedure\"></a>Procedure</h1><ul>\n<li>Import the public repository GPG keys</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Register the Microsoft Ubuntu repository</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | sudo tee /etc/apt/sources.list.d/microsoft.list</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Update the list of products</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get update</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Install PowerShell</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install -y powershell</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Start PowerShell</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">powershell</span><br></pre></td></tr></table></figure>\n<h1 id=\"Add-Azure-module\"><a href=\"#Add-Azure-module\" class=\"headerlink\" title=\"Add Azure module\"></a>Add Azure module</h1><p>Install Azure module in powershell running with <code>sudo</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Install-Module AzureRM</span><br><span class=\"line\">Install-Module Azure</span><br></pre></td></tr></table></figure>\n<p>Check module location</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(gmo -l Azure*).path</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Uninstall\"><a href=\"#Uninstall\" class=\"headerlink\" title=\"Uninstall\"></a>Uninstall</h1><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get remove powershell</span><br></pre></td></tr></table></figure>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>After some hours i’ve given up making this thing work on Linux. The time it takes doesn’t payoff, and even if i could make it work, i would still need to make Vagrant work with it for some VM’s provisioning in a private cloud. It would take me less time to do it by hand :|</p>\n<p>From a standard testing perspective it’s nice to have this thing on Linux, because you could centralize the management, but at the moment of this article it doesn’t seem to payoff the time one would need to setup this. </p>\n<p>If you have lot’s of Windows machines or Azure infrastructure to manage just get a Windows PC, or wait for Microsoft to fix this, as there are lot’s of reports regarding this issue. But seriously i don’t see an effort from Microft to make this work properly, what’s the gain, rigth !?.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://github.com/PowerShell/PowerShell\">https://github.com/PowerShell/PowerShell</a></li>\n<li><a href=\"https://github.com/bgelens/WAPTenantPublicAPI\">https://github.com/bgelens/WAPTenantPublicAPI</a></li>\n<li><a href=\"http://www.tech-coffee.net/windows-azure-pack-powershell-tenant-api/\">http://www.tech-coffee.net/windows-azure-pack-powershell-tenant-api/</a></li>\n<li><a href=\"https://blogs.msdn.microsoft.com/powershell/2016/08/18/powershell-on-linux-and-open-source-2/\">https://blogs.msdn.microsoft.com/powershell/2016/08/18/powershell-on-linux-and-open-source-2/</a></li>\n<li><a href=\"https://blogs.technet.microsoft.com/privatecloud/2013/12/10/windows-azure-pack-reconfigure-portal-names-ports-and-use-trusted-certificates/\">https://blogs.technet.microsoft.com/privatecloud/2013/12/10/windows-azure-pack-reconfigure-portal-names-ports-and-use-trusted-certificates/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>This article will present the required steps to install <a href=\"https://en.wikipedia.org/wiki/PowerShell\">powershell</a> on Linux.</p>\n<h1 id=\"Objective\"><a href=\"#Objective\" class=\"headerlink\" title=\"Objective\"></a>Objective</h1><p>The objective is to operate from Linux <a href=\"https://www.microsoft.com/en-us/cloud-platform/windows-azure-pack\">Windows Azure Pack</a>.</p>\n<h1 id=\"Procedure\"><a href=\"#Procedure\" class=\"headerlink\" title=\"Procedure\"></a>Procedure</h1><ul>\n<li>Import the public repository GPG keys</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl https://packages.microsoft.com/keys/microsoft.asc | sudo apt-key add -</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Register the Microsoft Ubuntu repository</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl https://packages.microsoft.com/config/ubuntu/16.04/prod.list | sudo tee /etc/apt/sources.list.d/microsoft.list</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Update the list of products</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get update</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Install PowerShell</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install -y powershell</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Start PowerShell</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">powershell</span><br></pre></td></tr></table></figure>\n<h1 id=\"Add-Azure-module\"><a href=\"#Add-Azure-module\" class=\"headerlink\" title=\"Add Azure module\"></a>Add Azure module</h1><p>Install Azure module in powershell running with <code>sudo</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Install-Module AzureRM</span><br><span class=\"line\">Install-Module Azure</span><br></pre></td></tr></table></figure>\n<p>Check module location</p>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">(gmo -l Azure*).path</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"Uninstall\"><a href=\"#Uninstall\" class=\"headerlink\" title=\"Uninstall\"></a>Uninstall</h1><figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get remove powershell</span><br></pre></td></tr></table></figure>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>After some hours i’ve given up making this thing work on Linux. The time it takes doesn’t payoff, and even if i could make it work, i would still need to make Vagrant work with it for some VM’s provisioning in a private cloud. It would take me less time to do it by hand :|</p>\n<p>From a standard testing perspective it’s nice to have this thing on Linux, because you could centralize the management, but at the moment of this article it doesn’t seem to payoff the time one would need to setup this. </p>\n<p>If you have lot’s of Windows machines or Azure infrastructure to manage just get a Windows PC, or wait for Microsoft to fix this, as there are lot’s of reports regarding this issue. But seriously i don’t see an effort from Microft to make this work properly, what’s the gain, rigth !?.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://github.com/PowerShell/PowerShell\">https://github.com/PowerShell/PowerShell</a></li>\n<li><a href=\"https://github.com/bgelens/WAPTenantPublicAPI\">https://github.com/bgelens/WAPTenantPublicAPI</a></li>\n<li><a href=\"http://www.tech-coffee.net/windows-azure-pack-powershell-tenant-api/\">http://www.tech-coffee.net/windows-azure-pack-powershell-tenant-api/</a></li>\n<li><a href=\"https://blogs.msdn.microsoft.com/powershell/2016/08/18/powershell-on-linux-and-open-source-2/\">https://blogs.msdn.microsoft.com/powershell/2016/08/18/powershell-on-linux-and-open-source-2/</a></li>\n<li><a href=\"https://blogs.technet.microsoft.com/privatecloud/2013/12/10/windows-azure-pack-reconfigure-portal-names-ports-and-use-trusted-certificates/\">https://blogs.technet.microsoft.com/privatecloud/2013/12/10/windows-azure-pack-reconfigure-portal-names-ports-and-use-trusted-certificates/</a></li>\n</ul>\n"},{"title":"Quick Setup Zeppelin Notebook","date":"2017-09-19T21:06:18.000Z","_content":"\n\n\nIn this article i describe a quick way to have zeepelin running so that you could quickly testing some Spark application.\n\n**NOTE:** This procedure shouldn't be used in production environments has you should setup the Notebook with auth and connected to your local infrastructure.\n\n## Requirements\n\n* One should have a docker environment setup. Check my previous {% post_link dockerclean article %} if you need some help with that\n* [Docker-compose](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-14-04)\n\n## Setup\n\n1. Create a folder named zeepelin\n```\nmkdir docker-zeepelin\n```\n\n2. Create a `data` where you could put some data to analyse.\n```\nmkdir -p docker-zeepelin/data\n```\n\n3. Create the following `docker-compose.yml` file in dir `docker-zeepelin` :\n\n```\nversion: '2'\nservices:\n  zeppelin:\n    ports:\n     - \"8080:8080\"\n    volumes:\n     - ./data:/opt/data\n    image: \"dylanmei/zeppelin\"\n\n```\n\n4. Launch docker-compose\n```\nsudo docker-compose up -d\n```\n\n5. That's it you should now be able to access http://localhost:8080 \n\n\n## Test it\n\n1. Lets download a demo file to our `data` dir. \n```\ncurl -s https://api.opendota.com/api/publicMatches -o ./data/OpenDotaPublic.json\n```\n\nYeah! I kinda like Dota so this makes sense :D\n\n2. Create a new NoteBook in the web Interface and use the following code\n\n```\n%spark\n\nval df = sqlContext.read.json(\"file:///opt/data/OpenDotaPublic.json\")\ndf.show\n```\n\nHit: **Shift-Enter**\n\n3. Let's register this dataframe as temp table and create some visuals\n\n```\n%spark\ndf.registerTempTable(\"publicmatches\")\n```\n\n4. Create the following to generate visualizations\n```\n%sql\nselect radiant_win,match_id\n from publicmatches\n```\n\n\n{% asset_img radiant_win.png [PieChart] %}\n\nGuess i need to start playing on Radiant side :D\n\n\nWell and that's it.\n\nCheers,\nRR\n\n# References\n\n* https://zeppelin.apache.org/docs/0.5.5-incubating/tutorial/tutorial.html\n* https://hortonworks.com/tutorial/getting-started-with-apache-zeppelin/\n* https://www.digitalocean.com/community/tutorials/how-to-install-docker-compose-on-ubuntu-16-04\n* https://docs.docker.com/compose\n* https://zeppelin.apache.org/","source":"_posts/quickzeppelin.md","raw":"---\ntitle: Quick Setup Zeppelin Notebook\ntags:\n  - Zeepelin\n  - Scala\n  - Docker\n  - Spark\ndate: 2017-09-19 22:06:18\n---\n\n\n\nIn this article i describe a quick way to have zeepelin running so that you could quickly testing some Spark application.\n\n**NOTE:** This procedure shouldn't be used in production environments has you should setup the Notebook with auth and connected to your local infrastructure.\n\n## Requirements\n\n* One should have a docker environment setup. Check my previous {% post_link dockerclean article %} if you need some help with that\n* [Docker-compose](https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-14-04)\n\n## Setup\n\n1. Create a folder named zeepelin\n```\nmkdir docker-zeepelin\n```\n\n2. Create a `data` where you could put some data to analyse.\n```\nmkdir -p docker-zeepelin/data\n```\n\n3. Create the following `docker-compose.yml` file in dir `docker-zeepelin` :\n\n```\nversion: '2'\nservices:\n  zeppelin:\n    ports:\n     - \"8080:8080\"\n    volumes:\n     - ./data:/opt/data\n    image: \"dylanmei/zeppelin\"\n\n```\n\n4. Launch docker-compose\n```\nsudo docker-compose up -d\n```\n\n5. That's it you should now be able to access http://localhost:8080 \n\n\n## Test it\n\n1. Lets download a demo file to our `data` dir. \n```\ncurl -s https://api.opendota.com/api/publicMatches -o ./data/OpenDotaPublic.json\n```\n\nYeah! I kinda like Dota so this makes sense :D\n\n2. Create a new NoteBook in the web Interface and use the following code\n\n```\n%spark\n\nval df = sqlContext.read.json(\"file:///opt/data/OpenDotaPublic.json\")\ndf.show\n```\n\nHit: **Shift-Enter**\n\n3. Let's register this dataframe as temp table and create some visuals\n\n```\n%spark\ndf.registerTempTable(\"publicmatches\")\n```\n\n4. Create the following to generate visualizations\n```\n%sql\nselect radiant_win,match_id\n from publicmatches\n```\n\n\n{% asset_img radiant_win.png [PieChart] %}\n\nGuess i need to start playing on Radiant side :D\n\n\nWell and that's it.\n\nCheers,\nRR\n\n# References\n\n* https://zeppelin.apache.org/docs/0.5.5-incubating/tutorial/tutorial.html\n* https://hortonworks.com/tutorial/getting-started-with-apache-zeppelin/\n* https://www.digitalocean.com/community/tutorials/how-to-install-docker-compose-on-ubuntu-16-04\n* https://docs.docker.com/compose\n* https://zeppelin.apache.org/","slug":"quickzeppelin","published":1,"updated":"2020-09-06T14:38:37.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxju000ochpf9zisdvn8","content":"<p>In this article i describe a quick way to have zeepelin running so that you could quickly testing some Spark application.</p>\n<p><strong>NOTE:</strong> This procedure shouldn’t be used in production environments has you should setup the Notebook with auth and connected to your local infrastructure.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><ul>\n<li>One should have a docker environment setup. Check my previous <a href=\"/2017/09/19/dockerclean/\" title=\"article\">article</a> if you need some help with that</li>\n<li><a href=\"https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-14-04\">Docker-compose</a></li>\n</ul>\n<h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><ol>\n<li>Create a folder named zeepelin<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir docker-zeepelin</span><br></pre></td></tr></table></figure></li>\n<li>Create a <code>data</code> where you could put some data to analyse.<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p docker-zeepelin&#x2F;data</span><br></pre></td></tr></table></figure></li>\n<li>Create the following <code>docker-compose.yml</code> file in dir <code>docker-zeepelin</code> :</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &#39;2&#39;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  zeppelin:</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;8080:8080&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - .&#x2F;data:&#x2F;opt&#x2F;data</span><br><span class=\"line\">    image: &quot;dylanmei&#x2F;zeppelin&quot;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>Launch docker-compose<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker-compose up -d</span><br></pre></td></tr></table></figure></li>\n<li>That’s it you should now be able to access <a href=\"http://localhost:8080/\">http://localhost:8080</a> </li>\n</ol>\n<h2 id=\"Test-it\"><a href=\"#Test-it\" class=\"headerlink\" title=\"Test it\"></a>Test it</h2><ol>\n<li><p>Lets download a demo file to our <code>data</code> dir. </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -s https:&#x2F;&#x2F;api.opendota.com&#x2F;api&#x2F;publicMatches -o .&#x2F;data&#x2F;OpenDotaPublic.json</span><br></pre></td></tr></table></figure>\n<p>Yeah! I kinda like Dota so this makes sense :D</p>\n</li>\n<li><p>Create a new NoteBook in the web Interface and use the following code</p>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%spark</span><br><span class=\"line\"></span><br><span class=\"line\">val df &#x3D; sqlContext.read.json(&quot;file:&#x2F;&#x2F;&#x2F;opt&#x2F;data&#x2F;OpenDotaPublic.json&quot;)</span><br><span class=\"line\">df.show</span><br></pre></td></tr></table></figure>\n<p>Hit: <strong>Shift-Enter</strong></p>\n<ol start=\"3\">\n<li>Let’s register this dataframe as temp table and create some visuals</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%spark</span><br><span class=\"line\">df.registerTempTable(&quot;publicmatches&quot;)</span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>Create the following to generate visualizations<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%sql</span><br><span class=\"line\">select radiant_win,match_id</span><br><span class=\"line\"> from publicmatches</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<img src=\"/2017/09/19/quickzeppelin/radiant_win.png\" class=\"\" title=\"[PieChart]\">\n\n<p>Guess i need to start playing on Radiant side :D</p>\n<p>Well and that’s it.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://zeppelin.apache.org/docs/0.5.5-incubating/tutorial/tutorial.html\">https://zeppelin.apache.org/docs/0.5.5-incubating/tutorial/tutorial.html</a></li>\n<li><a href=\"https://hortonworks.com/tutorial/getting-started-with-apache-zeppelin/\">https://hortonworks.com/tutorial/getting-started-with-apache-zeppelin/</a></li>\n<li><a href=\"https://www.digitalocean.com/community/tutorials/how-to-install-docker-compose-on-ubuntu-16-04\">https://www.digitalocean.com/community/tutorials/how-to-install-docker-compose-on-ubuntu-16-04</a></li>\n<li><a href=\"https://docs.docker.com/compose\">https://docs.docker.com/compose</a></li>\n<li><a href=\"https://zeppelin.apache.org/\">https://zeppelin.apache.org/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>In this article i describe a quick way to have zeepelin running so that you could quickly testing some Spark application.</p>\n<p><strong>NOTE:</strong> This procedure shouldn’t be used in production environments has you should setup the Notebook with auth and connected to your local infrastructure.</p>\n<h2 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h2><ul>\n<li>One should have a docker environment setup. Check my previous <a href=\"/2017/09/19/dockerclean/\" title=\"article\">article</a> if you need some help with that</li>\n<li><a href=\"https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-compose-on-ubuntu-14-04\">Docker-compose</a></li>\n</ul>\n<h2 id=\"Setup\"><a href=\"#Setup\" class=\"headerlink\" title=\"Setup\"></a>Setup</h2><ol>\n<li>Create a folder named zeepelin<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir docker-zeepelin</span><br></pre></td></tr></table></figure></li>\n<li>Create a <code>data</code> where you could put some data to analyse.<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir -p docker-zeepelin&#x2F;data</span><br></pre></td></tr></table></figure></li>\n<li>Create the following <code>docker-compose.yml</code> file in dir <code>docker-zeepelin</code> :</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">version: &#39;2&#39;</span><br><span class=\"line\">services:</span><br><span class=\"line\">  zeppelin:</span><br><span class=\"line\">    ports:</span><br><span class=\"line\">     - &quot;8080:8080&quot;</span><br><span class=\"line\">    volumes:</span><br><span class=\"line\">     - .&#x2F;data:&#x2F;opt&#x2F;data</span><br><span class=\"line\">    image: &quot;dylanmei&#x2F;zeppelin&quot;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>Launch docker-compose<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo docker-compose up -d</span><br></pre></td></tr></table></figure></li>\n<li>That’s it you should now be able to access <a href=\"http://localhost:8080/\">http://localhost:8080</a> </li>\n</ol>\n<h2 id=\"Test-it\"><a href=\"#Test-it\" class=\"headerlink\" title=\"Test it\"></a>Test it</h2><ol>\n<li><p>Lets download a demo file to our <code>data</code> dir. </p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">curl -s https:&#x2F;&#x2F;api.opendota.com&#x2F;api&#x2F;publicMatches -o .&#x2F;data&#x2F;OpenDotaPublic.json</span><br></pre></td></tr></table></figure>\n<p>Yeah! I kinda like Dota so this makes sense :D</p>\n</li>\n<li><p>Create a new NoteBook in the web Interface and use the following code</p>\n</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%spark</span><br><span class=\"line\"></span><br><span class=\"line\">val df &#x3D; sqlContext.read.json(&quot;file:&#x2F;&#x2F;&#x2F;opt&#x2F;data&#x2F;OpenDotaPublic.json&quot;)</span><br><span class=\"line\">df.show</span><br></pre></td></tr></table></figure>\n<p>Hit: <strong>Shift-Enter</strong></p>\n<ol start=\"3\">\n<li>Let’s register this dataframe as temp table and create some visuals</li>\n</ol>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%spark</span><br><span class=\"line\">df.registerTempTable(&quot;publicmatches&quot;)</span><br></pre></td></tr></table></figure>\n<ol start=\"4\">\n<li>Create the following to generate visualizations<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">%sql</span><br><span class=\"line\">select radiant_win,match_id</span><br><span class=\"line\"> from publicmatches</span><br></pre></td></tr></table></figure>\n\n</li>\n</ol>\n<img src=\"/2017/09/19/quickzeppelin/radiant_win.png\" class=\"\" title=\"[PieChart]\">\n\n<p>Guess i need to start playing on Radiant side :D</p>\n<p>Well and that’s it.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://zeppelin.apache.org/docs/0.5.5-incubating/tutorial/tutorial.html\">https://zeppelin.apache.org/docs/0.5.5-incubating/tutorial/tutorial.html</a></li>\n<li><a href=\"https://hortonworks.com/tutorial/getting-started-with-apache-zeppelin/\">https://hortonworks.com/tutorial/getting-started-with-apache-zeppelin/</a></li>\n<li><a href=\"https://www.digitalocean.com/community/tutorials/how-to-install-docker-compose-on-ubuntu-16-04\">https://www.digitalocean.com/community/tutorials/how-to-install-docker-compose-on-ubuntu-16-04</a></li>\n<li><a href=\"https://docs.docker.com/compose\">https://docs.docker.com/compose</a></li>\n<li><a href=\"https://zeppelin.apache.org/\">https://zeppelin.apache.org/</a></li>\n</ul>\n"},{"title":"RDD Basic Transformations Operations","date":"2017-10-09T19:54:12.000Z","_content":"\n\nJust a simple Basic CheatSheet on Spark RDD's\n\n* Basic Transformations on a RDD containing: **{1,2,3,3}**\n\n| Function Name | Example                   | Result           |\n|---------------|---------------------------|------------------|\n| map()         | rdd.map(x => x +1)        | {2,3,4,4}        |\n| flatmap()     | rdd.flatMap(x => x.to(3)) | {1,2,3,2,3,3,3}  |\n| filter()      | rdd.filter(x => x != 1 )  | {2,3,3}          |\n| distinct()    | rdd.distinct()            | {1,2,3}          |\n| sample()      | rdd.sample(false,0.5)     | Nondeterministic |\n\n* Basic two-RDD transformations on RDDs: **{1,2,3}** and **{3,4,5}**\n\n| Function Name | Example                   | Result               |\n|---------------|---------------------------|----------------------|\n| union()       | rdd.union(other)          | {1,2,3,3,4,5}        |\n| intersection()| rdd.intersection(other)   | {3}                  |\n| subtract()    | rdd.subtract(other)       | {1,2}                |\n| cartesian()   | rdd.cartesian(other)      | {(1,3),(1,4)...(3,5)}|\n\n* Basic Actions on RDD containing: **{1,2,3,3}**\n\n| Function Name | Example                   | Result              |\n|---------------|---------------------------|---------------------|\n| collect()     | rdd.collect()             | {1,2,3,3}           |\n| count()       | rdd.count()               | 4                   |\n| countByValue()| rdd.countByValue()        | {(1,1),(2,1),(3,2)} |\n| take()        | rdd.take(2)               | {1,2}               |\n| top()         | rdd.top(2)                | {3,3}               |\n| takeOrdered() | rdd.takeOrdered(2)(myOrdering)| {3,3}           |\n| takeSample()  | rdd.takeSample(false,1)   | Nondeterministic    |\n| reduce()      | rdd.reduce((x,y) => x + y )| 9                  |\n| fold()        | rdd.fold(0)((x,y) => x + y)| 9                  |\n| foreach()     | rdd.foreach(func)          | Nothing            |\n\n\n\n\n\n\n\n\n\n","source":"_posts/rddstransformations.md","raw":"---\ntitle: RDD Basic Transformations Operations\ntags:\n  - Spark\n  - RDD\n  - CheatSheet\ndate: 2017-10-09 20:54:12\n---\n\n\nJust a simple Basic CheatSheet on Spark RDD's\n\n* Basic Transformations on a RDD containing: **{1,2,3,3}**\n\n| Function Name | Example                   | Result           |\n|---------------|---------------------------|------------------|\n| map()         | rdd.map(x => x +1)        | {2,3,4,4}        |\n| flatmap()     | rdd.flatMap(x => x.to(3)) | {1,2,3,2,3,3,3}  |\n| filter()      | rdd.filter(x => x != 1 )  | {2,3,3}          |\n| distinct()    | rdd.distinct()            | {1,2,3}          |\n| sample()      | rdd.sample(false,0.5)     | Nondeterministic |\n\n* Basic two-RDD transformations on RDDs: **{1,2,3}** and **{3,4,5}**\n\n| Function Name | Example                   | Result               |\n|---------------|---------------------------|----------------------|\n| union()       | rdd.union(other)          | {1,2,3,3,4,5}        |\n| intersection()| rdd.intersection(other)   | {3}                  |\n| subtract()    | rdd.subtract(other)       | {1,2}                |\n| cartesian()   | rdd.cartesian(other)      | {(1,3),(1,4)...(3,5)}|\n\n* Basic Actions on RDD containing: **{1,2,3,3}**\n\n| Function Name | Example                   | Result              |\n|---------------|---------------------------|---------------------|\n| collect()     | rdd.collect()             | {1,2,3,3}           |\n| count()       | rdd.count()               | 4                   |\n| countByValue()| rdd.countByValue()        | {(1,1),(2,1),(3,2)} |\n| take()        | rdd.take(2)               | {1,2}               |\n| top()         | rdd.top(2)                | {3,3}               |\n| takeOrdered() | rdd.takeOrdered(2)(myOrdering)| {3,3}           |\n| takeSample()  | rdd.takeSample(false,1)   | Nondeterministic    |\n| reduce()      | rdd.reduce((x,y) => x + y )| 9                  |\n| fold()        | rdd.fold(0)((x,y) => x + y)| 9                  |\n| foreach()     | rdd.foreach(func)          | Nothing            |\n\n\n\n\n\n\n\n\n\n","slug":"rddstransformations","published":1,"updated":"2020-09-06T14:38:37.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxjv000pchpf9fo50hqx","content":"<p>Just a simple Basic CheatSheet on Spark RDD’s</p>\n<ul>\n<li>Basic Transformations on a RDD containing: <strong>{1,2,3,3}</strong></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n<th>Example</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>map()</td>\n<td>rdd.map(x =&gt; x +1)</td>\n<td>{2,3,4,4}</td>\n</tr>\n<tr>\n<td>flatmap()</td>\n<td>rdd.flatMap(x =&gt; x.to(3))</td>\n<td>{1,2,3,2,3,3,3}</td>\n</tr>\n<tr>\n<td>filter()</td>\n<td>rdd.filter(x =&gt; x != 1 )</td>\n<td>{2,3,3}</td>\n</tr>\n<tr>\n<td>distinct()</td>\n<td>rdd.distinct()</td>\n<td>{1,2,3}</td>\n</tr>\n<tr>\n<td>sample()</td>\n<td>rdd.sample(false,0.5)</td>\n<td>Nondeterministic</td>\n</tr>\n</tbody></table>\n<ul>\n<li>Basic two-RDD transformations on RDDs: <strong>{1,2,3}</strong> and <strong>{3,4,5}</strong></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n<th>Example</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>union()</td>\n<td>rdd.union(other)</td>\n<td>{1,2,3,3,4,5}</td>\n</tr>\n<tr>\n<td>intersection()</td>\n<td>rdd.intersection(other)</td>\n<td>{3}</td>\n</tr>\n<tr>\n<td>subtract()</td>\n<td>rdd.subtract(other)</td>\n<td>{1,2}</td>\n</tr>\n<tr>\n<td>cartesian()</td>\n<td>rdd.cartesian(other)</td>\n<td>{(1,3),(1,4)…(3,5)}</td>\n</tr>\n</tbody></table>\n<ul>\n<li>Basic Actions on RDD containing: <strong>{1,2,3,3}</strong></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n<th>Example</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>collect()</td>\n<td>rdd.collect()</td>\n<td>{1,2,3,3}</td>\n</tr>\n<tr>\n<td>count()</td>\n<td>rdd.count()</td>\n<td>4</td>\n</tr>\n<tr>\n<td>countByValue()</td>\n<td>rdd.countByValue()</td>\n<td>{(1,1),(2,1),(3,2)}</td>\n</tr>\n<tr>\n<td>take()</td>\n<td>rdd.take(2)</td>\n<td>{1,2}</td>\n</tr>\n<tr>\n<td>top()</td>\n<td>rdd.top(2)</td>\n<td>{3,3}</td>\n</tr>\n<tr>\n<td>takeOrdered()</td>\n<td>rdd.takeOrdered(2)(myOrdering)</td>\n<td>{3,3}</td>\n</tr>\n<tr>\n<td>takeSample()</td>\n<td>rdd.takeSample(false,1)</td>\n<td>Nondeterministic</td>\n</tr>\n<tr>\n<td>reduce()</td>\n<td>rdd.reduce((x,y) =&gt; x + y )</td>\n<td>9</td>\n</tr>\n<tr>\n<td>fold()</td>\n<td>rdd.fold(0)((x,y) =&gt; x + y)</td>\n<td>9</td>\n</tr>\n<tr>\n<td>foreach()</td>\n<td>rdd.foreach(func)</td>\n<td>Nothing</td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"excerpt":"","more":"<p>Just a simple Basic CheatSheet on Spark RDD’s</p>\n<ul>\n<li>Basic Transformations on a RDD containing: <strong>{1,2,3,3}</strong></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n<th>Example</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>map()</td>\n<td>rdd.map(x =&gt; x +1)</td>\n<td>{2,3,4,4}</td>\n</tr>\n<tr>\n<td>flatmap()</td>\n<td>rdd.flatMap(x =&gt; x.to(3))</td>\n<td>{1,2,3,2,3,3,3}</td>\n</tr>\n<tr>\n<td>filter()</td>\n<td>rdd.filter(x =&gt; x != 1 )</td>\n<td>{2,3,3}</td>\n</tr>\n<tr>\n<td>distinct()</td>\n<td>rdd.distinct()</td>\n<td>{1,2,3}</td>\n</tr>\n<tr>\n<td>sample()</td>\n<td>rdd.sample(false,0.5)</td>\n<td>Nondeterministic</td>\n</tr>\n</tbody></table>\n<ul>\n<li>Basic two-RDD transformations on RDDs: <strong>{1,2,3}</strong> and <strong>{3,4,5}</strong></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n<th>Example</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>union()</td>\n<td>rdd.union(other)</td>\n<td>{1,2,3,3,4,5}</td>\n</tr>\n<tr>\n<td>intersection()</td>\n<td>rdd.intersection(other)</td>\n<td>{3}</td>\n</tr>\n<tr>\n<td>subtract()</td>\n<td>rdd.subtract(other)</td>\n<td>{1,2}</td>\n</tr>\n<tr>\n<td>cartesian()</td>\n<td>rdd.cartesian(other)</td>\n<td>{(1,3),(1,4)…(3,5)}</td>\n</tr>\n</tbody></table>\n<ul>\n<li>Basic Actions on RDD containing: <strong>{1,2,3,3}</strong></li>\n</ul>\n<table>\n<thead>\n<tr>\n<th>Function Name</th>\n<th>Example</th>\n<th>Result</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>collect()</td>\n<td>rdd.collect()</td>\n<td>{1,2,3,3}</td>\n</tr>\n<tr>\n<td>count()</td>\n<td>rdd.count()</td>\n<td>4</td>\n</tr>\n<tr>\n<td>countByValue()</td>\n<td>rdd.countByValue()</td>\n<td>{(1,1),(2,1),(3,2)}</td>\n</tr>\n<tr>\n<td>take()</td>\n<td>rdd.take(2)</td>\n<td>{1,2}</td>\n</tr>\n<tr>\n<td>top()</td>\n<td>rdd.top(2)</td>\n<td>{3,3}</td>\n</tr>\n<tr>\n<td>takeOrdered()</td>\n<td>rdd.takeOrdered(2)(myOrdering)</td>\n<td>{3,3}</td>\n</tr>\n<tr>\n<td>takeSample()</td>\n<td>rdd.takeSample(false,1)</td>\n<td>Nondeterministic</td>\n</tr>\n<tr>\n<td>reduce()</td>\n<td>rdd.reduce((x,y) =&gt; x + y )</td>\n<td>9</td>\n</tr>\n<tr>\n<td>fold()</td>\n<td>rdd.fold(0)((x,y) =&gt; x + y)</td>\n<td>9</td>\n</tr>\n<tr>\n<td>foreach()</td>\n<td>rdd.foreach(func)</td>\n<td>Nothing</td>\n</tr>\n</tbody></table>\n"},{"title":"Optimize Your SSH Connections With SSH Config File","date":"2017-09-13T22:13:24.000Z","_content":"\n\nIf most of your work is done remotely though ssh, and you have to access several environments, there will come a time where you need to organize our connection settings. Which user you need to access server X or which port is configured or the case you work in consulting and have several ssh_keys.\n\nThere are some tools that migth help on this, but i'm old school and still stick with plain ssh command.\n\nssh config file helps quite a lot, here are some tips unknown to some:\n\n## Alias\n\nLet's say you want to take advantage of tab auto-completion when using your connections for a enviroment like\n\n```\n├── ClientA\n│   ├── server1\n│   └── server2\n├── ClientB\n│   └── server1\n├── DEV\n│   ├── BackOffice\n│   │   ├── server1\n│   │   └── server2\n│   └── FrontOffice\n│       ├── server1\n│       ├── server2\n│       └── server3\n├── LIVE\n│   ├── Cluster\n│   │   ├── node01\n│   │   ├── node02\n│   │   └── node03\n│   └── Servers\n│       ├── server01\n│       └── server02\n└── QA\n    └── server1\n```\n\nthis would be quicker to do something like `ssh DEV` **TAB** `Back` **TAB** `server1`\n\nthat's actually possible with ssh_config alias.\n\nAdd the following to `~/.ssh/config` to see this in action\n\n```\nHost ClientA.server1\n     Hostname localhost\nHost ClientA.server2\n     Hostname localhost\nHost ClientB.server1\n     Hostname localhost\nHost ClientA.server1\n     Hostname localhost\nHost DEV.BackOffice.server1\n     Hostname localhost\nHost DEV.BackOffice.server2\n     Hostname localhost\nHost DEV.FrontOffice.server1\n     Hostname localhost\nHost DEV.FrontOffice.server2\n     Hostname localhost\nHost DEV.FrontOffice.server3\n     Hostname localhost\nHost LIVE.Cluster.node01\n     Hostname localhost\nHost LIVE.Cluster.node02\n     Hostname localhost\nHost LIVE.Cluster.node03\n     Hostname localhost\nHost LIVE.Servers.server01\n     Hostname localhost\nHost LIVE.Servers.server02\n     Hostname localhost\nHost QA.server01\n     Hostname localhost\n```\n\nNow try out the power of Tab-autocompletion. this is just an example of a type of structure you could use. \n\nYou could also add alias like\n\n```\nHost LIVE.Servers.server02 server01.mydomain.com\n     Hostname localhost\n```\n\nSo that both ssh attempts to `LIVE.Servers.server02` and `server01.mydomain.com` would use the same configuration.\n\n## Access customizations\n\nNo let's say for accessing `LIVE.Servers.server01` you require account `admin` and ssh listens on port `2228` . one could setup the following\n\n```\nHost LIVE.Servers.server01\n     Hostname localhost\n     Port 2228\n     User admin\n```\n\nWith this configuration one could simple execute `ssh LIVE.Servers.server01` and it will use the configured user and port in the connection.\n\nOr if you have a specific ssh_key for it in `QA`\n\n```\nHost QA.server01\n     Hostname localhost\n     Port 2227\n     User qa_user01\n     IdentityFile ~/.ssh/id_rsa_qa\n```\n\n## Tunnels\n\none could also setup tunnels directly in ssh_config like\n\n```\nHost tunnel\n    HostName database.example.com\n    IdentityFile ~/.ssh/id_rsa_dev\n    LocalForward 9906 127.0.0.1:3306\n    User dba    \n```\n\nYou can simple execute `ssh -f -N tunnel`\n\nOr if you have access to server3 only from server1\n\n```\nHost DEV.FrontOffice.server3\n     Hostname 10.0.0.1\n     Port 22\n     User admin\n     IdentityFile ~/.ssh/id_rsa_dev\n     ProxyCommand ssh -q -W %h:%p DEV.FrontOffice.server1\n```\n\nOne configuration i normally use in development with containers or virtual machines wich are deprovision with regularity is the following:\n\n```\nHost 192.168.77.* mesos-*\n   StrictHostKeyChecking no\n   User rramos\n   IdentityFile ~/.ssh/id_rsa\n   UserKnownHostsFile=/dev/null\n   LogLevel QUIET\n```\n\nThis means any ssh connection to local network `192.168.77.*` or hosts with name `mesos-*` wont get registered in KnownHosts.\n\nYou could also use this to change your settings for `TCPKeepAlive` or any other specific connections settings you may need the [man](https://linux.die.net/man/5/ssh_config) page as the full list of options.\n\nCheers,\nRR\n\n\n# References\n\n* https://linux.die.net/man/5/ssh_config","source":"_posts/ssh-config.md","raw":"---\ntitle: Optimize your SSH Connections with SSH config File\ntags:\n  - Linux\n  - ssh\ndate: 2017-09-13 23:13:24\n---\n\n\nIf most of your work is done remotely though ssh, and you have to access several environments, there will come a time where you need to organize our connection settings. Which user you need to access server X or which port is configured or the case you work in consulting and have several ssh_keys.\n\nThere are some tools that migth help on this, but i'm old school and still stick with plain ssh command.\n\nssh config file helps quite a lot, here are some tips unknown to some:\n\n## Alias\n\nLet's say you want to take advantage of tab auto-completion when using your connections for a enviroment like\n\n```\n├── ClientA\n│   ├── server1\n│   └── server2\n├── ClientB\n│   └── server1\n├── DEV\n│   ├── BackOffice\n│   │   ├── server1\n│   │   └── server2\n│   └── FrontOffice\n│       ├── server1\n│       ├── server2\n│       └── server3\n├── LIVE\n│   ├── Cluster\n│   │   ├── node01\n│   │   ├── node02\n│   │   └── node03\n│   └── Servers\n│       ├── server01\n│       └── server02\n└── QA\n    └── server1\n```\n\nthis would be quicker to do something like `ssh DEV` **TAB** `Back` **TAB** `server1`\n\nthat's actually possible with ssh_config alias.\n\nAdd the following to `~/.ssh/config` to see this in action\n\n```\nHost ClientA.server1\n     Hostname localhost\nHost ClientA.server2\n     Hostname localhost\nHost ClientB.server1\n     Hostname localhost\nHost ClientA.server1\n     Hostname localhost\nHost DEV.BackOffice.server1\n     Hostname localhost\nHost DEV.BackOffice.server2\n     Hostname localhost\nHost DEV.FrontOffice.server1\n     Hostname localhost\nHost DEV.FrontOffice.server2\n     Hostname localhost\nHost DEV.FrontOffice.server3\n     Hostname localhost\nHost LIVE.Cluster.node01\n     Hostname localhost\nHost LIVE.Cluster.node02\n     Hostname localhost\nHost LIVE.Cluster.node03\n     Hostname localhost\nHost LIVE.Servers.server01\n     Hostname localhost\nHost LIVE.Servers.server02\n     Hostname localhost\nHost QA.server01\n     Hostname localhost\n```\n\nNow try out the power of Tab-autocompletion. this is just an example of a type of structure you could use. \n\nYou could also add alias like\n\n```\nHost LIVE.Servers.server02 server01.mydomain.com\n     Hostname localhost\n```\n\nSo that both ssh attempts to `LIVE.Servers.server02` and `server01.mydomain.com` would use the same configuration.\n\n## Access customizations\n\nNo let's say for accessing `LIVE.Servers.server01` you require account `admin` and ssh listens on port `2228` . one could setup the following\n\n```\nHost LIVE.Servers.server01\n     Hostname localhost\n     Port 2228\n     User admin\n```\n\nWith this configuration one could simple execute `ssh LIVE.Servers.server01` and it will use the configured user and port in the connection.\n\nOr if you have a specific ssh_key for it in `QA`\n\n```\nHost QA.server01\n     Hostname localhost\n     Port 2227\n     User qa_user01\n     IdentityFile ~/.ssh/id_rsa_qa\n```\n\n## Tunnels\n\none could also setup tunnels directly in ssh_config like\n\n```\nHost tunnel\n    HostName database.example.com\n    IdentityFile ~/.ssh/id_rsa_dev\n    LocalForward 9906 127.0.0.1:3306\n    User dba    \n```\n\nYou can simple execute `ssh -f -N tunnel`\n\nOr if you have access to server3 only from server1\n\n```\nHost DEV.FrontOffice.server3\n     Hostname 10.0.0.1\n     Port 22\n     User admin\n     IdentityFile ~/.ssh/id_rsa_dev\n     ProxyCommand ssh -q -W %h:%p DEV.FrontOffice.server1\n```\n\nOne configuration i normally use in development with containers or virtual machines wich are deprovision with regularity is the following:\n\n```\nHost 192.168.77.* mesos-*\n   StrictHostKeyChecking no\n   User rramos\n   IdentityFile ~/.ssh/id_rsa\n   UserKnownHostsFile=/dev/null\n   LogLevel QUIET\n```\n\nThis means any ssh connection to local network `192.168.77.*` or hosts with name `mesos-*` wont get registered in KnownHosts.\n\nYou could also use this to change your settings for `TCPKeepAlive` or any other specific connections settings you may need the [man](https://linux.die.net/man/5/ssh_config) page as the full list of options.\n\nCheers,\nRR\n\n\n# References\n\n* https://linux.die.net/man/5/ssh_config","slug":"ssh-config","published":1,"updated":"2020-09-06T14:38:37.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxkd000qchpfcntu1oau","content":"<p>If most of your work is done remotely though ssh, and you have to access several environments, there will come a time where you need to organize our connection settings. Which user you need to access server X or which port is configured or the case you work in consulting and have several ssh_keys.</p>\n<p>There are some tools that migth help on this, but i’m old school and still stick with plain ssh command.</p>\n<p>ssh config file helps quite a lot, here are some tips unknown to some:</p>\n<h2 id=\"Alias\"><a href=\"#Alias\" class=\"headerlink\" title=\"Alias\"></a>Alias</h2><p>Let’s say you want to take advantage of tab auto-completion when using your connections for a enviroment like</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">├── ClientA</span><br><span class=\"line\">│   ├── server1</span><br><span class=\"line\">│   └── server2</span><br><span class=\"line\">├── ClientB</span><br><span class=\"line\">│   └── server1</span><br><span class=\"line\">├── DEV</span><br><span class=\"line\">│   ├── BackOffice</span><br><span class=\"line\">│   │   ├── server1</span><br><span class=\"line\">│   │   └── server2</span><br><span class=\"line\">│   └── FrontOffice</span><br><span class=\"line\">│       ├── server1</span><br><span class=\"line\">│       ├── server2</span><br><span class=\"line\">│       └── server3</span><br><span class=\"line\">├── LIVE</span><br><span class=\"line\">│   ├── Cluster</span><br><span class=\"line\">│   │   ├── node01</span><br><span class=\"line\">│   │   ├── node02</span><br><span class=\"line\">│   │   └── node03</span><br><span class=\"line\">│   └── Servers</span><br><span class=\"line\">│       ├── server01</span><br><span class=\"line\">│       └── server02</span><br><span class=\"line\">└── QA</span><br><span class=\"line\">    └── server1</span><br></pre></td></tr></table></figure>\n<p>this would be quicker to do something like <code>ssh DEV</code> <strong>TAB</strong> <code>Back</code> <strong>TAB</strong> <code>server1</code></p>\n<p>that’s actually possible with ssh_config alias.</p>\n<p>Add the following to <code>~/.ssh/config</code> to see this in action</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host ClientA.server1</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host ClientA.server2</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host ClientB.server1</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host ClientA.server1</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host DEV.BackOffice.server1</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host DEV.BackOffice.server2</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host DEV.FrontOffice.server1</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host DEV.FrontOffice.server2</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host DEV.FrontOffice.server3</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host LIVE.Cluster.node01</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host LIVE.Cluster.node02</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host LIVE.Cluster.node03</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host LIVE.Servers.server01</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host LIVE.Servers.server02</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host QA.server01</span><br><span class=\"line\">     Hostname localhost</span><br></pre></td></tr></table></figure>\n<p>Now try out the power of Tab-autocompletion. this is just an example of a type of structure you could use. </p>\n<p>You could also add alias like</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host LIVE.Servers.server02 server01.mydomain.com</span><br><span class=\"line\">     Hostname localhost</span><br></pre></td></tr></table></figure>\n<p>So that both ssh attempts to <code>LIVE.Servers.server02</code> and <code>server01.mydomain.com</code> would use the same configuration.</p>\n<h2 id=\"Access-customizations\"><a href=\"#Access-customizations\" class=\"headerlink\" title=\"Access customizations\"></a>Access customizations</h2><p>No let’s say for accessing <code>LIVE.Servers.server01</code> you require account <code>admin</code> and ssh listens on port <code>2228</code> . one could setup the following</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host LIVE.Servers.server01</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">     Port 2228</span><br><span class=\"line\">     User admin</span><br></pre></td></tr></table></figure>\n<p>With this configuration one could simple execute <code>ssh LIVE.Servers.server01</code> and it will use the configured user and port in the connection.</p>\n<p>Or if you have a specific ssh_key for it in <code>QA</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host QA.server01</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">     Port 2227</span><br><span class=\"line\">     User qa_user01</span><br><span class=\"line\">     IdentityFile ~&#x2F;.ssh&#x2F;id_rsa_qa</span><br></pre></td></tr></table></figure>\n<h2 id=\"Tunnels\"><a href=\"#Tunnels\" class=\"headerlink\" title=\"Tunnels\"></a>Tunnels</h2><p>one could also setup tunnels directly in ssh_config like</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host tunnel</span><br><span class=\"line\">    HostName database.example.com</span><br><span class=\"line\">    IdentityFile ~&#x2F;.ssh&#x2F;id_rsa_dev</span><br><span class=\"line\">    LocalForward 9906 127.0.0.1:3306</span><br><span class=\"line\">    User dba    </span><br></pre></td></tr></table></figure>\n<p>You can simple execute <code>ssh -f -N tunnel</code></p>\n<p>Or if you have access to server3 only from server1</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host DEV.FrontOffice.server3</span><br><span class=\"line\">     Hostname 10.0.0.1</span><br><span class=\"line\">     Port 22</span><br><span class=\"line\">     User admin</span><br><span class=\"line\">     IdentityFile ~&#x2F;.ssh&#x2F;id_rsa_dev</span><br><span class=\"line\">     ProxyCommand ssh -q -W %h:%p DEV.FrontOffice.server1</span><br></pre></td></tr></table></figure>\n<p>One configuration i normally use in development with containers or virtual machines wich are deprovision with regularity is the following:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host 192.168.77.* mesos-*</span><br><span class=\"line\">   StrictHostKeyChecking no</span><br><span class=\"line\">   User rramos</span><br><span class=\"line\">   IdentityFile ~&#x2F;.ssh&#x2F;id_rsa</span><br><span class=\"line\">   UserKnownHostsFile&#x3D;&#x2F;dev&#x2F;null</span><br><span class=\"line\">   LogLevel QUIET</span><br></pre></td></tr></table></figure>\n<p>This means any ssh connection to local network <code>192.168.77.*</code> or hosts with name <code>mesos-*</code> wont get registered in KnownHosts.</p>\n<p>You could also use this to change your settings for <code>TCPKeepAlive</code> or any other specific connections settings you may need the <a href=\"https://linux.die.net/man/5/ssh_config\">man</a> page as the full list of options.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://linux.die.net/man/5/ssh_config\">https://linux.die.net/man/5/ssh_config</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>If most of your work is done remotely though ssh, and you have to access several environments, there will come a time where you need to organize our connection settings. Which user you need to access server X or which port is configured or the case you work in consulting and have several ssh_keys.</p>\n<p>There are some tools that migth help on this, but i’m old school and still stick with plain ssh command.</p>\n<p>ssh config file helps quite a lot, here are some tips unknown to some:</p>\n<h2 id=\"Alias\"><a href=\"#Alias\" class=\"headerlink\" title=\"Alias\"></a>Alias</h2><p>Let’s say you want to take advantage of tab auto-completion when using your connections for a enviroment like</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">├── ClientA</span><br><span class=\"line\">│   ├── server1</span><br><span class=\"line\">│   └── server2</span><br><span class=\"line\">├── ClientB</span><br><span class=\"line\">│   └── server1</span><br><span class=\"line\">├── DEV</span><br><span class=\"line\">│   ├── BackOffice</span><br><span class=\"line\">│   │   ├── server1</span><br><span class=\"line\">│   │   └── server2</span><br><span class=\"line\">│   └── FrontOffice</span><br><span class=\"line\">│       ├── server1</span><br><span class=\"line\">│       ├── server2</span><br><span class=\"line\">│       └── server3</span><br><span class=\"line\">├── LIVE</span><br><span class=\"line\">│   ├── Cluster</span><br><span class=\"line\">│   │   ├── node01</span><br><span class=\"line\">│   │   ├── node02</span><br><span class=\"line\">│   │   └── node03</span><br><span class=\"line\">│   └── Servers</span><br><span class=\"line\">│       ├── server01</span><br><span class=\"line\">│       └── server02</span><br><span class=\"line\">└── QA</span><br><span class=\"line\">    └── server1</span><br></pre></td></tr></table></figure>\n<p>this would be quicker to do something like <code>ssh DEV</code> <strong>TAB</strong> <code>Back</code> <strong>TAB</strong> <code>server1</code></p>\n<p>that’s actually possible with ssh_config alias.</p>\n<p>Add the following to <code>~/.ssh/config</code> to see this in action</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host ClientA.server1</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host ClientA.server2</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host ClientB.server1</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host ClientA.server1</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host DEV.BackOffice.server1</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host DEV.BackOffice.server2</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host DEV.FrontOffice.server1</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host DEV.FrontOffice.server2</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host DEV.FrontOffice.server3</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host LIVE.Cluster.node01</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host LIVE.Cluster.node02</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host LIVE.Cluster.node03</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host LIVE.Servers.server01</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host LIVE.Servers.server02</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">Host QA.server01</span><br><span class=\"line\">     Hostname localhost</span><br></pre></td></tr></table></figure>\n<p>Now try out the power of Tab-autocompletion. this is just an example of a type of structure you could use. </p>\n<p>You could also add alias like</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host LIVE.Servers.server02 server01.mydomain.com</span><br><span class=\"line\">     Hostname localhost</span><br></pre></td></tr></table></figure>\n<p>So that both ssh attempts to <code>LIVE.Servers.server02</code> and <code>server01.mydomain.com</code> would use the same configuration.</p>\n<h2 id=\"Access-customizations\"><a href=\"#Access-customizations\" class=\"headerlink\" title=\"Access customizations\"></a>Access customizations</h2><p>No let’s say for accessing <code>LIVE.Servers.server01</code> you require account <code>admin</code> and ssh listens on port <code>2228</code> . one could setup the following</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host LIVE.Servers.server01</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">     Port 2228</span><br><span class=\"line\">     User admin</span><br></pre></td></tr></table></figure>\n<p>With this configuration one could simple execute <code>ssh LIVE.Servers.server01</code> and it will use the configured user and port in the connection.</p>\n<p>Or if you have a specific ssh_key for it in <code>QA</code></p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host QA.server01</span><br><span class=\"line\">     Hostname localhost</span><br><span class=\"line\">     Port 2227</span><br><span class=\"line\">     User qa_user01</span><br><span class=\"line\">     IdentityFile ~&#x2F;.ssh&#x2F;id_rsa_qa</span><br></pre></td></tr></table></figure>\n<h2 id=\"Tunnels\"><a href=\"#Tunnels\" class=\"headerlink\" title=\"Tunnels\"></a>Tunnels</h2><p>one could also setup tunnels directly in ssh_config like</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host tunnel</span><br><span class=\"line\">    HostName database.example.com</span><br><span class=\"line\">    IdentityFile ~&#x2F;.ssh&#x2F;id_rsa_dev</span><br><span class=\"line\">    LocalForward 9906 127.0.0.1:3306</span><br><span class=\"line\">    User dba    </span><br></pre></td></tr></table></figure>\n<p>You can simple execute <code>ssh -f -N tunnel</code></p>\n<p>Or if you have access to server3 only from server1</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host DEV.FrontOffice.server3</span><br><span class=\"line\">     Hostname 10.0.0.1</span><br><span class=\"line\">     Port 22</span><br><span class=\"line\">     User admin</span><br><span class=\"line\">     IdentityFile ~&#x2F;.ssh&#x2F;id_rsa_dev</span><br><span class=\"line\">     ProxyCommand ssh -q -W %h:%p DEV.FrontOffice.server1</span><br></pre></td></tr></table></figure>\n<p>One configuration i normally use in development with containers or virtual machines wich are deprovision with regularity is the following:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Host 192.168.77.* mesos-*</span><br><span class=\"line\">   StrictHostKeyChecking no</span><br><span class=\"line\">   User rramos</span><br><span class=\"line\">   IdentityFile ~&#x2F;.ssh&#x2F;id_rsa</span><br><span class=\"line\">   UserKnownHostsFile&#x3D;&#x2F;dev&#x2F;null</span><br><span class=\"line\">   LogLevel QUIET</span><br></pre></td></tr></table></figure>\n<p>This means any ssh connection to local network <code>192.168.77.*</code> or hosts with name <code>mesos-*</code> wont get registered in KnownHosts.</p>\n<p>You could also use this to change your settings for <code>TCPKeepAlive</code> or any other specific connections settings you may need the <a href=\"https://linux.die.net/man/5/ssh_config\">man</a> page as the full list of options.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://linux.die.net/man/5/ssh_config\">https://linux.die.net/man/5/ssh_config</a></li>\n</ul>\n"},{"title":"Sublime Install Guide","date":"2017-09-23T08:18:18.000Z","_content":"\n\nFor me [Sublime](https://www.sublimetext.com/), this is one of the best smart editors in Linux. Here are the quick instructions to install for Ubuntu.\n\n```\nwget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -\necho \"deb https://download.sublimetext.com/ apt/stable/\" | sudo tee /etc/apt/sources.list.d/sublime-text.list\nsudo apt-get install apt-transport-https\nsudo apt-get update\nsudo apt-get install sublime-text\n\n```\n\n\nThat's it\nRR","source":"_posts/sublimeinstall.md","raw":"---\ntitle: Sublime Install Guide\ntags:\n  - IDE\n  - Install\n  - Linux\ndate: 2017-09-23 09:18:18\n---\n\n\nFor me [Sublime](https://www.sublimetext.com/), this is one of the best smart editors in Linux. Here are the quick instructions to install for Ubuntu.\n\n```\nwget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -\necho \"deb https://download.sublimetext.com/ apt/stable/\" | sudo tee /etc/apt/sources.list.d/sublime-text.list\nsudo apt-get install apt-transport-https\nsudo apt-get update\nsudo apt-get install sublime-text\n\n```\n\n\nThat's it\nRR","slug":"sublimeinstall","published":1,"updated":"2020-09-06T14:38:37.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxko000schpf2ulcgslz","content":"<p>For me <a href=\"https://www.sublimetext.com/\">Sublime</a>, this is one of the best smart editors in Linux. Here are the quick instructions to install for Ubuntu.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget -qO - https:&#x2F;&#x2F;download.sublimetext.com&#x2F;sublimehq-pub.gpg | sudo apt-key add -</span><br><span class=\"line\">echo &quot;deb https:&#x2F;&#x2F;download.sublimetext.com&#x2F; apt&#x2F;stable&#x2F;&quot; | sudo tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;sublime-text.list</span><br><span class=\"line\">sudo apt-get install apt-transport-https</span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get install sublime-text</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>That’s it<br>RR</p>\n","site":{"data":{}},"excerpt":"","more":"<p>For me <a href=\"https://www.sublimetext.com/\">Sublime</a>, this is one of the best smart editors in Linux. Here are the quick instructions to install for Ubuntu.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">wget -qO - https:&#x2F;&#x2F;download.sublimetext.com&#x2F;sublimehq-pub.gpg | sudo apt-key add -</span><br><span class=\"line\">echo &quot;deb https:&#x2F;&#x2F;download.sublimetext.com&#x2F; apt&#x2F;stable&#x2F;&quot; | sudo tee &#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;sublime-text.list</span><br><span class=\"line\">sudo apt-get install apt-transport-https</span><br><span class=\"line\">sudo apt-get update</span><br><span class=\"line\">sudo apt-get install sublime-text</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n\n<p>That’s it<br>RR</p>\n"},{"title":"Hive - Unpivot to Avoid Multiple Joins","date":"2017-09-07T16:57:08.000Z","_content":"\n\nIn Hive, when a table has multiple similar columns, such as:\n\n| product_id | image_name1 | image_name2 |... | image_name15 |\n|------------|-------------|-------------|----|--------------|\n|1           | a.jpg       | b.jpg       |... | c.jpg        |\n|2           | d.jpg       | e.jpg       |... | f.jpg        |\n\nand we want to join based on all image_names, the naive approach would be to perform 15 joins (in the running example).\n\nHive does not have an unpivot functionality, but it is possible to use the Hive builtin UDF explode to accomplish this:\nQuery example\n\n```\nselect x.product_id, x.image_name from (\n  select product_id, map(\"image_name1\", image_name1, ..., \"image_name15\", image_name15) as image_map\n  from table ) x\nlateral view explode(image_map) expl as image_nr, image_name\n```\n\n\nThis returns an unpivoted table like below, which allows us to perform a single join:\n\n|product_id| image_name|\n|----------|-----------|\n|1         | a.jpg     |\n|1         | b.jpg     |\n|1         | c.jpg     |\n|2         | d.jpg     | \n|2         | e.jpg     |\n|2         | f.jpg     |\n\nBig thanks to Diogo Franco, for this hint ;) check also is Blog Page [diogoalexandrefranco.github.io](https://diogoalexandrefranco.github.io)\n\n\n","source":"_posts/unpivot.md","raw":"---\ntitle: Hive - Unpivot to avoid multiple joins\ndate: 2017-09-07 17:57:08\ntags: \n  - HDFS\n  - Hive\n  - Optimization\n  - BigData\n---\n\n\nIn Hive, when a table has multiple similar columns, such as:\n\n| product_id | image_name1 | image_name2 |... | image_name15 |\n|------------|-------------|-------------|----|--------------|\n|1           | a.jpg       | b.jpg       |... | c.jpg        |\n|2           | d.jpg       | e.jpg       |... | f.jpg        |\n\nand we want to join based on all image_names, the naive approach would be to perform 15 joins (in the running example).\n\nHive does not have an unpivot functionality, but it is possible to use the Hive builtin UDF explode to accomplish this:\nQuery example\n\n```\nselect x.product_id, x.image_name from (\n  select product_id, map(\"image_name1\", image_name1, ..., \"image_name15\", image_name15) as image_map\n  from table ) x\nlateral view explode(image_map) expl as image_nr, image_name\n```\n\n\nThis returns an unpivoted table like below, which allows us to perform a single join:\n\n|product_id| image_name|\n|----------|-----------|\n|1         | a.jpg     |\n|1         | b.jpg     |\n|1         | c.jpg     |\n|2         | d.jpg     | \n|2         | e.jpg     |\n|2         | f.jpg     |\n\nBig thanks to Diogo Franco, for this hint ;) check also is Blog Page [diogoalexandrefranco.github.io](https://diogoalexandrefranco.github.io)\n\n\n","slug":"unpivot","published":1,"updated":"2020-09-06T14:38:37.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxkp000tchpf3nswdahn","content":"<p>In Hive, when a table has multiple similar columns, such as:</p>\n<table>\n<thead>\n<tr>\n<th>product_id</th>\n<th>image_name1</th>\n<th>image_name2</th>\n<th>…</th>\n<th>image_name15</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>a.jpg</td>\n<td>b.jpg</td>\n<td>…</td>\n<td>c.jpg</td>\n</tr>\n<tr>\n<td>2</td>\n<td>d.jpg</td>\n<td>e.jpg</td>\n<td>…</td>\n<td>f.jpg</td>\n</tr>\n</tbody></table>\n<p>and we want to join based on all image_names, the naive approach would be to perform 15 joins (in the running example).</p>\n<p>Hive does not have an unpivot functionality, but it is possible to use the Hive builtin UDF explode to accomplish this:<br>Query example</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select x.product_id, x.image_name from (</span><br><span class=\"line\">  select product_id, map(&quot;image_name1&quot;, image_name1, ..., &quot;image_name15&quot;, image_name15) as image_map</span><br><span class=\"line\">  from table ) x</span><br><span class=\"line\">lateral view explode(image_map) expl as image_nr, image_name</span><br></pre></td></tr></table></figure>\n\n<p>This returns an unpivoted table like below, which allows us to perform a single join:</p>\n<table>\n<thead>\n<tr>\n<th>product_id</th>\n<th>image_name</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>a.jpg</td>\n</tr>\n<tr>\n<td>1</td>\n<td>b.jpg</td>\n</tr>\n<tr>\n<td>1</td>\n<td>c.jpg</td>\n</tr>\n<tr>\n<td>2</td>\n<td>d.jpg</td>\n</tr>\n<tr>\n<td>2</td>\n<td>e.jpg</td>\n</tr>\n<tr>\n<td>2</td>\n<td>f.jpg</td>\n</tr>\n</tbody></table>\n<p>Big thanks to Diogo Franco, for this hint ;) check also is Blog Page <a href=\"https://diogoalexandrefranco.github.io/\">diogoalexandrefranco.github.io</a></p>\n","site":{"data":{}},"excerpt":"","more":"<p>In Hive, when a table has multiple similar columns, such as:</p>\n<table>\n<thead>\n<tr>\n<th>product_id</th>\n<th>image_name1</th>\n<th>image_name2</th>\n<th>…</th>\n<th>image_name15</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>a.jpg</td>\n<td>b.jpg</td>\n<td>…</td>\n<td>c.jpg</td>\n</tr>\n<tr>\n<td>2</td>\n<td>d.jpg</td>\n<td>e.jpg</td>\n<td>…</td>\n<td>f.jpg</td>\n</tr>\n</tbody></table>\n<p>and we want to join based on all image_names, the naive approach would be to perform 15 joins (in the running example).</p>\n<p>Hive does not have an unpivot functionality, but it is possible to use the Hive builtin UDF explode to accomplish this:<br>Query example</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">select x.product_id, x.image_name from (</span><br><span class=\"line\">  select product_id, map(&quot;image_name1&quot;, image_name1, ..., &quot;image_name15&quot;, image_name15) as image_map</span><br><span class=\"line\">  from table ) x</span><br><span class=\"line\">lateral view explode(image_map) expl as image_nr, image_name</span><br></pre></td></tr></table></figure>\n\n<p>This returns an unpivoted table like below, which allows us to perform a single join:</p>\n<table>\n<thead>\n<tr>\n<th>product_id</th>\n<th>image_name</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>1</td>\n<td>a.jpg</td>\n</tr>\n<tr>\n<td>1</td>\n<td>b.jpg</td>\n</tr>\n<tr>\n<td>1</td>\n<td>c.jpg</td>\n</tr>\n<tr>\n<td>2</td>\n<td>d.jpg</td>\n</tr>\n<tr>\n<td>2</td>\n<td>e.jpg</td>\n</tr>\n<tr>\n<td>2</td>\n<td>f.jpg</td>\n</tr>\n</tbody></table>\n<p>Big thanks to Diogo Franco, for this hint ;) check also is Blog Page <a href=\"https://diogoalexandrefranco.github.io/\">diogoalexandrefranco.github.io</a></p>\n"},{"title":"Quick Guide for Vagrant and Libvirt","date":"2017-09-26T22:43:35.000Z","_content":"\n\nThis article is going to be about a quick way to setup a PoC environment using [libvirt](https://libvirt.org/) ([QEMU](https://www.qemu.org/)) and [vagrant](https://www.vagrantup.com/), and auto configure the enviroment using [ansible](https://www.ansible.com/).\n\n# Why ?\n\nOne migth go a bit further in the testing enviroments, instead of dockers using virtualized enviroment. This kind of approcah whould be suitable when you like to try infrastructure services, or simple don't want to use dockers.\n\nIf you check Internet Trends we can see docker interest have grown quite a lot on the last 5 years. \n\n{% raw %}\n<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/1154_RC03/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"docker\",\"geo\":\"\",\"time\":\"today 5-y\"},{\"keyword\":\"Virtual Machines\",\"geo\":\"\",\"time\":\"today 5-y\"}],\"category\":13,\"property\":\"\"}, {\"exploreQuery\":\"cat=13&date=today 5-y,today 5-y&q=docker,Virtual%20Machines\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); </script> \n{% endraw %}\n\n**Still i'm OldSchool and i like my plain-simple-VMs some times :D**\n\nA later article would be about configuring a Mesos Cluster so this one makes sense as a preparation guide for it.\n\n# Requirements\n\n* Make sure you have vagrant installed\n* Make sure your machine supports virtualization :)\n* Make sure you have QEMU installed\n* Make sure you have libvirt up and running\n* Make sure you have ansible (Optional)\n\n**Note:** This guide was tested on Ubuntu 16. Use at your own risk\n\nOne can test libvirt version suing the command\n\n```\n$ virsh version\nCompiled against library: libvirt 1.3.1\nUsing library: libvirt 1.3.1\nUsing API: QEMU 1.3.1\nRunning hypervisor: QEMU 2.5.0\n```\n\n**NOTE:** Before you start using Vagrant-libvirt, please make sure your libvirt and qemu installation is working correctly and you are able to create qemu or kvm type virtual machines with virsh or virt-manager.\n\n# setup\n\nInstall dependencies\n\n``` \nsudo apt-get install qemu libvirt-bin ebtables dnsmasq\nsudo apt-get install libxslt-dev libxml2-dev libvirt-dev zlib1g-dev ruby-dev\nsudo apt-get install vagrant ruby-libvirt\n```\n\nInstall vagrant-libvirt plugin:\n\n```\nvagrant plugin install vagrant-libvirt\n```\n\nNow let's test by creating a startup Box. There are several available [here](https://app.vagrantup.com/boxes/search?provider=libvirt)\n\nExample: (Ubuntu 16)\n\n```\nmkdir Vagrant\ncd Vagrant\nvagrant init marczis/ubuntu_16_04\n```\n\nLet's start the VM\n\n```\nvagrant up --provider=libvirt\n```\n\nAfter executing the comman you should have some output similar\n\n```\n==> default: Successfully added box 'marczis/ubuntu_16_04' (v0.1.2) for 'libvirt'!\n==> default: Uploading base box image as volume into libvirt storage...\n==> default: Creating image (snapshot of base box volume).\n==> default: Creating domain with the following settings...\n==> default:  -- Name:              Vagrant_default\n==> default:  -- Domain type:       kvm\n==> default:  -- Cpus:              1\n==> default:  -- Feature:           acpi\n==> default:  -- Feature:           apic\n==> default:  -- Feature:           pae\n==> default:  -- Memory:            512M\n==> default:  -- Management MAC:    \n==> default:  -- Loader:            \n==> default:  -- Base box:          marczis/ubuntu_16_04\n==> default:  -- Storage pool:      default\n==> default:  -- Image:             /var/lib/libvirt/images/Vagrant_default.img (10G)\n==> default:  -- Volume Cache:      default\n==> default:  -- Kernel:            \n==> default:  -- Initrd:            \n==> default:  -- Graphics Type:     vnc\n==> default:  -- Graphics Port:     5900\n==> default:  -- Graphics IP:       127.0.0.1\n==> default:  -- Graphics Password: Not defined\n==> default:  -- Video Type:        cirrus\n==> default:  -- Video VRAM:        9216\n==> default:  -- Sound Type:    \n==> default:  -- Keymap:            en-us\n==> default:  -- TPM Path:          \n==> default:  -- INPUT:             type=mouse, bus=ps2\n==> default: Creating shared folders metadata...\n==> default: Starting domain.\n==> default: Waiting for domain to get an IP address...\n==> default: Waiting for SSH to become available...\n    default: \n    default: Vagrant insecure key detected. Vagrant will automatically replace\n    default: this with a newly generated keypair for better security.\n    default: \n    default: Inserting generated public key within guest...\n    default: Removing insecure key from the guest if it's present...\n    default: Key inserted! Disconnecting and reconnecting using new SSH key...\n==> default: Configuring and enabling network interfaces...\n==> default: Rsyncing folder: /home/rramos/Development/Vagrant/ => /vagrant\n==> default: Check for insecure vagrant key: OK (not present)\n```\n\nIf you execute `virt-manager` you can see the virtual machine specification and adapt the Vagrant file to you needs.\n\n\n{% asset_img \"virt-manager.png\" \"virt-manager\" %}\n\nYou can then ssh to the VM using the command `vagrant ssh`. You should have a prompt like:\n\n```\nWelcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-77-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\nvagrant@ubuntu:~$ \n```\n\nYou can add the ssh key to your ssh configuration executing the following command. ( Check my previous  {% post_link ssh-config [article] %} regarding this topic)\n\n```\nvagrant ssh-config >> ~/.ssh/config\n```\n\n\nthen try the command `ssh default` . Default if the default name of the VM defined in the vagrant file. \n\n# Let's extend a bit.\n\nThe next step is to set Ansible as our provisioning provider for the Vagrant Box. Add the following lines before the end statement in your Vagrantfile to set Ansible as the provisioning provider:\n\n```\nconfig.vm.provision :ansible do |ansible|\n  ansible.playbook = \"playbook.yml\"\nend\n```\n\nLet's create the playbook.yml file with the following content\n\n```\n---\n- hosts: all\n  become: yes\n  become_user: root\n  tasks:\n    - name: install apache2\n      apt: name=apache2 update_cache=yes state=latest\n  handlers:\n    - name: restart apache2\n      service: name=apache2 state=restarted\n\n```\n\nThis example playbook will install apache service on the all hosts given by Vagrantfile and start the service.\n\nThen we can run the command to just run the provision stuff\n\n```\nvagrant provision\n```\n\nLet's add the following configuration on our Vagrant file\n\n```\nconfig.vm.network \"forwarded_port\", guest: 80, host: 8080\n```\n\nAnd execute `vagrant reload`\n\nIf one access http://localhost:8080 we are now accessing the VM installed service.\n\nIf you recreate the VM it would run the ansible part at the end. \n\nSo it's quite easy to recreate your environment.\n\n# Cluster Configuration\n\nOne could have Vagrant more complex configuration like the following for several VMs.\n\n```\nVagrant.configure(\"2\") do |config|\n  config.vm.provision \"shell\", inline: \"echo Hello\"\n\n  config.vm.define \"web\" do |web|\n    web.vm.box = \"apache\"\n  end\n\n  config.vm.define \"db\" do |db|\n    db.vm.box = \"mysql\"\n  end\nend\n```\n\n# Conclusion\n\nIn this article is described a simple way to integrate Vangrant with libvirt for quickly spinning VM's and running ansible playbooks on them.\n\nThis could be usefull for testing ansible playbooks before shipping them to LIVE enviroments or testing purposes. \n\nAlso to quickly configure a more complex infrastructure, where you don't wont to use dockers.\n\nOn the next article i will write a step-by-step guide to spin up a Mesos Cluster using this approach, so this article could be used as prep-guide.\n\nCheers,\nRR\n\n# References\n\n* https://libvirt.org/\n* https://www.qemu.org/\n* https://www.vagrantup.com/\n* https://www.ansible.com/\n* https://github.com/vagrant-libvirt/vagrant-libvirt\n* https://app.vagrantup.com/boxes/search?provider=libvirt\n* https://fedoramagazine.org/using-ansible-provision-vagrant-boxes/\n* https://www.vagrantup.com/docs/multi-machine/\n","source":"_posts/vagrant-qemu.md","raw":"---\ntitle: Quick Guide for Vagrant and Libvirt\ntags:\n  - Vagrant\n  - Qemu\n  - KVM\n  - libvirt\n  - Mesos\n  - Ansible\ndate: 2017-09-26 23:43:35\n---\n\n\nThis article is going to be about a quick way to setup a PoC environment using [libvirt](https://libvirt.org/) ([QEMU](https://www.qemu.org/)) and [vagrant](https://www.vagrantup.com/), and auto configure the enviroment using [ansible](https://www.ansible.com/).\n\n# Why ?\n\nOne migth go a bit further in the testing enviroments, instead of dockers using virtualized enviroment. This kind of approcah whould be suitable when you like to try infrastructure services, or simple don't want to use dockers.\n\nIf you check Internet Trends we can see docker interest have grown quite a lot on the last 5 years. \n\n{% raw %}\n<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/1154_RC03/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"docker\",\"geo\":\"\",\"time\":\"today 5-y\"},{\"keyword\":\"Virtual Machines\",\"geo\":\"\",\"time\":\"today 5-y\"}],\"category\":13,\"property\":\"\"}, {\"exploreQuery\":\"cat=13&date=today 5-y,today 5-y&q=docker,Virtual%20Machines\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); </script> \n{% endraw %}\n\n**Still i'm OldSchool and i like my plain-simple-VMs some times :D**\n\nA later article would be about configuring a Mesos Cluster so this one makes sense as a preparation guide for it.\n\n# Requirements\n\n* Make sure you have vagrant installed\n* Make sure your machine supports virtualization :)\n* Make sure you have QEMU installed\n* Make sure you have libvirt up and running\n* Make sure you have ansible (Optional)\n\n**Note:** This guide was tested on Ubuntu 16. Use at your own risk\n\nOne can test libvirt version suing the command\n\n```\n$ virsh version\nCompiled against library: libvirt 1.3.1\nUsing library: libvirt 1.3.1\nUsing API: QEMU 1.3.1\nRunning hypervisor: QEMU 2.5.0\n```\n\n**NOTE:** Before you start using Vagrant-libvirt, please make sure your libvirt and qemu installation is working correctly and you are able to create qemu or kvm type virtual machines with virsh or virt-manager.\n\n# setup\n\nInstall dependencies\n\n``` \nsudo apt-get install qemu libvirt-bin ebtables dnsmasq\nsudo apt-get install libxslt-dev libxml2-dev libvirt-dev zlib1g-dev ruby-dev\nsudo apt-get install vagrant ruby-libvirt\n```\n\nInstall vagrant-libvirt plugin:\n\n```\nvagrant plugin install vagrant-libvirt\n```\n\nNow let's test by creating a startup Box. There are several available [here](https://app.vagrantup.com/boxes/search?provider=libvirt)\n\nExample: (Ubuntu 16)\n\n```\nmkdir Vagrant\ncd Vagrant\nvagrant init marczis/ubuntu_16_04\n```\n\nLet's start the VM\n\n```\nvagrant up --provider=libvirt\n```\n\nAfter executing the comman you should have some output similar\n\n```\n==> default: Successfully added box 'marczis/ubuntu_16_04' (v0.1.2) for 'libvirt'!\n==> default: Uploading base box image as volume into libvirt storage...\n==> default: Creating image (snapshot of base box volume).\n==> default: Creating domain with the following settings...\n==> default:  -- Name:              Vagrant_default\n==> default:  -- Domain type:       kvm\n==> default:  -- Cpus:              1\n==> default:  -- Feature:           acpi\n==> default:  -- Feature:           apic\n==> default:  -- Feature:           pae\n==> default:  -- Memory:            512M\n==> default:  -- Management MAC:    \n==> default:  -- Loader:            \n==> default:  -- Base box:          marczis/ubuntu_16_04\n==> default:  -- Storage pool:      default\n==> default:  -- Image:             /var/lib/libvirt/images/Vagrant_default.img (10G)\n==> default:  -- Volume Cache:      default\n==> default:  -- Kernel:            \n==> default:  -- Initrd:            \n==> default:  -- Graphics Type:     vnc\n==> default:  -- Graphics Port:     5900\n==> default:  -- Graphics IP:       127.0.0.1\n==> default:  -- Graphics Password: Not defined\n==> default:  -- Video Type:        cirrus\n==> default:  -- Video VRAM:        9216\n==> default:  -- Sound Type:    \n==> default:  -- Keymap:            en-us\n==> default:  -- TPM Path:          \n==> default:  -- INPUT:             type=mouse, bus=ps2\n==> default: Creating shared folders metadata...\n==> default: Starting domain.\n==> default: Waiting for domain to get an IP address...\n==> default: Waiting for SSH to become available...\n    default: \n    default: Vagrant insecure key detected. Vagrant will automatically replace\n    default: this with a newly generated keypair for better security.\n    default: \n    default: Inserting generated public key within guest...\n    default: Removing insecure key from the guest if it's present...\n    default: Key inserted! Disconnecting and reconnecting using new SSH key...\n==> default: Configuring and enabling network interfaces...\n==> default: Rsyncing folder: /home/rramos/Development/Vagrant/ => /vagrant\n==> default: Check for insecure vagrant key: OK (not present)\n```\n\nIf you execute `virt-manager` you can see the virtual machine specification and adapt the Vagrant file to you needs.\n\n\n{% asset_img \"virt-manager.png\" \"virt-manager\" %}\n\nYou can then ssh to the VM using the command `vagrant ssh`. You should have a prompt like:\n\n```\nWelcome to Ubuntu 16.04.2 LTS (GNU/Linux 4.4.0-77-generic x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/advantage\nvagrant@ubuntu:~$ \n```\n\nYou can add the ssh key to your ssh configuration executing the following command. ( Check my previous  {% post_link ssh-config [article] %} regarding this topic)\n\n```\nvagrant ssh-config >> ~/.ssh/config\n```\n\n\nthen try the command `ssh default` . Default if the default name of the VM defined in the vagrant file. \n\n# Let's extend a bit.\n\nThe next step is to set Ansible as our provisioning provider for the Vagrant Box. Add the following lines before the end statement in your Vagrantfile to set Ansible as the provisioning provider:\n\n```\nconfig.vm.provision :ansible do |ansible|\n  ansible.playbook = \"playbook.yml\"\nend\n```\n\nLet's create the playbook.yml file with the following content\n\n```\n---\n- hosts: all\n  become: yes\n  become_user: root\n  tasks:\n    - name: install apache2\n      apt: name=apache2 update_cache=yes state=latest\n  handlers:\n    - name: restart apache2\n      service: name=apache2 state=restarted\n\n```\n\nThis example playbook will install apache service on the all hosts given by Vagrantfile and start the service.\n\nThen we can run the command to just run the provision stuff\n\n```\nvagrant provision\n```\n\nLet's add the following configuration on our Vagrant file\n\n```\nconfig.vm.network \"forwarded_port\", guest: 80, host: 8080\n```\n\nAnd execute `vagrant reload`\n\nIf one access http://localhost:8080 we are now accessing the VM installed service.\n\nIf you recreate the VM it would run the ansible part at the end. \n\nSo it's quite easy to recreate your environment.\n\n# Cluster Configuration\n\nOne could have Vagrant more complex configuration like the following for several VMs.\n\n```\nVagrant.configure(\"2\") do |config|\n  config.vm.provision \"shell\", inline: \"echo Hello\"\n\n  config.vm.define \"web\" do |web|\n    web.vm.box = \"apache\"\n  end\n\n  config.vm.define \"db\" do |db|\n    db.vm.box = \"mysql\"\n  end\nend\n```\n\n# Conclusion\n\nIn this article is described a simple way to integrate Vangrant with libvirt for quickly spinning VM's and running ansible playbooks on them.\n\nThis could be usefull for testing ansible playbooks before shipping them to LIVE enviroments or testing purposes. \n\nAlso to quickly configure a more complex infrastructure, where you don't wont to use dockers.\n\nOn the next article i will write a step-by-step guide to spin up a Mesos Cluster using this approach, so this article could be used as prep-guide.\n\nCheers,\nRR\n\n# References\n\n* https://libvirt.org/\n* https://www.qemu.org/\n* https://www.vagrantup.com/\n* https://www.ansible.com/\n* https://github.com/vagrant-libvirt/vagrant-libvirt\n* https://app.vagrantup.com/boxes/search?provider=libvirt\n* https://fedoramagazine.org/using-ansible-provision-vagrant-boxes/\n* https://www.vagrantup.com/docs/multi-machine/\n","slug":"vagrant-qemu","published":1,"updated":"2020-09-06T14:38:37.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxkq000vchpfc3jl00sc","content":"<p>This article is going to be about a quick way to setup a PoC environment using <a href=\"https://libvirt.org/\">libvirt</a> (<a href=\"https://www.qemu.org/\">QEMU</a>) and <a href=\"https://www.vagrantup.com/\">vagrant</a>, and auto configure the enviroment using <a href=\"https://www.ansible.com/\">ansible</a>.</p>\n<h1 id=\"Why\"><a href=\"#Why\" class=\"headerlink\" title=\"Why ?\"></a>Why ?</h1><p>One migth go a bit further in the testing enviroments, instead of dockers using virtualized enviroment. This kind of approcah whould be suitable when you like to try infrastructure services, or simple don’t want to use dockers.</p>\n<p>If you check Internet Trends we can see docker interest have grown quite a lot on the last 5 years. </p>\n\n<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/1154_RC03/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"docker\",\"geo\":\"\",\"time\":\"today 5-y\"},{\"keyword\":\"Virtual Machines\",\"geo\":\"\",\"time\":\"today 5-y\"}],\"category\":13,\"property\":\"\"}, {\"exploreQuery\":\"cat=13&date=today 5-y,today 5-y&q=docker,Virtual%20Machines\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); </script> \n\n\n<p><strong>Still i’m OldSchool and i like my plain-simple-VMs some times :D</strong></p>\n<p>A later article would be about configuring a Mesos Cluster so this one makes sense as a preparation guide for it.</p>\n<h1 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h1><ul>\n<li>Make sure you have vagrant installed</li>\n<li>Make sure your machine supports virtualization :)</li>\n<li>Make sure you have QEMU installed</li>\n<li>Make sure you have libvirt up and running</li>\n<li>Make sure you have ansible (Optional)</li>\n</ul>\n<p><strong>Note:</strong> This guide was tested on Ubuntu 16. Use at your own risk</p>\n<p>One can test libvirt version suing the command</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ virsh version</span><br><span class=\"line\">Compiled against library: libvirt 1.3.1</span><br><span class=\"line\">Using library: libvirt 1.3.1</span><br><span class=\"line\">Using API: QEMU 1.3.1</span><br><span class=\"line\">Running hypervisor: QEMU 2.5.0</span><br></pre></td></tr></table></figure>\n<p><strong>NOTE:</strong> Before you start using Vagrant-libvirt, please make sure your libvirt and qemu installation is working correctly and you are able to create qemu or kvm type virtual machines with virsh or virt-manager.</p>\n<h1 id=\"setup\"><a href=\"#setup\" class=\"headerlink\" title=\"setup\"></a>setup</h1><p>Install dependencies</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install qemu libvirt-bin ebtables dnsmasq</span><br><span class=\"line\">sudo apt-get install libxslt-dev libxml2-dev libvirt-dev zlib1g-dev ruby-dev</span><br><span class=\"line\">sudo apt-get install vagrant ruby-libvirt</span><br></pre></td></tr></table></figure>\n<p>Install vagrant-libvirt plugin:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vagrant plugin install vagrant-libvirt</span><br></pre></td></tr></table></figure>\n<p>Now let’s test by creating a startup Box. There are several available <a href=\"https://app.vagrantup.com/boxes/search?provider=libvirt\">here</a></p>\n<p>Example: (Ubuntu 16)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir Vagrant</span><br><span class=\"line\">cd Vagrant</span><br><span class=\"line\">vagrant init marczis&#x2F;ubuntu_16_04</span><br></pre></td></tr></table></figure>\n<p>Let’s start the VM</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vagrant up --provider&#x3D;libvirt</span><br></pre></td></tr></table></figure>\n<p>After executing the comman you should have some output similar</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x3D;&#x3D;&gt; default: Successfully added box &#39;marczis&#x2F;ubuntu_16_04&#39; (v0.1.2) for &#39;libvirt&#39;!</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Uploading base box image as volume into libvirt storage...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Creating image (snapshot of base box volume).</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Creating domain with the following settings...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Name:              Vagrant_default</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Domain type:       kvm</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Cpus:              1</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Feature:           acpi</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Feature:           apic</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Feature:           pae</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Memory:            512M</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Management MAC:    </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Loader:            </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Base box:          marczis&#x2F;ubuntu_16_04</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Storage pool:      default</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Image:             &#x2F;var&#x2F;lib&#x2F;libvirt&#x2F;images&#x2F;Vagrant_default.img (10G)</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Volume Cache:      default</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Kernel:            </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Initrd:            </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Graphics Type:     vnc</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Graphics Port:     5900</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Graphics IP:       127.0.0.1</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Graphics Password: Not defined</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Video Type:        cirrus</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Video VRAM:        9216</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Sound Type:    </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Keymap:            en-us</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- TPM Path:          </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- INPUT:             type&#x3D;mouse, bus&#x3D;ps2</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Creating shared folders metadata...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Starting domain.</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Waiting for domain to get an IP address...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Waiting for SSH to become available...</span><br><span class=\"line\">    default: </span><br><span class=\"line\">    default: Vagrant insecure key detected. Vagrant will automatically replace</span><br><span class=\"line\">    default: this with a newly generated keypair for better security.</span><br><span class=\"line\">    default: </span><br><span class=\"line\">    default: Inserting generated public key within guest...</span><br><span class=\"line\">    default: Removing insecure key from the guest if it&#39;s present...</span><br><span class=\"line\">    default: Key inserted! Disconnecting and reconnecting using new SSH key...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Configuring and enabling network interfaces...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Rsyncing folder: &#x2F;home&#x2F;rramos&#x2F;Development&#x2F;Vagrant&#x2F; &#x3D;&gt; &#x2F;vagrant</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Check for insecure vagrant key: OK (not present)</span><br></pre></td></tr></table></figure>\n<p>If you execute <code>virt-manager</code> you can see the virtual machine specification and adapt the Vagrant file to you needs.</p>\n<img src=\"/2017/09/26/vagrant-qemu/virt-manager.png\" class=\"\" title=\"virt-manager\">\n\n<p>You can then ssh to the VM using the command <code>vagrant ssh</code>. You should have a prompt like:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Welcome to Ubuntu 16.04.2 LTS (GNU&#x2F;Linux 4.4.0-77-generic x86_64)</span><br><span class=\"line\"></span><br><span class=\"line\"> * Documentation:  https:&#x2F;&#x2F;help.ubuntu.com</span><br><span class=\"line\"> * Management:     https:&#x2F;&#x2F;landscape.canonical.com</span><br><span class=\"line\"> * Support:        https:&#x2F;&#x2F;ubuntu.com&#x2F;advantage</span><br><span class=\"line\">vagrant@ubuntu:~$ </span><br></pre></td></tr></table></figure>\n<p>You can add the ssh key to your ssh configuration executing the following command. ( Check my previous  <a href=\"/2017/09/13/ssh-config/\" title=\"[article]\">[article]</a> regarding this topic)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vagrant ssh-config &gt;&gt; ~&#x2F;.ssh&#x2F;config</span><br></pre></td></tr></table></figure>\n\n<p>then try the command <code>ssh default</code> . Default if the default name of the VM defined in the vagrant file. </p>\n<h1 id=\"Let’s-extend-a-bit\"><a href=\"#Let’s-extend-a-bit\" class=\"headerlink\" title=\"Let’s extend a bit.\"></a>Let’s extend a bit.</h1><p>The next step is to set Ansible as our provisioning provider for the Vagrant Box. Add the following lines before the end statement in your Vagrantfile to set Ansible as the provisioning provider:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">config.vm.provision :ansible do |ansible|</span><br><span class=\"line\">  ansible.playbook &#x3D; &quot;playbook.yml&quot;</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure>\n<p>Let’s create the playbook.yml file with the following content</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">- hosts: all</span><br><span class=\"line\">  become: yes</span><br><span class=\"line\">  become_user: root</span><br><span class=\"line\">  tasks:</span><br><span class=\"line\">    - name: install apache2</span><br><span class=\"line\">      apt: name&#x3D;apache2 update_cache&#x3D;yes state&#x3D;latest</span><br><span class=\"line\">  handlers:</span><br><span class=\"line\">    - name: restart apache2</span><br><span class=\"line\">      service: name&#x3D;apache2 state&#x3D;restarted</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>This example playbook will install apache service on the all hosts given by Vagrantfile and start the service.</p>\n<p>Then we can run the command to just run the provision stuff</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vagrant provision</span><br></pre></td></tr></table></figure>\n<p>Let’s add the following configuration on our Vagrant file</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">config.vm.network &quot;forwarded_port&quot;, guest: 80, host: 8080</span><br></pre></td></tr></table></figure>\n<p>And execute <code>vagrant reload</code></p>\n<p>If one access <a href=\"http://localhost:8080/\">http://localhost:8080</a> we are now accessing the VM installed service.</p>\n<p>If you recreate the VM it would run the ansible part at the end. </p>\n<p>So it’s quite easy to recreate your environment.</p>\n<h1 id=\"Cluster-Configuration\"><a href=\"#Cluster-Configuration\" class=\"headerlink\" title=\"Cluster Configuration\"></a>Cluster Configuration</h1><p>One could have Vagrant more complex configuration like the following for several VMs.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Vagrant.configure(&quot;2&quot;) do |config|</span><br><span class=\"line\">  config.vm.provision &quot;shell&quot;, inline: &quot;echo Hello&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  config.vm.define &quot;web&quot; do |web|</span><br><span class=\"line\">    web.vm.box &#x3D; &quot;apache&quot;</span><br><span class=\"line\">  end</span><br><span class=\"line\"></span><br><span class=\"line\">  config.vm.define &quot;db&quot; do |db|</span><br><span class=\"line\">    db.vm.box &#x3D; &quot;mysql&quot;</span><br><span class=\"line\">  end</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>In this article is described a simple way to integrate Vangrant with libvirt for quickly spinning VM’s and running ansible playbooks on them.</p>\n<p>This could be usefull for testing ansible playbooks before shipping them to LIVE enviroments or testing purposes. </p>\n<p>Also to quickly configure a more complex infrastructure, where you don’t wont to use dockers.</p>\n<p>On the next article i will write a step-by-step guide to spin up a Mesos Cluster using this approach, so this article could be used as prep-guide.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://libvirt.org/\">https://libvirt.org/</a></li>\n<li><a href=\"https://www.qemu.org/\">https://www.qemu.org/</a></li>\n<li><a href=\"https://www.vagrantup.com/\">https://www.vagrantup.com/</a></li>\n<li><a href=\"https://www.ansible.com/\">https://www.ansible.com/</a></li>\n<li><a href=\"https://github.com/vagrant-libvirt/vagrant-libvirt\">https://github.com/vagrant-libvirt/vagrant-libvirt</a></li>\n<li><a href=\"https://app.vagrantup.com/boxes/search?provider=libvirt\">https://app.vagrantup.com/boxes/search?provider=libvirt</a></li>\n<li><a href=\"https://fedoramagazine.org/using-ansible-provision-vagrant-boxes/\">https://fedoramagazine.org/using-ansible-provision-vagrant-boxes/</a></li>\n<li><a href=\"https://www.vagrantup.com/docs/multi-machine/\">https://www.vagrantup.com/docs/multi-machine/</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>This article is going to be about a quick way to setup a PoC environment using <a href=\"https://libvirt.org/\">libvirt</a> (<a href=\"https://www.qemu.org/\">QEMU</a>) and <a href=\"https://www.vagrantup.com/\">vagrant</a>, and auto configure the enviroment using <a href=\"https://www.ansible.com/\">ansible</a>.</p>\n<h1 id=\"Why\"><a href=\"#Why\" class=\"headerlink\" title=\"Why ?\"></a>Why ?</h1><p>One migth go a bit further in the testing enviroments, instead of dockers using virtualized enviroment. This kind of approcah whould be suitable when you like to try infrastructure services, or simple don’t want to use dockers.</p>\n<p>If you check Internet Trends we can see docker interest have grown quite a lot on the last 5 years. </p>\n\n<script type=\"text/javascript\" src=\"https://ssl.gstatic.com/trends_nrtr/1154_RC03/embed_loader.js\"></script> <script type=\"text/javascript\"> trends.embed.renderExploreWidget(\"TIMESERIES\", {\"comparisonItem\":[{\"keyword\":\"docker\",\"geo\":\"\",\"time\":\"today 5-y\"},{\"keyword\":\"Virtual Machines\",\"geo\":\"\",\"time\":\"today 5-y\"}],\"category\":13,\"property\":\"\"}, {\"exploreQuery\":\"cat=13&date=today 5-y,today 5-y&q=docker,Virtual%20Machines\",\"guestPath\":\"https://trends.google.com:443/trends/embed/\"}); </script> \n\n\n<p><strong>Still i’m OldSchool and i like my plain-simple-VMs some times :D</strong></p>\n<p>A later article would be about configuring a Mesos Cluster so this one makes sense as a preparation guide for it.</p>\n<h1 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h1><ul>\n<li>Make sure you have vagrant installed</li>\n<li>Make sure your machine supports virtualization :)</li>\n<li>Make sure you have QEMU installed</li>\n<li>Make sure you have libvirt up and running</li>\n<li>Make sure you have ansible (Optional)</li>\n</ul>\n<p><strong>Note:</strong> This guide was tested on Ubuntu 16. Use at your own risk</p>\n<p>One can test libvirt version suing the command</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">$ virsh version</span><br><span class=\"line\">Compiled against library: libvirt 1.3.1</span><br><span class=\"line\">Using library: libvirt 1.3.1</span><br><span class=\"line\">Using API: QEMU 1.3.1</span><br><span class=\"line\">Running hypervisor: QEMU 2.5.0</span><br></pre></td></tr></table></figure>\n<p><strong>NOTE:</strong> Before you start using Vagrant-libvirt, please make sure your libvirt and qemu installation is working correctly and you are able to create qemu or kvm type virtual machines with virsh or virt-manager.</p>\n<h1 id=\"setup\"><a href=\"#setup\" class=\"headerlink\" title=\"setup\"></a>setup</h1><p>Install dependencies</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install qemu libvirt-bin ebtables dnsmasq</span><br><span class=\"line\">sudo apt-get install libxslt-dev libxml2-dev libvirt-dev zlib1g-dev ruby-dev</span><br><span class=\"line\">sudo apt-get install vagrant ruby-libvirt</span><br></pre></td></tr></table></figure>\n<p>Install vagrant-libvirt plugin:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vagrant plugin install vagrant-libvirt</span><br></pre></td></tr></table></figure>\n<p>Now let’s test by creating a startup Box. There are several available <a href=\"https://app.vagrantup.com/boxes/search?provider=libvirt\">here</a></p>\n<p>Example: (Ubuntu 16)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">mkdir Vagrant</span><br><span class=\"line\">cd Vagrant</span><br><span class=\"line\">vagrant init marczis&#x2F;ubuntu_16_04</span><br></pre></td></tr></table></figure>\n<p>Let’s start the VM</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vagrant up --provider&#x3D;libvirt</span><br></pre></td></tr></table></figure>\n<p>After executing the comman you should have some output similar</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br><span class=\"line\">29</span><br><span class=\"line\">30</span><br><span class=\"line\">31</span><br><span class=\"line\">32</span><br><span class=\"line\">33</span><br><span class=\"line\">34</span><br><span class=\"line\">35</span><br><span class=\"line\">36</span><br><span class=\"line\">37</span><br><span class=\"line\">38</span><br><span class=\"line\">39</span><br><span class=\"line\">40</span><br><span class=\"line\">41</span><br><span class=\"line\">42</span><br><span class=\"line\">43</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">&#x3D;&#x3D;&gt; default: Successfully added box &#39;marczis&#x2F;ubuntu_16_04&#39; (v0.1.2) for &#39;libvirt&#39;!</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Uploading base box image as volume into libvirt storage...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Creating image (snapshot of base box volume).</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Creating domain with the following settings...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Name:              Vagrant_default</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Domain type:       kvm</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Cpus:              1</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Feature:           acpi</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Feature:           apic</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Feature:           pae</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Memory:            512M</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Management MAC:    </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Loader:            </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Base box:          marczis&#x2F;ubuntu_16_04</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Storage pool:      default</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Image:             &#x2F;var&#x2F;lib&#x2F;libvirt&#x2F;images&#x2F;Vagrant_default.img (10G)</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Volume Cache:      default</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Kernel:            </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Initrd:            </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Graphics Type:     vnc</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Graphics Port:     5900</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Graphics IP:       127.0.0.1</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Graphics Password: Not defined</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Video Type:        cirrus</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Video VRAM:        9216</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Sound Type:    </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- Keymap:            en-us</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- TPM Path:          </span><br><span class=\"line\">&#x3D;&#x3D;&gt; default:  -- INPUT:             type&#x3D;mouse, bus&#x3D;ps2</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Creating shared folders metadata...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Starting domain.</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Waiting for domain to get an IP address...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Waiting for SSH to become available...</span><br><span class=\"line\">    default: </span><br><span class=\"line\">    default: Vagrant insecure key detected. Vagrant will automatically replace</span><br><span class=\"line\">    default: this with a newly generated keypair for better security.</span><br><span class=\"line\">    default: </span><br><span class=\"line\">    default: Inserting generated public key within guest...</span><br><span class=\"line\">    default: Removing insecure key from the guest if it&#39;s present...</span><br><span class=\"line\">    default: Key inserted! Disconnecting and reconnecting using new SSH key...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Configuring and enabling network interfaces...</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Rsyncing folder: &#x2F;home&#x2F;rramos&#x2F;Development&#x2F;Vagrant&#x2F; &#x3D;&gt; &#x2F;vagrant</span><br><span class=\"line\">&#x3D;&#x3D;&gt; default: Check for insecure vagrant key: OK (not present)</span><br></pre></td></tr></table></figure>\n<p>If you execute <code>virt-manager</code> you can see the virtual machine specification and adapt the Vagrant file to you needs.</p>\n<img src=\"/2017/09/26/vagrant-qemu/virt-manager.png\" class=\"\" title=\"virt-manager\">\n\n<p>You can then ssh to the VM using the command <code>vagrant ssh</code>. You should have a prompt like:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Welcome to Ubuntu 16.04.2 LTS (GNU&#x2F;Linux 4.4.0-77-generic x86_64)</span><br><span class=\"line\"></span><br><span class=\"line\"> * Documentation:  https:&#x2F;&#x2F;help.ubuntu.com</span><br><span class=\"line\"> * Management:     https:&#x2F;&#x2F;landscape.canonical.com</span><br><span class=\"line\"> * Support:        https:&#x2F;&#x2F;ubuntu.com&#x2F;advantage</span><br><span class=\"line\">vagrant@ubuntu:~$ </span><br></pre></td></tr></table></figure>\n<p>You can add the ssh key to your ssh configuration executing the following command. ( Check my previous  <a href=\"/2017/09/13/ssh-config/\" title=\"[article]\">[article]</a> regarding this topic)</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vagrant ssh-config &gt;&gt; ~&#x2F;.ssh&#x2F;config</span><br></pre></td></tr></table></figure>\n\n<p>then try the command <code>ssh default</code> . Default if the default name of the VM defined in the vagrant file. </p>\n<h1 id=\"Let’s-extend-a-bit\"><a href=\"#Let’s-extend-a-bit\" class=\"headerlink\" title=\"Let’s extend a bit.\"></a>Let’s extend a bit.</h1><p>The next step is to set Ansible as our provisioning provider for the Vagrant Box. Add the following lines before the end statement in your Vagrantfile to set Ansible as the provisioning provider:</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">config.vm.provision :ansible do |ansible|</span><br><span class=\"line\">  ansible.playbook &#x3D; &quot;playbook.yml&quot;</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure>\n<p>Let’s create the playbook.yml file with the following content</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">---</span><br><span class=\"line\">- hosts: all</span><br><span class=\"line\">  become: yes</span><br><span class=\"line\">  become_user: root</span><br><span class=\"line\">  tasks:</span><br><span class=\"line\">    - name: install apache2</span><br><span class=\"line\">      apt: name&#x3D;apache2 update_cache&#x3D;yes state&#x3D;latest</span><br><span class=\"line\">  handlers:</span><br><span class=\"line\">    - name: restart apache2</span><br><span class=\"line\">      service: name&#x3D;apache2 state&#x3D;restarted</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure>\n<p>This example playbook will install apache service on the all hosts given by Vagrantfile and start the service.</p>\n<p>Then we can run the command to just run the provision stuff</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">vagrant provision</span><br></pre></td></tr></table></figure>\n<p>Let’s add the following configuration on our Vagrant file</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">config.vm.network &quot;forwarded_port&quot;, guest: 80, host: 8080</span><br></pre></td></tr></table></figure>\n<p>And execute <code>vagrant reload</code></p>\n<p>If one access <a href=\"http://localhost:8080/\">http://localhost:8080</a> we are now accessing the VM installed service.</p>\n<p>If you recreate the VM it would run the ansible part at the end. </p>\n<p>So it’s quite easy to recreate your environment.</p>\n<h1 id=\"Cluster-Configuration\"><a href=\"#Cluster-Configuration\" class=\"headerlink\" title=\"Cluster Configuration\"></a>Cluster Configuration</h1><p>One could have Vagrant more complex configuration like the following for several VMs.</p>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">Vagrant.configure(&quot;2&quot;) do |config|</span><br><span class=\"line\">  config.vm.provision &quot;shell&quot;, inline: &quot;echo Hello&quot;</span><br><span class=\"line\"></span><br><span class=\"line\">  config.vm.define &quot;web&quot; do |web|</span><br><span class=\"line\">    web.vm.box &#x3D; &quot;apache&quot;</span><br><span class=\"line\">  end</span><br><span class=\"line\"></span><br><span class=\"line\">  config.vm.define &quot;db&quot; do |db|</span><br><span class=\"line\">    db.vm.box &#x3D; &quot;mysql&quot;</span><br><span class=\"line\">  end</span><br><span class=\"line\">end</span><br></pre></td></tr></table></figure>\n<h1 id=\"Conclusion\"><a href=\"#Conclusion\" class=\"headerlink\" title=\"Conclusion\"></a>Conclusion</h1><p>In this article is described a simple way to integrate Vangrant with libvirt for quickly spinning VM’s and running ansible playbooks on them.</p>\n<p>This could be usefull for testing ansible playbooks before shipping them to LIVE enviroments or testing purposes. </p>\n<p>Also to quickly configure a more complex infrastructure, where you don’t wont to use dockers.</p>\n<p>On the next article i will write a step-by-step guide to spin up a Mesos Cluster using this approach, so this article could be used as prep-guide.</p>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://libvirt.org/\">https://libvirt.org/</a></li>\n<li><a href=\"https://www.qemu.org/\">https://www.qemu.org/</a></li>\n<li><a href=\"https://www.vagrantup.com/\">https://www.vagrantup.com/</a></li>\n<li><a href=\"https://www.ansible.com/\">https://www.ansible.com/</a></li>\n<li><a href=\"https://github.com/vagrant-libvirt/vagrant-libvirt\">https://github.com/vagrant-libvirt/vagrant-libvirt</a></li>\n<li><a href=\"https://app.vagrantup.com/boxes/search?provider=libvirt\">https://app.vagrantup.com/boxes/search?provider=libvirt</a></li>\n<li><a href=\"https://fedoramagazine.org/using-ansible-provision-vagrant-boxes/\">https://fedoramagazine.org/using-ansible-provision-vagrant-boxes/</a></li>\n<li><a href=\"https://www.vagrantup.com/docs/multi-machine/\">https://www.vagrantup.com/docs/multi-machine/</a></li>\n</ul>\n"},{"title":"Convert VirtualBox Images to KVM","date":"2017-10-02T21:32:20.000Z","_content":"\n\nIn this article i will explain a quick way to convert [VirtualBox](https://www.virtualbox.org) images (vdi) into qcow2 and load them in [KVM](https://www.linux-kvm.org).\n\n# Why ?\n\nThis is something i normally do, because i get better performance running Linux VM's on KVM and saves me some space with qcow2 volume format.\n\n## qcow2 Features\n\nOne of the main characteristics of qcow disk images is that files with this format can grow as data is added. This allows for smaller file sizes than raw disk images, which allocate the whole image space to a file, even if parts of it are empty\n\n\n# Requirements\n\n* You should have KVM configured correctly\n\n```\nsudo apt-get install qemu-kvm\n```\n\n* Install qemu-utils\n\n```\nsudo apt-get install qemu-utils\n```\n\n* Let's get an example image. \n    *  I'm fetching one from http://www.osboxes.org/ubuntu. (ex: Ubuntu 17.10 10th-August build (64bit).vdi)\n\n**NOTE:** Procedure executed with qemu-img version 2.5.0\n\n# Converting\n\n* Convert the image\n\n```sh\nqemu-img convert -f vdi -O qcow2 Ubuntu_17.10.vdi Ubuntu_17.10.qcow2\n```\n\n* You can check the image details by executing `qemu-img info Ubuntu_17.10.qcow2` .\n\n```sh\nqemu-img info Ubuntu_17.10.qcow2\nimage: Ubuntu_17.10.qcow2\nfile format: qcow2\nvirtual size: 100G (107374182400 bytes)\ndisk size: 4.1G\ncluster_size: 65536\nFormat specific information:\n    compat: 1.1\n    lazy refcounts: false\n    refcount bits: 16\n    corrupt: false\n```\n\n* Now lets Upload this image to our KVM pool and create a new VM from it:\n\n```sh\nsudo virsh vol-create-as default ubuntu-17.10 100G --format qcow2\nsudo virsh vol-upload --pool default ubuntu-17.10 Ubuntu_17.10.qcow2 \n```\n\n* Start [virt-manager](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/chap-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Guest_Installation_Virt_Manager-Creating_guests_with_virt_manager.html) and select **Import existing Disk Image**\n\n* The default password for this images are `osboxes.org` . You should have something like the following in the console.\n\n{% asset_img \"virt-manager.png\" \"virt-manager\" %}\n\n* If you want to create brand new images you could use [virt-install](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/sect-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Guest_Installation-Creating_guests_with_virt_install.html), but that one would go to another article.\n\n\nCheers,\nRR\n\n\n# References\n\n* https://www.virtualbox.org/wiki/Downloads\n* https://www.linux-kvm.org/page/Main_Page\n* http://www.osboxes.org/ubuntu\n* https://docs.openstack.org/image-guide/convert-images.html\n","source":"_posts/vdi2qemu.md","raw":"---\ntitle: Convert VirtualBox images to KVM\ntags:\n  - VirtualBox\n  - KVM\n  - QEMU\n  - vdi\n  - qcow2\n  - Utils\ndate: 2017-10-02 22:32:20\n---\n\n\nIn this article i will explain a quick way to convert [VirtualBox](https://www.virtualbox.org) images (vdi) into qcow2 and load them in [KVM](https://www.linux-kvm.org).\n\n# Why ?\n\nThis is something i normally do, because i get better performance running Linux VM's on KVM and saves me some space with qcow2 volume format.\n\n## qcow2 Features\n\nOne of the main characteristics of qcow disk images is that files with this format can grow as data is added. This allows for smaller file sizes than raw disk images, which allocate the whole image space to a file, even if parts of it are empty\n\n\n# Requirements\n\n* You should have KVM configured correctly\n\n```\nsudo apt-get install qemu-kvm\n```\n\n* Install qemu-utils\n\n```\nsudo apt-get install qemu-utils\n```\n\n* Let's get an example image. \n    *  I'm fetching one from http://www.osboxes.org/ubuntu. (ex: Ubuntu 17.10 10th-August build (64bit).vdi)\n\n**NOTE:** Procedure executed with qemu-img version 2.5.0\n\n# Converting\n\n* Convert the image\n\n```sh\nqemu-img convert -f vdi -O qcow2 Ubuntu_17.10.vdi Ubuntu_17.10.qcow2\n```\n\n* You can check the image details by executing `qemu-img info Ubuntu_17.10.qcow2` .\n\n```sh\nqemu-img info Ubuntu_17.10.qcow2\nimage: Ubuntu_17.10.qcow2\nfile format: qcow2\nvirtual size: 100G (107374182400 bytes)\ndisk size: 4.1G\ncluster_size: 65536\nFormat specific information:\n    compat: 1.1\n    lazy refcounts: false\n    refcount bits: 16\n    corrupt: false\n```\n\n* Now lets Upload this image to our KVM pool and create a new VM from it:\n\n```sh\nsudo virsh vol-create-as default ubuntu-17.10 100G --format qcow2\nsudo virsh vol-upload --pool default ubuntu-17.10 Ubuntu_17.10.qcow2 \n```\n\n* Start [virt-manager](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/chap-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Guest_Installation_Virt_Manager-Creating_guests_with_virt_manager.html) and select **Import existing Disk Image**\n\n* The default password for this images are `osboxes.org` . You should have something like the following in the console.\n\n{% asset_img \"virt-manager.png\" \"virt-manager\" %}\n\n* If you want to create brand new images you could use [virt-install](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/sect-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Guest_Installation-Creating_guests_with_virt_install.html), but that one would go to another article.\n\n\nCheers,\nRR\n\n\n# References\n\n* https://www.virtualbox.org/wiki/Downloads\n* https://www.linux-kvm.org/page/Main_Page\n* http://www.osboxes.org/ubuntu\n* https://docs.openstack.org/image-guide/convert-images.html\n","slug":"vdi2qemu","published":1,"updated":"2020-09-06T14:38:37.859Z","comments":1,"layout":"post","photos":[],"link":"","_id":"ckle9cxkr000wchpf9va83uca","content":"<p>In this article i will explain a quick way to convert <a href=\"https://www.virtualbox.org/\">VirtualBox</a> images (vdi) into qcow2 and load them in <a href=\"https://www.linux-kvm.org/\">KVM</a>.</p>\n<h1 id=\"Why\"><a href=\"#Why\" class=\"headerlink\" title=\"Why ?\"></a>Why ?</h1><p>This is something i normally do, because i get better performance running Linux VM’s on KVM and saves me some space with qcow2 volume format.</p>\n<h2 id=\"qcow2-Features\"><a href=\"#qcow2-Features\" class=\"headerlink\" title=\"qcow2 Features\"></a>qcow2 Features</h2><p>One of the main characteristics of qcow disk images is that files with this format can grow as data is added. This allows for smaller file sizes than raw disk images, which allocate the whole image space to a file, even if parts of it are empty</p>\n<h1 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h1><ul>\n<li>You should have KVM configured correctly</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install qemu-kvm</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Install qemu-utils</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install qemu-utils</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Let’s get an example image. <ul>\n<li> I’m fetching one from <a href=\"http://www.osboxes.org/ubuntu\">http://www.osboxes.org/ubuntu</a>. (ex: Ubuntu 17.10 10th-August build (64bit).vdi)</li>\n</ul>\n</li>\n</ul>\n<p><strong>NOTE:</strong> Procedure executed with qemu-img version 2.5.0</p>\n<h1 id=\"Converting\"><a href=\"#Converting\" class=\"headerlink\" title=\"Converting\"></a>Converting</h1><ul>\n<li>Convert the image</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">qemu-img convert -f vdi -O qcow2 Ubuntu_17.10.vdi Ubuntu_17.10.qcow2</span><br></pre></td></tr></table></figure>\n<ul>\n<li>You can check the image details by executing <code>qemu-img info Ubuntu_17.10.qcow2</code> .</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">qemu-img info Ubuntu_17.10.qcow2</span><br><span class=\"line\">image: Ubuntu_17.10.qcow2</span><br><span class=\"line\">file format: qcow2</span><br><span class=\"line\">virtual size: 100G (107374182400 bytes)</span><br><span class=\"line\">disk size: 4.1G</span><br><span class=\"line\">cluster_size: 65536</span><br><span class=\"line\">Format specific information:</span><br><span class=\"line\">    compat: 1.1</span><br><span class=\"line\">    lazy refcounts: <span class=\"literal\">false</span></span><br><span class=\"line\">    refcount bits: 16</span><br><span class=\"line\">    corrupt: <span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>Now lets Upload this image to our KVM pool and create a new VM from it:</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo virsh vol-create-as default ubuntu-17.10 100G --format qcow2</span><br><span class=\"line\">sudo virsh vol-upload --pool default ubuntu-17.10 Ubuntu_17.10.qcow2 </span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>Start <a href=\"https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/chap-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Guest_Installation_Virt_Manager-Creating_guests_with_virt_manager.html\">virt-manager</a> and select <strong>Import existing Disk Image</strong></p>\n</li>\n<li><p>The default password for this images are <code>osboxes.org</code> . You should have something like the following in the console.</p>\n</li>\n</ul>\n<img src=\"/2017/10/02/vdi2qemu/virt-manager.png\" class=\"\" title=\"virt-manager\">\n\n<ul>\n<li>If you want to create brand new images you could use <a href=\"https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/sect-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Guest_Installation-Creating_guests_with_virt_install.html\">virt-install</a>, but that one would go to another article.</li>\n</ul>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://www.virtualbox.org/wiki/Downloads\">https://www.virtualbox.org/wiki/Downloads</a></li>\n<li><a href=\"https://www.linux-kvm.org/page/Main_Page\">https://www.linux-kvm.org/page/Main_Page</a></li>\n<li><a href=\"http://www.osboxes.org/ubuntu\">http://www.osboxes.org/ubuntu</a></li>\n<li><a href=\"https://docs.openstack.org/image-guide/convert-images.html\">https://docs.openstack.org/image-guide/convert-images.html</a></li>\n</ul>\n","site":{"data":{}},"excerpt":"","more":"<p>In this article i will explain a quick way to convert <a href=\"https://www.virtualbox.org/\">VirtualBox</a> images (vdi) into qcow2 and load them in <a href=\"https://www.linux-kvm.org/\">KVM</a>.</p>\n<h1 id=\"Why\"><a href=\"#Why\" class=\"headerlink\" title=\"Why ?\"></a>Why ?</h1><p>This is something i normally do, because i get better performance running Linux VM’s on KVM and saves me some space with qcow2 volume format.</p>\n<h2 id=\"qcow2-Features\"><a href=\"#qcow2-Features\" class=\"headerlink\" title=\"qcow2 Features\"></a>qcow2 Features</h2><p>One of the main characteristics of qcow disk images is that files with this format can grow as data is added. This allows for smaller file sizes than raw disk images, which allocate the whole image space to a file, even if parts of it are empty</p>\n<h1 id=\"Requirements\"><a href=\"#Requirements\" class=\"headerlink\" title=\"Requirements\"></a>Requirements</h1><ul>\n<li>You should have KVM configured correctly</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install qemu-kvm</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Install qemu-utils</li>\n</ul>\n<figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo apt-get install qemu-utils</span><br></pre></td></tr></table></figure>\n<ul>\n<li>Let’s get an example image. <ul>\n<li> I’m fetching one from <a href=\"http://www.osboxes.org/ubuntu\">http://www.osboxes.org/ubuntu</a>. (ex: Ubuntu 17.10 10th-August build (64bit).vdi)</li>\n</ul>\n</li>\n</ul>\n<p><strong>NOTE:</strong> Procedure executed with qemu-img version 2.5.0</p>\n<h1 id=\"Converting\"><a href=\"#Converting\" class=\"headerlink\" title=\"Converting\"></a>Converting</h1><ul>\n<li>Convert the image</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">qemu-img convert -f vdi -O qcow2 Ubuntu_17.10.vdi Ubuntu_17.10.qcow2</span><br></pre></td></tr></table></figure>\n<ul>\n<li>You can check the image details by executing <code>qemu-img info Ubuntu_17.10.qcow2</code> .</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">qemu-img info Ubuntu_17.10.qcow2</span><br><span class=\"line\">image: Ubuntu_17.10.qcow2</span><br><span class=\"line\">file format: qcow2</span><br><span class=\"line\">virtual size: 100G (107374182400 bytes)</span><br><span class=\"line\">disk size: 4.1G</span><br><span class=\"line\">cluster_size: 65536</span><br><span class=\"line\">Format specific information:</span><br><span class=\"line\">    compat: 1.1</span><br><span class=\"line\">    lazy refcounts: <span class=\"literal\">false</span></span><br><span class=\"line\">    refcount bits: 16</span><br><span class=\"line\">    corrupt: <span class=\"literal\">false</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>Now lets Upload this image to our KVM pool and create a new VM from it:</li>\n</ul>\n<figure class=\"highlight sh\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sudo virsh vol-create-as default ubuntu-17.10 100G --format qcow2</span><br><span class=\"line\">sudo virsh vol-upload --pool default ubuntu-17.10 Ubuntu_17.10.qcow2 </span><br></pre></td></tr></table></figure>\n<ul>\n<li><p>Start <a href=\"https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/chap-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Guest_Installation_Virt_Manager-Creating_guests_with_virt_manager.html\">virt-manager</a> and select <strong>Import existing Disk Image</strong></p>\n</li>\n<li><p>The default password for this images are <code>osboxes.org</code> . You should have something like the following in the console.</p>\n</li>\n</ul>\n<img src=\"/2017/10/02/vdi2qemu/virt-manager.png\" class=\"\" title=\"virt-manager\">\n\n<ul>\n<li>If you want to create brand new images you could use <a href=\"https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Virtualization_Host_Configuration_and_Guest_Installation_Guide/sect-Virtualization_Host_Configuration_and_Guest_Installation_Guide-Guest_Installation-Creating_guests_with_virt_install.html\">virt-install</a>, but that one would go to another article.</li>\n</ul>\n<p>Cheers,<br>RR</p>\n<h1 id=\"References\"><a href=\"#References\" class=\"headerlink\" title=\"References\"></a>References</h1><ul>\n<li><a href=\"https://www.virtualbox.org/wiki/Downloads\">https://www.virtualbox.org/wiki/Downloads</a></li>\n<li><a href=\"https://www.linux-kvm.org/page/Main_Page\">https://www.linux-kvm.org/page/Main_Page</a></li>\n<li><a href=\"http://www.osboxes.org/ubuntu\">http://www.osboxes.org/ubuntu</a></li>\n<li><a href=\"https://docs.openstack.org/image-guide/convert-images.html\">https://docs.openstack.org/image-guide/convert-images.html</a></li>\n</ul>\n"}],"PostAsset":[{"_id":"source/_posts/GA-CustomDimension/Custom_Dimension.png","slug":"Custom_Dimension.png","post":"ckle9cxji0007chpfgfiidg7d","modified":0,"renderable":0},{"_id":"source/_posts/GA-CustomDimension/GA_CustomDimension_js.png","slug":"GA_CustomDimension_js.png","post":"ckle9cxji0007chpfgfiidg7d","modified":0,"renderable":0},{"_id":"source/_posts/ansible-cmdb/screenshot-overview.png","slug":"screenshot-overview.png","post":"ckle9cxjk000achpf0xsn4xkr","modified":0,"renderable":0},{"_id":"source/_posts/drelephat/DrElephant-2-dashboard.jpg","slug":"DrElephant-2-dashboard.jpg","post":"ckle9cxjn000dchpf89zbbote","modified":0,"renderable":0},{"_id":"source/_posts/portainer/portainer.io.png","slug":"portainer.io.png","post":"ckle9cxjs000lchpf70bkfhu6","modified":0,"renderable":0},{"_id":"source/_posts/nifiparcel/parcel01.png","slug":"parcel01.png","post":"ckle9cxjr000kchpfdku0bfju","modified":0,"renderable":0},{"_id":"source/_posts/nifiparcel/parcel02.png","slug":"parcel02.png","post":"ckle9cxjr000kchpfdku0bfju","modified":0,"renderable":0},{"_id":"source/_posts/nifiparcel/parcel03.png","slug":"parcel03.png","post":"ckle9cxjr000kchpfdku0bfju","modified":0,"renderable":0},{"_id":"source/_posts/quickzeppelin/radiant_win.png","slug":"radiant_win.png","post":"ckle9cxju000ochpf9zisdvn8","modified":0,"renderable":0},{"_id":"source/_posts/vagrant-qemu/virt-manager.png","slug":"virt-manager.png","post":"ckle9cxkq000vchpfc3jl00sc","modified":0,"renderable":0},{"_id":"source/_posts/vdi2qemu/virt-manager.png","slug":"virt-manager.png","post":"ckle9cxkr000wchpf9va83uca","modified":0,"renderable":0}],"PostCategory":[],"PostTag":[{"post_id":"ckle9cxjb0001chpfb2tsdaod","tag_id":"ckle9cxkd000rchpf0agiaqus","_id":"ckle9cxm0003ychpf4y1lbs8o"},{"post_id":"ckle9cxjb0001chpfb2tsdaod","tag_id":"ckle9cxkp000uchpf0yr57w5h","_id":"ckle9cxm10040chpf1v4ohrym"},{"post_id":"ckle9cxjb0001chpfb2tsdaod","tag_id":"ckle9cxkr000xchpfbvog9kmu","_id":"ckle9cxm10041chpf0qslhlw7"},{"post_id":"ckle9cxjb0001chpfb2tsdaod","tag_id":"ckle9cxks000ychpfbcjo3739","_id":"ckle9cxm10043chpf8pb45lcw"},{"post_id":"ckle9cxjb0001chpfb2tsdaod","tag_id":"ckle9cxks000zchpf9noddpxc","_id":"ckle9cxm10044chpf6qxyf5zk"},{"post_id":"ckle9cxjb0001chpfb2tsdaod","tag_id":"ckle9cxks0010chpf2fvt2pim","_id":"ckle9cxm20046chpfdif30jfv"},{"post_id":"ckle9cxjb0001chpfb2tsdaod","tag_id":"ckle9cxks0011chpfhkml0ypf","_id":"ckle9cxm20047chpf3uaz7r6c"},{"post_id":"ckle9cxjb0001chpfb2tsdaod","tag_id":"ckle9cxkt0012chpf9jblc5nr","_id":"ckle9cxm20049chpf8lxucxsx"},{"post_id":"ckle9cxjb0001chpfb2tsdaod","tag_id":"ckle9cxkt0013chpfh8tnhomf","_id":"ckle9cxm2004achpf9c10bel7"},{"post_id":"ckle9cxje0002chpfdv3i3whm","tag_id":"ckle9cxks000ychpfbcjo3739","_id":"ckle9cxm3004bchpf0ljj94v6"},{"post_id":"ckle9cxje0002chpfdv3i3whm","tag_id":"ckle9cxku0015chpf6oeeg68p","_id":"ckle9cxm3004dchpfbqiy8uic"},{"post_id":"ckle9cxje0002chpfdv3i3whm","tag_id":"ckle9cxkd000rchpf0agiaqus","_id":"ckle9cxm3004echpf8gpgbpas"},{"post_id":"ckle9cxje0002chpfdv3i3whm","tag_id":"ckle9cxkv0017chpfgjbkarma","_id":"ckle9cxm3004gchpfdj4h53lp"},{"post_id":"ckle9cxje0002chpfdv3i3whm","tag_id":"ckle9cxkw0018chpf0xodejcr","_id":"ckle9cxm4004hchpfh8nd4luv"},{"post_id":"ckle9cxjf0003chpf50na27lv","tag_id":"ckle9cxkt0012chpf9jblc5nr","_id":"ckle9cxm4004jchpf05hyer6x"},{"post_id":"ckle9cxjf0003chpf50na27lv","tag_id":"ckle9cxks0011chpfhkml0ypf","_id":"ckle9cxm4004kchpf95yd8fpb"},{"post_id":"ckle9cxjf0003chpf50na27lv","tag_id":"ckle9cxkw0018chpf0xodejcr","_id":"ckle9cxm4004mchpfe7o9147x"},{"post_id":"ckle9cxjg0004chpfhiwd26o4","tag_id":"ckle9cxkx001cchpfcx4b5qk0","_id":"ckle9cxm5004nchpfa9dw2g2f"},{"post_id":"ckle9cxjg0004chpfhiwd26o4","tag_id":"ckle9cxkx001dchpff0y397f3","_id":"ckle9cxm5004ochpf9y91ecl3"},{"post_id":"ckle9cxjg0004chpfhiwd26o4","tag_id":"ckle9cxky001echpf4ui8582d","_id":"ckle9cxm5004qchpfbjpr5foq"},{"post_id":"ckle9cxjg0004chpfhiwd26o4","tag_id":"ckle9cxky001fchpf09488z6g","_id":"ckle9cxm5004rchpffz160mff"},{"post_id":"ckle9cxjg0004chpfhiwd26o4","tag_id":"ckle9cxky001gchpf3rxyeu7j","_id":"ckle9cxm6004tchpfhgas8joh"},{"post_id":"ckle9cxjg0005chpf654pf8ch","tag_id":"ckle9cxks000zchpf9noddpxc","_id":"ckle9cxm6004uchpf5ty57r4y"},{"post_id":"ckle9cxjg0005chpf654pf8ch","tag_id":"ckle9cxkz001ichpfhhhlc8pj","_id":"ckle9cxm6004wchpffkp10m2v"},{"post_id":"ckle9cxjg0005chpf654pf8ch","tag_id":"ckle9cxkz001jchpf2cii74ji","_id":"ckle9cxm6004xchpfbvbmg6rm"},{"post_id":"ckle9cxjg0005chpf654pf8ch","tag_id":"ckle9cxkd000rchpf0agiaqus","_id":"ckle9cxm7004zchpf0abqf8pu"},{"post_id":"ckle9cxjg0005chpf654pf8ch","tag_id":"ckle9cxl0001lchpfdruz29nd","_id":"ckle9cxm70050chpf3xc4edcz"},{"post_id":"ckle9cxjg0005chpf654pf8ch","tag_id":"ckle9cxl0001mchpfh5f2ha8t","_id":"ckle9cxm70052chpf2r8ch6ff"},{"post_id":"ckle9cxjh0006chpf15vlem3w","tag_id":"ckle9cxl1001nchpfbxmb17jo","_id":"ckle9cxm70053chpfgmcqbi54"},{"post_id":"ckle9cxjh0006chpf15vlem3w","tag_id":"ckle9cxl1001ochpfdhy41c83","_id":"ckle9cxm80054chpf9kjehd51"},{"post_id":"ckle9cxjh0006chpf15vlem3w","tag_id":"ckle9cxl1001pchpf4jpk1lgc","_id":"ckle9cxm80056chpfg0nk559u"},{"post_id":"ckle9cxjh0006chpf15vlem3w","tag_id":"ckle9cxl2001qchpfarj85dq8","_id":"ckle9cxm80057chpf6pli3yrk"},{"post_id":"ckle9cxjh0006chpf15vlem3w","tag_id":"ckle9cxl2001rchpf0gxn5zfg","_id":"ckle9cxm80058chpf6k0x4shw"},{"post_id":"ckle9cxji0007chpfgfiidg7d","tag_id":"ckle9cxl2001schpfhl4d68pq","_id":"ckle9cxm80059chpfa4vf6ci4"},{"post_id":"ckle9cxji0007chpfgfiidg7d","tag_id":"ckle9cxl2001tchpfcds7f21j","_id":"ckle9cxm8005achpf1olt42g6"},{"post_id":"ckle9cxji0007chpfgfiidg7d","tag_id":"ckle9cxkt0013chpfh8tnhomf","_id":"ckle9cxm8005bchpf7glv3y07"},{"post_id":"ckle9cxji0007chpfgfiidg7d","tag_id":"ckle9cxl3001vchpfc2w0eyes","_id":"ckle9cxm9005cchpf0uomb4v3"},{"post_id":"ckle9cxjj0008chpf909rfhtb","tag_id":"ckle9cxl3001wchpf39gh5hbi","_id":"ckle9cxm9005dchpf8xbgbpjm"},{"post_id":"ckle9cxjj0008chpf909rfhtb","tag_id":"ckle9cxl4001xchpfa6ps68jw","_id":"ckle9cxm9005echpf6fuv2bax"},{"post_id":"ckle9cxjj0008chpf909rfhtb","tag_id":"ckle9cxkd000rchpf0agiaqus","_id":"ckle9cxm9005fchpf7kez2dxw"},{"post_id":"ckle9cxjj0008chpf909rfhtb","tag_id":"ckle9cxks0010chpf2fvt2pim","_id":"ckle9cxm9005gchpf5r6e2hgl"},{"post_id":"ckle9cxjj0008chpf909rfhtb","tag_id":"ckle9cxl50020chpfflvp91k1","_id":"ckle9cxm9005hchpfgcvz1ofx"},{"post_id":"ckle9cxjj0008chpf909rfhtb","tag_id":"ckle9cxl50021chpfafkd0qsn","_id":"ckle9cxm9005ichpfbzao8yrn"},{"post_id":"ckle9cxjj0009chpfe1hddzec","tag_id":"ckle9cxl50022chpfarxw6igk","_id":"ckle9cxm9005jchpfbavq65ou"},{"post_id":"ckle9cxjj0009chpfe1hddzec","tag_id":"ckle9cxkt0012chpf9jblc5nr","_id":"ckle9cxm9005kchpf49wv2oxt"},{"post_id":"ckle9cxjj0009chpfe1hddzec","tag_id":"ckle9cxl60024chpfa46vdy2o","_id":"ckle9cxm9005lchpfhzwydkwh"},{"post_id":"ckle9cxjj0009chpfe1hddzec","tag_id":"ckle9cxl60025chpf0o836kwp","_id":"ckle9cxm9005mchpf7aem7to2"},{"post_id":"ckle9cxjj0009chpfe1hddzec","tag_id":"ckle9cxl70026chpf9mkqh2ku","_id":"ckle9cxma005nchpf66jnhm2c"},{"post_id":"ckle9cxjj0009chpfe1hddzec","tag_id":"ckle9cxl70027chpfasx12a7h","_id":"ckle9cxma005ochpfgt7e7loo"},{"post_id":"ckle9cxjk000achpf0xsn4xkr","tag_id":"ckle9cxl70028chpfgwrpcc3d","_id":"ckle9cxma005pchpffprcajlt"},{"post_id":"ckle9cxjl000bchpf1i8v4pyn","tag_id":"ckle9cxks0011chpfhkml0ypf","_id":"ckle9cxma005qchpfacow2e46"},{"post_id":"ckle9cxjl000bchpf1i8v4pyn","tag_id":"ckle9cxky001gchpf3rxyeu7j","_id":"ckle9cxma005rchpf19gk8ojy"},{"post_id":"ckle9cxjl000bchpf1i8v4pyn","tag_id":"ckle9cxl8002bchpfgivs0388","_id":"ckle9cxma005schpf0h0m6v17"},{"post_id":"ckle9cxjl000cchpfakiggudq","tag_id":"ckle9cxks0011chpfhkml0ypf","_id":"ckle9cxma005tchpf53oq4e7x"},{"post_id":"ckle9cxjl000cchpfakiggudq","tag_id":"ckle9cxl9002dchpf2stn2j80","_id":"ckle9cxma005uchpf8r3uci9w"},{"post_id":"ckle9cxjl000cchpfakiggudq","tag_id":"ckle9cxl9002echpf2nsudoot","_id":"ckle9cxma005vchpfhubtdmjn"},{"post_id":"ckle9cxjn000dchpf89zbbote","tag_id":"ckle9cxla002fchpfb32bcjqc","_id":"ckle9cxma005wchpfge8daam3"},{"post_id":"ckle9cxjn000dchpf89zbbote","tag_id":"ckle9cxl60025chpf0o836kwp","_id":"ckle9cxmb005xchpf70vuh9tt"},{"post_id":"ckle9cxjn000dchpf89zbbote","tag_id":"ckle9cxla002hchpf91483gh2","_id":"ckle9cxmb005ychpfco67atyn"},{"post_id":"ckle9cxjn000dchpf89zbbote","tag_id":"ckle9cxlb002ichpfe30x8zke","_id":"ckle9cxmb005zchpf507d2h6m"},{"post_id":"ckle9cxjn000dchpf89zbbote","tag_id":"ckle9cxlb002jchpf0eyxc3pe","_id":"ckle9cxmb0060chpfclmj9ti1"},{"post_id":"ckle9cxjn000dchpf89zbbote","tag_id":"ckle9cxkt0012chpf9jblc5nr","_id":"ckle9cxmb0061chpf0lrm20kt"},{"post_id":"ckle9cxjn000echpfe6ug7jfs","tag_id":"ckle9cxl2001qchpfarj85dq8","_id":"ckle9cxmb0062chpf76zufmig"},{"post_id":"ckle9cxjn000echpfe6ug7jfs","tag_id":"ckle9cxlc002mchpfa17gfkcb","_id":"ckle9cxmb0063chpf4zkta07s"},{"post_id":"ckle9cxjn000echpfe6ug7jfs","tag_id":"ckle9cxlc002nchpf5lbb753p","_id":"ckle9cxmb0064chpfgqnl3qlr"},{"post_id":"ckle9cxjo000fchpf56kgcodo","tag_id":"ckle9cxks000zchpf9noddpxc","_id":"ckle9cxmb0065chpf7qr39n1c"},{"post_id":"ckle9cxjo000fchpf56kgcodo","tag_id":"ckle9cxld002pchpfggml2o9m","_id":"ckle9cxmb0066chpf8zf28eh9"},{"post_id":"ckle9cxjo000fchpf56kgcodo","tag_id":"ckle9cxks000ychpfbcjo3739","_id":"ckle9cxmb0067chpf8vdbazcx"},{"post_id":"ckle9cxjo000fchpf56kgcodo","tag_id":"ckle9cxl60025chpf0o836kwp","_id":"ckle9cxmc0068chpf5sgbfnod"},{"post_id":"ckle9cxjo000fchpf56kgcodo","tag_id":"ckle9cxle002schpfa0n6awl1","_id":"ckle9cxmc0069chpf7tg9d19q"},{"post_id":"ckle9cxjo000fchpf56kgcodo","tag_id":"ckle9cxl3001wchpf39gh5hbi","_id":"ckle9cxmc006achpf7a80926z"},{"post_id":"ckle9cxjp000gchpf8makcx19","tag_id":"ckle9cxlf002uchpf93f8hwnh","_id":"ckle9cxmc006bchpf92u02a7q"},{"post_id":"ckle9cxjp000gchpf8makcx19","tag_id":"ckle9cxlf002vchpf1yoe37jp","_id":"ckle9cxmc006cchpfhqr20iox"},{"post_id":"ckle9cxjp000gchpf8makcx19","tag_id":"ckle9cxlf002wchpfgzxj1jua","_id":"ckle9cxmc006dchpf784n6f35"},{"post_id":"ckle9cxjp000gchpf8makcx19","tag_id":"ckle9cxlg002xchpfbn6r1dcm","_id":"ckle9cxmc006echpf6uafg4av"},{"post_id":"ckle9cxjp000gchpf8makcx19","tag_id":"ckle9cxlg002ychpf706o8kse","_id":"ckle9cxmc006fchpfbwzl8tb0"},{"post_id":"ckle9cxjp000hchpf8udlc14f","tag_id":"ckle9cxlg002zchpfhf6lhryy","_id":"ckle9cxme006gchpf6y4uha9l"},{"post_id":"ckle9cxjp000hchpf8udlc14f","tag_id":"ckle9cxlh0030chpf9otial2r","_id":"ckle9cxme006hchpfbalg3t6y"},{"post_id":"ckle9cxjp000hchpf8udlc14f","tag_id":"ckle9cxlh0031chpf6sejhq2b","_id":"ckle9cxme006ichpffkd17yk4"},{"post_id":"ckle9cxjp000hchpf8udlc14f","tag_id":"ckle9cxlh0032chpfffu3f7z1","_id":"ckle9cxme006jchpf8crs365y"},{"post_id":"ckle9cxjp000hchpf8udlc14f","tag_id":"ckle9cxli0033chpf83f7akor","_id":"ckle9cxme006kchpf3dgn6fo6"},{"post_id":"ckle9cxjp000hchpf8udlc14f","tag_id":"ckle9cxli0034chpfgzjd5krl","_id":"ckle9cxme006lchpfg2axb4lm"},{"post_id":"ckle9cxjq000ichpfcnli7gqb","tag_id":"ckle9cxli0035chpf6o0uhpv9","_id":"ckle9cxme006mchpfgy0kg0oh"},{"post_id":"ckle9cxjq000ichpfcnli7gqb","tag_id":"ckle9cxl70027chpfasx12a7h","_id":"ckle9cxme006nchpf3tdp2h5g"},{"post_id":"ckle9cxjq000ichpfcnli7gqb","tag_id":"ckle9cxlj0037chpfgf8xfetd","_id":"ckle9cxme006ochpf7s496hy1"},{"post_id":"ckle9cxjq000ichpfcnli7gqb","tag_id":"ckle9cxlk0038chpffvjv5nmk","_id":"ckle9cxme006pchpfcc476p74"},{"post_id":"ckle9cxjq000ichpfcnli7gqb","tag_id":"ckle9cxks0011chpfhkml0ypf","_id":"ckle9cxmf006qchpfg8ifd9hi"},{"post_id":"ckle9cxjq000jchpfbzixbjxq","tag_id":"ckle9cxlk003achpfb3p57l15","_id":"ckle9cxmf006rchpfhlxbc3qs"},{"post_id":"ckle9cxjq000jchpfbzixbjxq","tag_id":"ckle9cxll003bchpf21gr1y8m","_id":"ckle9cxmf006schpfg7fj41xk"},{"post_id":"ckle9cxjq000jchpfbzixbjxq","tag_id":"ckle9cxll003cchpf416mgfd3","_id":"ckle9cxmf006tchpf8euyaymm"},{"post_id":"ckle9cxjq000jchpfbzixbjxq","tag_id":"ckle9cxky001gchpf3rxyeu7j","_id":"ckle9cxmf006uchpf3ikycef5"},{"post_id":"ckle9cxjr000kchpfdku0bfju","tag_id":"ckle9cxl3001wchpf39gh5hbi","_id":"ckle9cxmf006vchpf153ogkw9"},{"post_id":"ckle9cxjr000kchpfdku0bfju","tag_id":"ckle9cxlm003fchpfa4s33ihg","_id":"ckle9cxmf006wchpf8hazhn7h"},{"post_id":"ckle9cxjr000kchpfdku0bfju","tag_id":"ckle9cxlm003gchpf9lj34v46","_id":"ckle9cxmf006xchpf368idshb"},{"post_id":"ckle9cxjr000kchpfdku0bfju","tag_id":"ckle9cxl60025chpf0o836kwp","_id":"ckle9cxmf006ychpfedu9cleh"},{"post_id":"ckle9cxjs000lchpf70bkfhu6","tag_id":"ckle9cxks0011chpfhkml0ypf","_id":"ckle9cxmf006zchpf8sxw7v8q"},{"post_id":"ckle9cxjs000lchpf70bkfhu6","tag_id":"ckle9cxln003jchpf7lbuhozw","_id":"ckle9cxmf0070chpf17225hb3"},{"post_id":"ckle9cxjt000mchpf4wg91kgz","tag_id":"ckle9cxlo003kchpf6rkn2f6m","_id":"ckle9cxmg0071chpfcorqht42"},{"post_id":"ckle9cxjt000mchpf4wg91kgz","tag_id":"ckle9cxl50022chpfarxw6igk","_id":"ckle9cxmg0072chpf1lo15jg1"},{"post_id":"ckle9cxjt000mchpf4wg91kgz","tag_id":"ckle9cxlp003mchpffu1h26d8","_id":"ckle9cxmg0073chpff2ijgj4z"},{"post_id":"ckle9cxjt000mchpf4wg91kgz","tag_id":"ckle9cxlp003nchpf6bbo0jxh","_id":"ckle9cxmg0074chpf585r4l7e"},{"post_id":"ckle9cxjt000nchpf3noxc07s","tag_id":"ckle9cxlp003ochpf2jg29t0k","_id":"ckle9cxmg0075chpfcwzk5c67"},{"post_id":"ckle9cxjt000nchpf3noxc07s","tag_id":"ckle9cxky001gchpf3rxyeu7j","_id":"ckle9cxmg0076chpf73x1geqr"},{"post_id":"ckle9cxjt000nchpf3noxc07s","tag_id":"ckle9cxlq003qchpf2olpa8gw","_id":"ckle9cxmg0077chpf01wo8fqr"},{"post_id":"ckle9cxju000ochpf9zisdvn8","tag_id":"ckle9cxlq003rchpfc95e3ayn","_id":"ckle9cxmg0078chpf9wh18zqu"},{"post_id":"ckle9cxju000ochpf9zisdvn8","tag_id":"ckle9cxlr003schpf6sf8fqar","_id":"ckle9cxmg0079chpfa55zeeja"},{"post_id":"ckle9cxju000ochpf9zisdvn8","tag_id":"ckle9cxks0011chpfhkml0ypf","_id":"ckle9cxmg007achpf5jqyfx8b"},{"post_id":"ckle9cxju000ochpf9zisdvn8","tag_id":"ckle9cxkt0012chpf9jblc5nr","_id":"ckle9cxmg007bchpf4vgu1i8c"},{"post_id":"ckle9cxjv000pchpf9fo50hqx","tag_id":"ckle9cxkt0012chpf9jblc5nr","_id":"ckle9cxmh007cchpfc1xb7i8w"},{"post_id":"ckle9cxjv000pchpf9fo50hqx","tag_id":"ckle9cxls003wchpffjf82sft","_id":"ckle9cxmh007dchpf7tgdbhss"},{"post_id":"ckle9cxjv000pchpf9fo50hqx","tag_id":"ckle9cxlt003xchpfgwr9f5op","_id":"ckle9cxmh007echpfackycpcl"},{"post_id":"ckle9cxkd000qchpfcntu1oau","tag_id":"ckle9cxky001gchpf3rxyeu7j","_id":"ckle9cxmp007fchpfbbonfniw"},{"post_id":"ckle9cxkd000qchpfcntu1oau","tag_id":"ckle9cxm0003zchpfdqwg99ro","_id":"ckle9cxmq007gchpf0ghh6ac7"},{"post_id":"ckle9cxko000schpf2ulcgslz","tag_id":"ckle9cxm10042chpff52693tq","_id":"ckle9cxmq007hchpfezykhdh4"},{"post_id":"ckle9cxko000schpf2ulcgslz","tag_id":"ckle9cxm10045chpfe1i4exzw","_id":"ckle9cxmq007ichpfb413heyf"},{"post_id":"ckle9cxko000schpf2ulcgslz","tag_id":"ckle9cxky001gchpf3rxyeu7j","_id":"ckle9cxmr007jchpf2435dxds"},{"post_id":"ckle9cxkp000tchpf3nswdahn","tag_id":"ckle9cxkd000rchpf0agiaqus","_id":"ckle9cxms007kchpf78t59aur"},{"post_id":"ckle9cxkp000tchpf3nswdahn","tag_id":"ckle9cxks000zchpf9noddpxc","_id":"ckle9cxms007lchpf4abh22rt"},{"post_id":"ckle9cxkp000tchpf3nswdahn","tag_id":"ckle9cxm20048chpf67w74ahx","_id":"ckle9cxmt007mchpf0vixe56q"},{"post_id":"ckle9cxkp000tchpf3nswdahn","tag_id":"ckle9cxks000ychpfbcjo3739","_id":"ckle9cxmt007nchpfe6sa5xen"},{"post_id":"ckle9cxkq000vchpfc3jl00sc","tag_id":"ckle9cxm3004cchpf05up6vdl","_id":"ckle9cxmt007ochpf3y2tbdhd"},{"post_id":"ckle9cxkq000vchpfc3jl00sc","tag_id":"ckle9cxm3004fchpf8pze5s2a","_id":"ckle9cxmu007pchpfcsofd9na"},{"post_id":"ckle9cxkq000vchpfc3jl00sc","tag_id":"ckle9cxm4004ichpfbyfsgwdw","_id":"ckle9cxmu007qchpf4z6658aa"},{"post_id":"ckle9cxkq000vchpfc3jl00sc","tag_id":"ckle9cxm4004lchpfh0oj0g9k","_id":"ckle9cxmu007rchpfeqh1d7h1"},{"post_id":"ckle9cxkq000vchpfc3jl00sc","tag_id":"ckle9cxm5004pchpf5t0uhzka","_id":"ckle9cxmu007schpfdyeb6tsr"},{"post_id":"ckle9cxkq000vchpfc3jl00sc","tag_id":"ckle9cxl70028chpfgwrpcc3d","_id":"ckle9cxmu007tchpfgg53f1hj"},{"post_id":"ckle9cxkr000wchpf9va83uca","tag_id":"ckle9cxm5004schpfa4ib12mb","_id":"ckle9cxmu007uchpf8cbobdvt"},{"post_id":"ckle9cxkr000wchpf9va83uca","tag_id":"ckle9cxm4004ichpfbyfsgwdw","_id":"ckle9cxmu007vchpfhm3mh8uj"},{"post_id":"ckle9cxkr000wchpf9va83uca","tag_id":"ckle9cxm6004ychpfef4749zd","_id":"ckle9cxmu007wchpf91fofh8j"},{"post_id":"ckle9cxkr000wchpf9va83uca","tag_id":"ckle9cxm70051chpf2g126sng","_id":"ckle9cxmu007xchpf6dqgcer7"},{"post_id":"ckle9cxkr000wchpf9va83uca","tag_id":"ckle9cxm80055chpfeulpc72d","_id":"ckle9cxmu007ychpfgchzb4ko"},{"post_id":"ckle9cxkr000wchpf9va83uca","tag_id":"ckle9cxll003cchpf416mgfd3","_id":"ckle9cxmu007zchpf5glm9jv8"}],"Tag":[{"name":"HDFS","_id":"ckle9cxkd000rchpf0agiaqus"},{"name":"Airflow","_id":"ckle9cxkp000uchpf0yr57w5h"},{"name":"Orchestration","_id":"ckle9cxkr000xchpfbvog9kmu"},{"name":"BigData","_id":"ckle9cxks000ychpfbcjo3739"},{"name":"Hive","_id":"ckle9cxks000zchpf9noddpxc"},{"name":"Sqoop","_id":"ckle9cxks0010chpf2fvt2pim"},{"name":"Docker","_id":"ckle9cxks0011chpfhkml0ypf"},{"name":"Spark","_id":"ckle9cxkt0012chpf9jblc5nr"},{"name":"Metrics","_id":"ckle9cxkt0013chpfh8tnhomf"},{"name":"Alluxio","_id":"ckle9cxku0015chpf6oeeg68p"},{"name":"Distributed Storage","_id":"ckle9cxkv0017chpfgjbkarma"},{"name":"PoC","_id":"ckle9cxkw0018chpf0xodejcr"},{"name":"GoogleChrome","_id":"ckle9cxkx001cchpfcx4b5qk0"},{"name":"Customization","_id":"ckle9cxkx001dchpff0y397f3"},{"name":"Internet","_id":"ckle9cxky001echpf4ui8582d"},{"name":"Proxy","_id":"ckle9cxky001fchpf09488z6g"},{"name":"Linux","_id":"ckle9cxky001gchpf3rxyeu7j"},{"name":"BigQuery","_id":"ckle9cxkz001ichpfhhhlc8pj"},{"name":"Avro","_id":"ckle9cxkz001jchpf2cii74ji"},{"name":"Parquet","_id":"ckle9cxl0001lchpfdruz29nd"},{"name":"GCS","_id":"ckle9cxl0001mchpfh5f2ha8t"},{"name":"github","_id":"ckle9cxl1001nchpfbxmb17jo"},{"name":"hexo","_id":"ckle9cxl1001ochpfdhy41c83"},{"name":"blog","_id":"ckle9cxl1001pchpf4jpk1lgc"},{"name":"git","_id":"ckle9cxl2001qchpfarj85dq8"},{"name":"markdown","_id":"ckle9cxl2001rchpf0gxn5zfg"},{"name":"Google Analytics","_id":"ckle9cxl2001schpfhl4d68pq"},{"name":"Analytics","_id":"ckle9cxl2001tchpfcds7f21j"},{"name":"Dimensions","_id":"ckle9cxl3001vchpfc2w0eyes"},{"name":"Cloudera","_id":"ckle9cxl3001wchpf39gh5hbi"},{"name":"Polybase","_id":"ckle9cxl4001xchpfa6ps68jw"},{"name":"SQLServer","_id":"ckle9cxl50020chpfflvp91k1"},{"name":"Data Ingestion","_id":"ckle9cxl50021chpfafkd0qsn"},{"name":"Conferences","_id":"ckle9cxl50022chpfarxw6igk"},{"name":"SparkSummit","_id":"ckle9cxl60024chpfa46vdy2o"},{"name":"Hadoop","_id":"ckle9cxl60025chpf0o836kwp"},{"name":"Streamming","_id":"ckle9cxl70026chpf9mkqh2ku"},{"name":"Kafka","_id":"ckle9cxl70027chpfasx12a7h"},{"name":"Ansible","_id":"ckle9cxl70028chpfgwrpcc3d"},{"name":"btrfs","_id":"ckle9cxl8002bchpfgivs0388"},{"name":"docker-compose","_id":"ckle9cxl9002dchpf2stn2j80"},{"name":"utils","_id":"ckle9cxl9002echpf2nsudoot"},{"name":"dr-elephant","_id":"ckle9cxla002fchpfb32bcjqc"},{"name":"Performance","_id":"ckle9cxla002hchpf91483gh2"},{"name":"Monitoring","_id":"ckle9cxlb002ichpfe30x8zke"},{"name":"Tunning","_id":"ckle9cxlb002jchpf0eyxc3pe"},{"name":"cheat","_id":"ckle9cxlc002mchpfa17gfkcb"},{"name":"sheet","_id":"ckle9cxlc002nchpf5lbb753p"},{"name":"udfs","_id":"ckle9cxld002pchpfggml2o9m"},{"name":"Java","_id":"ckle9cxle002schpfa0n6awl1"},{"name":"hadoop","_id":"ckle9cxlf002uchpf93f8hwnh"},{"name":"hive","_id":"ckle9cxlf002vchpf1yoe37jp"},{"name":"GC","_id":"ckle9cxlf002wchpfgzxj1jua"},{"name":"exception","_id":"ckle9cxlg002xchpfbn6r1dcm"},{"name":"beeline","_id":"ckle9cxlg002ychpf706o8kse"},{"name":"keybase","_id":"ckle9cxlg002zchpfhf6lhryy"},{"name":"pgp","_id":"ckle9cxlh0030chpf9otial2r"},{"name":"encryption","_id":"ckle9cxlh0031chpf6sejhq2b"},{"name":"pki","_id":"ckle9cxlh0032chpfffu3f7z1"},{"name":"keys","_id":"ckle9cxli0033chpf83f7akor"},{"name":"kbfs","_id":"ckle9cxli0034chpfgzjd5krl"},{"name":"KSQL","_id":"ckle9cxli0035chpf6o0uhpv9"},{"name":"SQL","_id":"ckle9cxlj0037chpfgf8xfetd"},{"name":"Streaming","_id":"ckle9cxlk0038chpffvjv5nmk"},{"name":"Markdown","_id":"ckle9cxlk003achpfb3p57l15"},{"name":"Tools","_id":"ckle9cxll003bchpf21gr1y8m"},{"name":"Utils","_id":"ckle9cxll003cchpf416mgfd3"},{"name":"Nifi","_id":"ckle9cxlm003fchpfa4s33ihg"},{"name":"CDH Parcel","_id":"ckle9cxlm003gchpf9lj34v46"},{"name":"Portainer","_id":"ckle9cxln003jchpf7lbuhozw"},{"name":"PortoTechHub","_id":"ckle9cxlo003kchpf6rkn2f6m"},{"name":"Porto","_id":"ckle9cxlp003mchpffu1h26d8"},{"name":"Tech","_id":"ckle9cxlp003nchpf6bbo0jxh"},{"name":"PowerShell","_id":"ckle9cxlp003ochpf2jg29t0k"},{"name":"Windows","_id":"ckle9cxlq003qchpf2olpa8gw"},{"name":"Zeepelin","_id":"ckle9cxlq003rchpfc95e3ayn"},{"name":"Scala","_id":"ckle9cxlr003schpf6sf8fqar"},{"name":"RDD","_id":"ckle9cxls003wchpffjf82sft"},{"name":"CheatSheet","_id":"ckle9cxlt003xchpfgwr9f5op"},{"name":"ssh","_id":"ckle9cxm0003zchpfdqwg99ro"},{"name":"IDE","_id":"ckle9cxm10042chpff52693tq"},{"name":"Install","_id":"ckle9cxm10045chpfe1i4exzw"},{"name":"Optimization","_id":"ckle9cxm20048chpf67w74ahx"},{"name":"Vagrant","_id":"ckle9cxm3004cchpf05up6vdl"},{"name":"Qemu","_id":"ckle9cxm3004fchpf8pze5s2a"},{"name":"KVM","_id":"ckle9cxm4004ichpfbyfsgwdw"},{"name":"libvirt","_id":"ckle9cxm4004lchpfh0oj0g9k"},{"name":"Mesos","_id":"ckle9cxm5004pchpf5t0uhzka"},{"name":"VirtualBox","_id":"ckle9cxm5004schpfa4ib12mb"},{"name":"QEMU","_id":"ckle9cxm6004ychpfef4749zd"},{"name":"vdi","_id":"ckle9cxm70051chpf2g126sng"},{"name":"qcow2","_id":"ckle9cxm80055chpfeulpc72d"}]}}